{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = keys['GOOGLE_CSE_ID']\n",
    "os.environ[\"GOOGLE_API_KEY\"] = keys['GOOGLE_API_KEY']\n",
    "os.environ[\"OPENAI_API_KEY\"] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Test\n",
    "- I/O\n",
    "    - Input: Query\n",
    "    - Output: {\"Link\", \"Title\", \"Snippet\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Custom Search Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Search \n",
    "- Comments\n",
    "    - Only in Google Programmable Search Engine's Setting can we config Search Range\n",
    "    - {snippet} might be less information\n",
    "\n",
    "- I/O\n",
    "    - Input: Query\n",
    "    - Output: dict{\"link\", \"title\", \"snippet\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GoogleSearchAPIWrapper\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass  `google_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m GoogleSearchAPIWrapper\n\u001b[0;32m----> 3\u001b[0m google_search \u001b[39m=\u001b[39m GoogleSearchAPIWrapper(\n\u001b[1;32m      4\u001b[0m     k \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     siterestrict \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GoogleSearchAPIWrapper\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass  `google_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper(\n",
    "    k = 100,\n",
    "    siterestrict = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the difference between GPT and BERT?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place is really low robustness, need to dig down\n",
    "search_result = google_search.results(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'GPT-3 vs. BERT: Comparing the Two Most Popular Language Models', 'link': 'https://blog.invgate.com/gpt-3-vs-bert', 'snippet': 'Feb 9, 2023 ... The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT\\xa0...'}, {'title': 'GPT-3 Versus BERT: A High-Level Comparison - Symbl.ai', 'link': 'https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/', 'snippet': \"Nov 1, 2022 ... The foremost architectural distinction is that in a transformer's encoder-decoder model, BERT is the encoder part, while GPT-3 is the decoder\\xa0...\"}, {'title': 'machine learning - BERT vs GPT architectural, conceptual and ...', 'link': 'https://datascience.stackexchange.com/questions/104536/bert-vs-gpt-architectural-conceptual-and-implemetational-differences', 'snippet': 'Nov 26, 2021 ... To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective\\xa0...'}, {'title': 'Notes on GPT-2 and BERT models | Kaggle', 'link': 'https://www.kaggle.com/code/residentmario/notes-on-gpt-2-and-bert-models', 'snippet': 'Whereas GPT-2 learns on the \"predict next\" task directly, BERT learns on the task \"learn the word in a sentence in which 15% of the words are masked out\". The\\xa0...'}, {'title': 'GPT and BERT: A Comparison of Transformer Architectures - DEV ...', 'link': 'https://dev.to/meetkern/gpt-and-bert-a-comparison-of-transformer-architectures-2k46', 'snippet': 'Feb 9, 2023 ... Unlike GPT, which only processes input from left to right like humans read words, BERT processes input both left to right and right to left in\\xa0...'}, {'title': 'Comparison between BERT, GPT-2 and ELMo | by Gaurav Ghati ...', 'link': 'https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda', 'snippet': 'May 3, 2020 ... Comparison of BERT, GPT-2 and ELMo · BERT and GPT are transformer-based architecture while ELMo is Bi-LSTM Language model. · BERT is purely Bi-\\xa0...'}, {'title': 'GPT vs. BERT: What Are the Differences Between the Two Most ...', 'link': 'https://www.makeuseof.com/gpt-vs-bert/', 'snippet': 'Apr 26, 2023 ... BERT and GPT differ in the types of training data they use. BERT is trained using a masked language model, meaning certain words are masked, and\\xa0...'}, {'title': 'GPT-3 Vs BERT For NLP Tasks', 'link': 'https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/', 'snippet': 'Sep 11, 2020 ... While GPT3 generates output one token at a time, BERT, on the other hand, is not autoregressive, thus uses deep bidirectional context for\\xa0...'}, {'title': 'BERT vs GPT: An In-depth Comparison of Two Leading Language ...', 'link': 'https://360digitmg.com/blog/gpt-vs-bert', 'snippet': \"BERT architecture has '340' million parameters compared to 175 billion parameters of GPT-3. The average user may run out of memory in an attempt to run the GPT\\xa0...\"}, {'title': 'gpt 2 - Key difference between BERT and GPT2? - Stack Overflow', 'link': 'https://stackoverflow.com/questions/66852791/key-difference-between-bert-and-gpt2', 'snippet': 'Mar 29, 2021 ... But the key difference in structure between them is just adding a mask or not in self-attention, and trained the model in different ways. From\\xa0...'}]\n"
     ]
    }
   ],
   "source": [
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   https://blog.invgate.com/gpt-3-vs-bert\n",
      "1   https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/\n",
      "2   https://datascience.stackexchange.com/questions/104536/bert-vs-gpt-architectural-conceptual-and-implemetational-differences\n",
      "3   https://www.kaggle.com/code/residentmario/notes-on-gpt-2-and-bert-models\n",
      "4   https://dev.to/meetkern/gpt-and-bert-a-comparison-of-transformer-architectures-2k46\n",
      "5   https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda\n",
      "6   https://www.makeuseof.com/gpt-vs-bert/\n",
      "7   https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/\n",
      "8   https://360digitmg.com/blog/gpt-vs-bert\n",
      "9   https://stackoverflow.com/questions/66852791/key-difference-between-bert-and-gpt2\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sr in search_result:\n",
    "    print(i,' ', sr['link'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "1   GPT-3 Versus BERT: A High-Level Comparison - Symbl.ai\n",
      "2   machine learning - BERT vs GPT architectural, conceptual and ...\n",
      "3   Notes on GPT-2 and BERT models | Kaggle\n",
      "4   GPT and BERT: A Comparison of Transformer Architectures - DEV ...\n",
      "5   Comparison between BERT, GPT-2 and ELMo | by Gaurav Ghati ...\n",
      "6   GPT vs. BERT: What Are the Differences Between the Two Most ...\n",
      "7   GPT-3 Vs BERT For NLP Tasks\n",
      "8   BERT vs GPT: An In-depth Comparison of Two Leading Language ...\n",
      "9   gpt 2 - Key difference between BERT and GPT2? - Stack Overflow\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sr in search_result:\n",
    "    print(i,' ', sr['title'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   Feb 9, 2023 ... The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT ...\n",
      "1   Nov 1, 2022 ... The foremost architectural distinction is that in a transformer's encoder-decoder model, BERT is the encoder part, while GPT-3 is the decoder ...\n",
      "2   Nov 26, 2021 ... To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective ...\n",
      "3   Whereas GPT-2 learns on the \"predict next\" task directly, BERT learns on the task \"learn the word in a sentence in which 15% of the words are masked out\". The ...\n",
      "4   Feb 9, 2023 ... Unlike GPT, which only processes input from left to right like humans read words, BERT processes input both left to right and right to left in ...\n",
      "5   May 3, 2020 ... Comparison of BERT, GPT-2 and ELMo · BERT and GPT are transformer-based architecture while ELMo is Bi-LSTM Language model. · BERT is purely Bi- ...\n",
      "6   Apr 26, 2023 ... BERT and GPT differ in the types of training data they use. BERT is trained using a masked language model, meaning certain words are masked, and ...\n",
      "7   Sep 11, 2020 ... While GPT3 generates output one token at a time, BERT, on the other hand, is not autoregressive, thus uses deep bidirectional context for ...\n",
      "8   BERT architecture has '340' million parameters compared to 175 billion parameters of GPT-3. The average user may run out of memory in an attempt to run the GPT ...\n",
      "9   Mar 29, 2021 ... But the key difference in structure between them is just adding a mask or not in self-attention, and trained the model in different ways. From ...\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sr in search_result:\n",
    "    print(i,' ', sr['snippet'])\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Digging\n",
    "- I/O\n",
    "    - Input: {\"Link\", \"Title\", \"Snippet\"}\n",
    "    - Output: Text String"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Method\n",
    "- Description\n",
    "    - Directly select the top k result return from Google CSE\n",
    "    - Load the Web Link and Get the Text String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   https://blog.invgate.com/gpt-3-vs-bert\n",
      "1   https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/\n",
      "2   https://datascience.stackexchange.com/questions/104536/bert-vs-gpt-architectural-conceptual-and-implemetational-differences\n",
      "3   https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models\n",
      "4   https://dev.to/meetkern/gpt-and-bert-a-comparison-of-transformer-architectures-2k46\n",
      "5   https://www.kaggle.com/code/residentmario/notes-on-gpt-2-and-bert-models\n",
      "6   https://www.makeuseof.com/gpt-vs-bert/\n",
      "7   https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda\n",
      "8   https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/\n",
      "9   https://360digitmg.com/blog/gpt-vs-bert\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sr in search_result:\n",
    "    print(i,' ', sr['link'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://blog.invgate.com/gpt-3-vs-bert', 'https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/', 'https://datascience.stackexchange.com/questions/104536/bert-vs-gpt-architectural-conceptual-and-implemetational-differences']\n"
     ]
    }
   ],
   "source": [
    "# Top k Method\n",
    "k = 3\n",
    "web_url = []\n",
    "\n",
    "for i in range(k):\n",
    "    web_url.append(search_result[i]['link'])\n",
    "\n",
    "print(web_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "url_loader = UnstructuredURLLoader(\n",
    "    urls = web_url,\n",
    "    continue_on_failure = True,  # determines whether the loader should continue loading files even if there is a failure\n",
    "    mode = \"single\"  # determines whether the loader should return a single document or a list of documents\n",
    ")\n",
    "\n",
    "url_data = url_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(url_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://blog.invgate.com/gpt-3-vs-bert'}\n"
     ]
    }
   ],
   "source": [
    "print(url_data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITSM\n",
      "Ticket Volume\n",
      "Workplace\n",
      "Product\n",
      "Case Studies\n",
      "Guides\n",
      "\n",
      "\n",
      "Meet InvGate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "English\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Español\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "English\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Español\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ITSM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workplace\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Product\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Case Study\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Languages\n",
      "\n",
      "\n",
      "English\n",
      " \n",
      "Español\n",
      "\n",
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "\n",
      "Celeste Mottesi\n",
      "            February 9, 2023\n",
      "\n",
      "3 min read\n",
      "\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "\n",
      "What is GPT-3?\n",
      "\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "\n",
      "Examples of AI-writing tools based on GPT-3\n",
      "\n",
      "Several AI content writing tools currently use GPT-3, such as:\n",
      "\n",
      "Jasper\n",
      "\n",
      "ChibiAI\n",
      "\n",
      "WriteSonic\n",
      "\n",
      "Simplified\n",
      "\n",
      "Kafkai\n",
      "\n",
      "Copysmith\n",
      "\n",
      "What is BERT?\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
      "\n",
      "BERT use cases\n",
      "\n",
      "BERT serves as the base for a number of services, like:\n",
      "\n",
      "Google Search Engine\n",
      "\n",
      "Huggingface Transformer Library\n",
      "\n",
      "Microsoft Azure Cognitive Services\n",
      "\n",
      "Google Natural Language API\n",
      "\n",
      "Differences between GPT-3 and BERT\n",
      "\n",
      "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
      "\n",
      "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
      "\n",
      "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
      "\n",
      "Similarities between GPT-3 and BERT\n",
      "\n",
      "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
      "\n",
      "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
      "\n",
      "They are unsupervised learning models (they don’t require labeled data for training).\n",
      "\n",
      "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
      "\n",
      "GPT-3 vs. BERT: capabilities comparison\n",
      "\n",
      "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
      "\n",
      "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
      "\n",
      "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
      "\n",
      "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n",
      "\n",
      "Read other articles like this :\n",
      "            \n",
      "            AI\n",
      "\n",
      "Read other articles like this:\n",
      "\n",
      "Evaluate InvGate as Your ITSM Solution\n",
      "30-day free trial - No credit card needed\n",
      "\n",
      "Get Started\n",
      "\n",
      "Service Desk\n",
      "\n",
      "IT Service Management\n",
      "Enterprise Service Management\n",
      "Ticket Management\n",
      "Self-Service\n",
      "Workflows\n",
      "\n",
      "\n",
      "\n",
      "ITAM\n",
      "\n",
      "IT Asset Management\n",
      "Asset Inventory\n",
      "CMDB\n",
      "Network Discovery\n",
      "Metering\n",
      "\n",
      "\n",
      "\n",
      "Learn\n",
      "\n",
      "ITSM\n",
      "ITIL\n",
      "ESM\n",
      "SLAs\n",
      "Help Desks\n",
      "Ticket Volume\n",
      "\n",
      "\n",
      "\n",
      "InvGate\n",
      "\n",
      "About Us\n",
      "Why InvGate?\n",
      "Partners\n",
      "Careers\n",
      "Contact Us\n",
      "Leave a Review\n",
      "\n",
      "\n",
      "\n",
      "Compare With\n",
      "\n",
      "Jira\n",
      "Service Now\n",
      "Clarity\n",
      "Freshservice\n",
      "Manage Engine\n",
      "Other Alternatives\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Privacy Policy\n",
      "Quality Policy\n",
      "Cookie Policy\n",
      "Terms and Conditions\n",
      "Solutions Terms and Conditions\n",
      "© InvGate\n"
     ]
    }
   ],
   "source": [
    "print(url_data[0].page_content)\n",
    "text1 = url_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack Exchange Network\n",
      "\n",
      "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
      "\n",
      "Visit Stack Exchange\n",
      "\n",
      "Loading…\n",
      "\n",
      "\n",
      "\n",
      "Tour\n",
      "                                \n",
      "                                    Start here for a quick overview of the site\n",
      "\n",
      "Help Center\n",
      "                            \n",
      "                                Detailed answers to any questions you might have\n",
      "\n",
      "Meta\n",
      "                                        \n",
      "                                            Discuss the workings and policies of this site\n",
      "\n",
      "About Us\n",
      "                                    \n",
      "                                        Learn more about Stack Overflow the company, and our products.\n",
      "\n",
      "\n",
      "\n",
      "current community\n",
      "            \n",
      "            \n",
      "                \n",
      "                    \n",
      "                        \n",
      "                    \n",
      "                \n",
      "            \n",
      "        \n",
      "        \n",
      "            \n",
      "                    \n",
      "                            \n",
      "                \n",
      "        \n",
      "        \n",
      "            Data Science\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "            help\n",
      "            chat\n",
      "    \n",
      "\n",
      "                    \n",
      "                    \n",
      "                            \n",
      "        \n",
      "    \n",
      "\n",
      "                            \n",
      "        \n",
      "        \n",
      "            Data Science Meta\n",
      "        \n",
      "    \n",
      "\n",
      "                    \n",
      "            \n",
      "        \n",
      "\n",
      "        \n",
      "            \n",
      "your communities            \n",
      "\n",
      "        \n",
      "        \n",
      "\n",
      "                \n",
      "Sign up or log in to customize your list.                \n",
      "        \n",
      "\n",
      "        \n",
      "            more stack exchange communities\n",
      "            \n",
      "            company blog\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      "Sign up\n",
      "\n",
      "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
      "\n",
      "Sign up to join this community\n",
      "\n",
      "Anybody can ask a question\n",
      "\n",
      "Anybody can answer\n",
      "\n",
      "The best answers are voted up and rise to the top\n",
      "\n",
      "Home\n",
      "\n",
      "Public\n",
      "\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "            Questions\n",
      "    \n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "            \n",
      "                \n",
      "                    Tags\n",
      "                \n",
      "            \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "            \n",
      "                \n",
      "                    Users\n",
      "                \n",
      "            \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "            \n",
      "                \n",
      "                    Companies\n",
      "                \n",
      "            \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "            \n",
      "                \n",
      "                    Unanswered\n",
      "\n",
      "Teams\n",
      "\n",
      "    \n",
      "        Stack Overflow for Teams\n",
      "        – Start collaborating and sharing organizational knowledge.\n",
      "        \n",
      "        \n",
      "\n",
      "        Create a free Team\n",
      "        Why Teams?\n",
      "\n",
      "Teams\n",
      "\n",
      "Create free Team\n",
      "\n",
      "Teams\n",
      "\n",
      "Q&A for work\n",
      "\n",
      "Connect and share knowledge within a single location that is structured and easy to search.\n",
      "\n",
      "Learn more about Teams\n",
      "\n",
      "BERT vs GPT architectural, conceptual and implemetational differences\n",
      "\n",
      "Ask Question\n",
      "\n",
      "Asked\n",
      "\n",
      "Modified\n",
      "\n",
      "1 year, 5 months ago\n",
      "\n",
      "Viewed\n",
      "                        6k times\n",
      "\n",
      "$\\begingroup$\n",
      "\n",
      "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
      "\n",
      "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
      "\n",
      "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
      "\n",
      "But I have following doubts:\n",
      "\n",
      "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
      "\n",
      "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
      "\n",
      "Gpt2Model.forward does indeed contain\n",
      "\n",
      "BertModel.forward. So, I guess swapping out\n",
      "\n",
      "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
      "\n",
      "machine-learning\n",
      "\n",
      "nlp\n",
      "\n",
      "bert\n",
      "\n",
      "transformer\n",
      "\n",
      "gpt\n",
      "\n",
      "Share\n",
      "\n",
      "Improve this question\n",
      "\n",
      "\n",
      "        asked \n",
      "\n",
      "Nov 26, 2021 at 21:22\n",
      "\n",
      "Rnj\n",
      "\n",
      "Rnj\n",
      "\n",
      "205\n",
      "\n",
      "2 silver badges\n",
      "\n",
      "7 bronze badges\n",
      "\n",
      "$\\endgroup$\n",
      "\n",
      "Add a comment\n",
      "\n",
      "1 Answer\n",
      "                                    1\n",
      "\n",
      "Reset to default\n",
      "\n",
      "$\\begingroup$\n",
      "\n",
      "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
      "\n",
      "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
      "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
      "\n",
      "Then you check what score was would good and bad get at the position of the [MASK] token.\n",
      "\n",
      "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
      "\n",
      "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
      "\n",
      "Share\n",
      "\n",
      "Improve this answer\n",
      "\n",
      "\n",
      "        answered \n",
      "\n",
      "Nov 30, 2021 at 10:46\n",
      "\n",
      "Jindřich\n",
      "\n",
      "Jindřich\n",
      "\n",
      "1,631\n",
      "\n",
      "4 silver badges\n",
      "\n",
      "8 bronze badges\n",
      "\n",
      "$\\endgroup$\n",
      "\n",
      "Add a comment\n",
      "\n",
      "Your Answer\n",
      "\n",
      "Thanks for contributing an answer to Data Science Stack Exchange!\n",
      "\n",
      "Please be sure to answer the question. Provide details and share your research!\n",
      "\n",
      "But avoid …\n",
      "\n",
      "Asking for help, clarification, or responding to other answers.\n",
      "\n",
      "Making statements based on opinion; back them up with references or personal experience.\n",
      "\n",
      "Use MathJax to format equations. MathJax reference.\n",
      "\n",
      "To learn more, see our tips on writing great answers.\n",
      "\n",
      "Draft saved\n",
      "\n",
      "Draft discarded\n",
      "\n",
      "Sign up or log in\n",
      "\n",
      "Post as a guest\n",
      "\n",
      "Required, but never shown\n",
      "\n",
      "Post as a guest\n",
      "\n",
      "Required, but never shown\n",
      "\n",
      "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
      "\n",
      "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
      "\n",
      "The Overflow Blog\n",
      "\n",
      "How to use marketing techniques to build a better resume\n",
      "\n",
      "How the creator of Angular is dehydrating the web (Ep. 574)\n",
      "\n",
      "Featured on Meta\n",
      "\n",
      "AI/ML Tool examples part 3 - Title-Drafting Assistant\n",
      "\n",
      "We are graduating the updated button styling for vote arrows\n",
      "\n",
      "Middle Sidebar Advertising\n",
      "\n",
      "Related\n",
      "\n",
      "How Transformer is Bidirectional - Machine Learning\n",
      "\n",
      "Does the transformer decoder reuse previous tokens' intermediate states like GPT2?\n",
      "\n",
      "Transformer-based architectures for regression tasks\n",
      "\n",
      "What is the difference between GPT blocks and Transformer Decoder blocks?\n",
      "\n",
      "What is the difference between GPT blocks and BERT blocks\n",
      "\n",
      "What are the inputs to the first decoder layer in a Transformer model during the training phase?\n",
      "\n",
      "ChatGPT's Architecture - Decoder Only? Or Encoder-Decoder?\n",
      "\n",
      "Do transformers (e.g. BERT) have an unlimited input size?\n",
      "\n",
      "Hot Network Questions\n",
      "\n",
      "Is there anything important to know about flying at ~9000ft for the first time?\n",
      "\n",
      "If a character could replace the result of weapon die rolls with their proficiency bonus, what's the average die result for weapons that use 2d6?\n",
      "\n",
      "How to list only tar files\n",
      "\n",
      "A word to cover meanings of religion, mysticism, occult, and philosophy\n",
      "\n",
      "How to handle it if one player wants to take a risky course of action that nobody else wants?\n",
      "\n",
      "Is every rational sequence topology homeomorphic?\n",
      "\n",
      "SQL Server BACKUP FULL by variables\n",
      "\n",
      "What does the \"proportional symbol\" (∝) mean when used as a time unit prefix?\n",
      "\n",
      "Why would I use \\dimexpr .. \\relax in \\setlength?\n",
      "\n",
      "Why does ChatGPT fail in playing \"20 questions\"?\n",
      "\n",
      "What happens if an insurance company does not mention a critical clause of a policy during a telephone purchase?\n",
      "\n",
      "SPARC coprocessor instructions\n",
      "\n",
      "Tiny Creatures Can't Kill Players, Right?\n",
      "\n",
      "Does every USB device have a vendor id and product id?\n",
      "\n",
      "How far apart has the sun drifted from Alpha Centari due to the expansion of the universe since its formation?\n",
      "\n",
      "How to use the new LLMFunction?\n",
      "\n",
      "Old biscuit recipe question - \"until the dough blisters\"\n",
      "\n",
      "Solve this trigonomtery problem without using trigonometric functions.\n",
      "\n",
      "Can you be fired for refusing to lie?\n",
      "\n",
      "Is it appropriate to cite vulgar websites used for gathering data?\n",
      "\n",
      "Resultant of two polynomials\n",
      "\n",
      "What is the basic difference between curry powder and garam masala?\n",
      "\n",
      "How can the US deny access to G7 markets?\n",
      "\n",
      "Must a strong reducing agent be a weak oxidising agent, and vice versa?\n",
      "\n",
      "more hot questions\n",
      "\n",
      "Question feed\n",
      "\n",
      "Subscribe to RSS\n",
      "\n",
      "To subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n",
      "\n",
      "Data Science\n",
      "\n",
      "Tour\n",
      "\n",
      "Help\n",
      "\n",
      "Chat\n",
      "\n",
      "Contact\n",
      "\n",
      "Feedback\n",
      "\n",
      "Company\n",
      "\n",
      "Stack Overflow\n",
      "\n",
      "Teams\n",
      "\n",
      "Advertising\n",
      "\n",
      "Collectives\n",
      "\n",
      "Talent\n",
      "\n",
      "About\n",
      "\n",
      "Press\n",
      "\n",
      "Legal\n",
      "\n",
      "Privacy Policy\n",
      "\n",
      "Terms of Service\n",
      "\n",
      "Cookie Settings\n",
      "\n",
      "Cookie Policy\n",
      "\n",
      "Stack Exchange Network\n",
      "\n",
      "Technology\n",
      "\n",
      "Culture & recreation\n",
      "\n",
      "Life & arts\n",
      "\n",
      "Science\n",
      "\n",
      "Professional\n",
      "\n",
      "Business\n",
      "\n",
      "API\n",
      "\n",
      "Data\n",
      "\n",
      "Blog\n",
      "\n",
      "Facebook\n",
      "\n",
      "Twitter\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Instagram\n",
      "\n",
      "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
      "\n",
      "Your privacy\n",
      "\n",
      "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n"
     ]
    }
   ],
   "source": [
    "print(url_data[2].page_content)\n",
    "text2 = url_data[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Several AI content writing tools currently use GPT-3, such as:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
      "BERT serves as the base for a number of services, like:\n",
      "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
      "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
      "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
      "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
      "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
      "They are unsupervised learning models (they don’t require labeled data for training).\n",
      "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
      "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
      "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
      "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
      "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
      "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n"
     ]
    }
   ],
   "source": [
    "text1_lines = text1.split('\\n')\n",
    "text1_processed = []\n",
    "for line in text1_lines:\n",
    "    if (len(line)) > 50:\n",
    "        text1_processed.append(line)\n",
    "\n",
    "text1_processed = '\\n'.join(text1_processed)\n",
    "print(text1_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
      "                                        Learn more about Stack Overflow the company, and our products.\n",
      "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
      "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
      "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
      "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
      "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
      "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
      "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
      "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
      "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
      "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
      "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
      "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
      "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
      "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
      "If a character could replace the result of weapon die rolls with their proficiency bonus, what's the average die result for weapons that use 2d6?\n",
      "What happens if an insurance company does not mention a critical clause of a policy during a telephone purchase?\n",
      "How far apart has the sun drifted from Alpha Centari due to the expansion of the universe since its formation?\n",
      "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
      "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n"
     ]
    }
   ],
   "source": [
    "text2_lines = text2.split('\\n')\n",
    "text2_processed = []\n",
    "for line in text2_lines:\n",
    "    if (len(line)) > 100:\n",
    "        text2_processed.append(line)\n",
    "\n",
    "text2_processed = '\\n'.join(text2_processed)\n",
    "print(text2_processed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Summarization\n",
    "- I/O\n",
    "    - Input: Text String\n",
    "    - Output: Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "openai_llm = OpenAI(\n",
    "    temperature = 0,\n",
    "    max_tokens = 1000,\n",
    "    model_name = 'text-davinci-003',\n",
    "    top_p = 1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text1.txt', 'r') as f:\n",
    "    text1_processed = f.read()\n",
    "\n",
    "with open('text2.txt', 'r') as f:\n",
    "    text2_processed = f.read()\n",
    "\n",
    "\n",
    "texts1 = text_splitter.split_text(text1_processed)\n",
    "texts2 = text_splitter.split_text(text2_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Come from: #url1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts1[0] + '\\n' + 'Come from: #url1 \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts1)):\n",
    "    texts1[i] = texts1[i] + '\\n' + 'Come from: #url1 \\n'\n",
    "    \n",
    "for i in range(len(texts2)):\n",
    "    texts2[i] = texts2[i] + '\\n' + 'Come from: #url2 \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts1 + texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = \"\"\"Write a summary of the following text:\n",
    "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
    "{text}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    template = summarize_template,\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "\n",
    "combine_template = \"\"\"Write a summary in detail of the following text:\n",
    "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
    "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
    "\n",
    "Example:\n",
    "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
    "\n",
    "text:\n",
    "<{text}>\n",
    "\n",
    "Summary (in detail):\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template = combine_template,\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm = openai_llm,\n",
    "    chain_type = \"map_reduce\",\n",
    "    map_prompt = summarize_prompt,\n",
    "    combine_prompt = combine_prompt,\n",
    "    combine_document_variable_name = \"text\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Several AI content writing tools currently use GPT-3, such as:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
      "BERT serves as the base for a number of services, like:\n",
      "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
      "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
      "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
      "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "They are unsupervised learning models (they don’t require labeled data for training).\n",
      "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
      "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
      "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
      "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
      "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
      "                                        Learn more about Stack Overflow the company, and our products.\n",
      "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
      "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
      "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
      "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
      "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
      "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
      "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
      "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
      "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
      "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
      "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
      "YA novel about a girl in dystopian society who gets surgically altered to be a double for a dead girl\n",
      "Is lying in an application for a job at a private company, signed under penalty of perjury, prosecutable as perjury?\n",
      "DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\". Something new or based on existing theories?\n",
      "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
      "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary in detail of the following text:\n",
      "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
      "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
      "\n",
      "Example:\n",
      "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
      "\n",
      "text:\n",
      "<This text compares two of the most popular language models in natural language processing (NLP): GPT-3 and BERT. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It is capable of generating human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is also a powerful language model, used for tasks such as sentiment analysis and question answering. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding (NLU). BERT serves as the base for a number of services. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. The main difference between them lies in their training datasets, with GPT-3 having access to 45TB of data and BERT having access to 3TB of data. GPT-3 is also significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they share some similarities such as using the Transformer architecture and attention mechanisms. [from url1]\n",
      "\n",
      "GPT-3 and BERT are unsupervised learning models that can be used for various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks that require more data, such as summarization or translation, while BERT is better suited for tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]\n",
      "\n",
      "GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks. However, due to their different architectures and training dataset sizes, each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. Ultimately, the choice between the two models depends on the specific needs of the user and the task they are trying to accomplish. [from url1]\n",
      "\n",
      "Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. In the BERT paper, it is learnt that BERT is an encoder-only model, involving only transformer encoder blocks. In the GPT paper, it is learnt that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]\n",
      "\n",
      "This text discusses the differences between GPT-2 and BERT, two models used for natural language processing. GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. Apart from technical differences in hyper-parameters, there are no other conceptual differences between the two models. [from url2]\n",
      "\n",
      "BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). This uses templates to exploit the language modeling abilities of BERT, such as for sentiment analysis. Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [from url2]\n",
      "\n",
      "This text discusses two topics: a YA novel about a girl in a dystopian society who gets surgically altered to be a double for a dead girl, and whether lying in an application for a job at a private company, signed under penalty of perjury, is prosecutable as perjury. It also mentions DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\" and the Stack Exchange website's policy on cookies. [from url2]>\n",
      "\n",
      "Summary (in detail):\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summary = map_reduce_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3498"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''' GPT-3 and BERT are two of the most popular language models used in natural language processing (NLP). This article compares the two models, exploring their capabilities and discussing the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data, and capable of generating human-like text. BERT is a transformer-based model that can be used for tasks such as question answering, summarization, and translation. [from url1]',\n",
    " ' GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model while BERT is bidirectional, taking into account both left and right context when making predictions, making it better suited for sentiment analysis or natural language understanding tasks. [from url1]',\n",
    " ' GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets using attention mechanisms. GPT-3 has 1.5 billion parameters and was trained on 45TB of data, while BERT has 340 million parameters and was trained on 3TB of data. GPT-3 is significantly larger than BERT due to its much more extensive training dataset size (470 times bigger than the one used to train BERT). [from url1]',\n",
    " ' GPT-3 and BERT are unsupervised learning models that can perform various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks such as summarization or translation due to its larger training dataset size, while BERT tends to do better in tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]',\n",
    " ' GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks, but each model is better suited for certain tasks than others. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. The choice between the two models depends on the specific needs of the task. [from url1]',\n",
    " ' Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. The BERT paper explains that BERT is an encoder-only model, while the GPT paper explains that GPT is a decoder-only model. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]',\n",
    " ' GPT2 and BERT have technical differences in hyper-parameters, but no other conceptual differences. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. [from url2]',\n",
    " ' BERT can be used for zero- or few-shot learning through PET (Pattern-Exploiting Training) which uses templates. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done with tutorials from Huggingface. [from url2]',\n",
    " ' This text discusses two topics: a YA novel about a girl in a dystopian society and the prosecutability of lying in an application for a job at a private company. It also mentions DeSantis\\' theory about the US Constitution\\'s \"leverage points\" and the use of cookies on Stack Exchange. [from url2]']''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3989"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('''This text compares two of the most popular language models in natural language processing (NLP): GPT-3 and BERT. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It is capable of generating human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is also a powerful language model, used for tasks such as sentiment analysis and question answering. [from url1]\n",
    "GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding (NLU). BERT serves as the base for a number of services. [from url1]\n",
    "GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. The main difference between them lies in their training datasets, with GPT-3 having access to 45TB of data and BERT having access to 3TB of data. GPT-3 is also significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they share some similarities such as using the Transformer architecture and attention mechanisms. [from url1]\n",
    "GPT-3 and BERT are unsupervised learning models that can be used for various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks that require more data, such as summarization or translation, while BERT is better suited for tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]\n",
    "GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks. However, due to their different architectures and training dataset sizes, each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. Ultimately, the choice between the two models depends on the specific needs of the user and the task they are trying to accomplish. [from url1]\n",
    "Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. In the BERT paper, it is learnt that BERT is an encoder-only model, involving only transformer encoder blocks. In the GPT paper, it is learnt that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]\n",
    "This text discusses the differences between GPT-2 and BERT, two models used for natural language processing. GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. Apart from technical differences in hyper-parameters, there are no other conceptual differences between the two models. [from url2]\n",
    "BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). This uses templates to exploit the language modeling abilities of BERT, such as for sentiment analysis. Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [from url2]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[GPT-3 and BERT] GPT-3 and BERT are two popular language models in natural language processing (NLP). GPT-3 is an autoregressive model trained on 45TB of text data, while BERT is a bidirectional transformer model trained on 3TB of data. GPT-3 is better for tasks that require more data, such as summarization or translation, while BERT is better for tasks such as sentiment analysis or natural language understanding (NLU). [url1]\\n\\n[PET] BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [url2]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\n\n",
    "[GPT-3 and BERT] GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data, and GPT-3 has 1.5 billion parameters while BERT has 340 million parameters. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. [url1]\\n\n",
    "\\n\n",
    "[PET] BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [url2]\\n\n",
    "\\n\n",
    "[Irrelevant Text] This text discusses three different topics: replacing weapon die rolls with a character's proficiency bonus, what happens when an insurance company does not mention a critical clause of a policy during a telephone purchase, and how far apart the sun has drifted from Alpha Centari due to the expansion of the universe since its formation. It also mentions the terms of service, privacy policy, and code of conduct for Stack Exchange, as well as the Cookie Policy. [url2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
