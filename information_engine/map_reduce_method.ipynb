{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Information Engine\n",
    "- Complete the information summarization with Map Reduce\n",
    "- I/O\n",
    "    - Input: text to summarization\n",
    "    - Output: Structured Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Langchain\n",
    "- Problem:\n",
    "    - Change to gpt-3.5-turbo\n",
    "    - Text Splitter using tiktoken\n",
    "    - Need to have seperate page so that we could add 'source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "openai_llm = OpenAI(\n",
    "    temperature = 0,\n",
    "    max_tokens = 1000,\n",
    "    model_name = 'text-davinci-003',\n",
    "    top_p = 1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text1.txt', 'r') as f:\n",
    "    text1_processed = f.read()\n",
    "\n",
    "with open('text2.txt', 'r') as f:\n",
    "    text2_processed = f.read()\n",
    "\n",
    "\n",
    "texts1 = text_splitter.split_text(text1_processed)\n",
    "texts2 = text_splitter.split_text(text2_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Come from: #url1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts1[0] + '\\n' + 'Come from: #url1 \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts1)):\n",
    "    texts1[i] = texts1[i] + '\\n' + 'Come from: #url1 \\n'\n",
    "    \n",
    "for i in range(len(texts2)):\n",
    "    texts2[i] = texts2[i] + '\\n' + 'Come from: #url2 \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts1 + texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = \"\"\"Write a summary of the following text:\n",
    "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
    "{text}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    template = summarize_template,\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "\n",
    "combine_template = \"\"\"Write a summary in detail of the following text:\n",
    "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
    "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
    "\n",
    "Example:\n",
    "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
    "\n",
    "text:\n",
    "<{text}>\n",
    "\n",
    "Summary (in detail):\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template = combine_template,\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm = openai_llm,\n",
    "    chain_type = \"map_reduce\",\n",
    "    map_prompt = summarize_prompt,\n",
    "    combine_prompt = combine_prompt,\n",
    "    combine_document_variable_name = \"text\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Several AI content writing tools currently use GPT-3, such as:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
      "BERT serves as the base for a number of services, like:\n",
      "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
      "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
      "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
      "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "They are unsupervised learning models (they don’t require labeled data for training).\n",
      "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
      "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
      "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
      "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
      "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
      "                                        Learn more about Stack Overflow the company, and our products.\n",
      "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
      "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
      "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
      "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
      "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
      "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
      "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
      "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
      "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
      "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
      "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
      "YA novel about a girl in dystopian society who gets surgically altered to be a double for a dead girl\n",
      "Is lying in an application for a job at a private company, signed under penalty of perjury, prosecutable as perjury?\n",
      "DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\". Something new or based on existing theories?\n",
      "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
      "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary in detail of the following text:\n",
      "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
      "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
      "\n",
      "Example:\n",
      "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
      "\n",
      "text:\n",
      "<This text compares two of the most popular language models in natural language processing (NLP): GPT-3 and BERT. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It is capable of generating human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is also a powerful language model, used for tasks such as sentiment analysis and question answering. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding (NLU). BERT serves as the base for a number of services. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. The main difference between them lies in their training datasets, with GPT-3 having access to 45TB of data and BERT having access to 3TB of data. GPT-3 is also significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they share some similarities such as using the Transformer architecture and attention mechanisms. [from url1]\n",
      "\n",
      "GPT-3 and BERT are unsupervised learning models that can be used for various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks that require more data, such as summarization or translation, while BERT is better suited for tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]\n",
      "\n",
      "GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks. However, due to their different architectures and training dataset sizes, each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. Ultimately, the choice between the two models depends on the specific needs of the user and the task they are trying to accomplish. [from url1]\n",
      "\n",
      "Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. In the BERT paper, it is learnt that BERT is an encoder-only model, involving only transformer encoder blocks. In the GPT paper, it is learnt that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]\n",
      "\n",
      "This text discusses the differences between GPT-2 and BERT, two models used for natural language processing. GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. Apart from technical differences in hyper-parameters, there are no other conceptual differences between the two models. [from url2]\n",
      "\n",
      "BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). This uses templates to exploit the language modeling abilities of BERT, such as for sentiment analysis. Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [from url2]\n",
      "\n",
      "This text discusses two topics: a YA novel about a girl in a dystopian society who gets surgically altered to be a double for a dead girl, and whether lying in an application for a job at a private company, signed under penalty of perjury, is prosecutable as perjury. It also mentions DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\" and the Stack Exchange website's policy on cookies. [from url2]>\n",
      "\n",
      "Summary (in detail):\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summary = map_reduce_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3498"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''' GPT-3 and BERT are two of the most popular language models used in natural language processing (NLP). This article compares the two models, exploring their capabilities and discussing the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data, and capable of generating human-like text. BERT is a transformer-based model that can be used for tasks such as question answering, summarization, and translation. [from url1]',\n",
    " ' GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model while BERT is bidirectional, taking into account both left and right context when making predictions, making it better suited for sentiment analysis or natural language understanding tasks. [from url1]',\n",
    " ' GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets using attention mechanisms. GPT-3 has 1.5 billion parameters and was trained on 45TB of data, while BERT has 340 million parameters and was trained on 3TB of data. GPT-3 is significantly larger than BERT due to its much more extensive training dataset size (470 times bigger than the one used to train BERT). [from url1]',\n",
    " ' GPT-3 and BERT are unsupervised learning models that can perform various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks such as summarization or translation due to its larger training dataset size, while BERT tends to do better in tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]',\n",
    " ' GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks, but each model is better suited for certain tasks than others. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. The choice between the two models depends on the specific needs of the task. [from url1]',\n",
    " ' Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. The BERT paper explains that BERT is an encoder-only model, while the GPT paper explains that GPT is a decoder-only model. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]',\n",
    " ' GPT2 and BERT have technical differences in hyper-parameters, but no other conceptual differences. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. [from url2]',\n",
    " ' BERT can be used for zero- or few-shot learning through PET (Pattern-Exploiting Training) which uses templates. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done with tutorials from Huggingface. [from url2]',\n",
    " ' This text discusses two topics: a YA novel about a girl in a dystopian society and the prosecutability of lying in an application for a job at a private company. It also mentions DeSantis\\' theory about the US Constitution\\'s \"leverage points\" and the use of cookies on Stack Exchange. [from url2]']''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3989"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('''This text compares two of the most popular language models in natural language processing (NLP): GPT-3 and BERT. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It is capable of generating human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is also a powerful language model, used for tasks such as sentiment analysis and question answering. [from url1]\n",
    "GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding (NLU). BERT serves as the base for a number of services. [from url1]\n",
    "GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. The main difference between them lies in their training datasets, with GPT-3 having access to 45TB of data and BERT having access to 3TB of data. GPT-3 is also significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they share some similarities such as using the Transformer architecture and attention mechanisms. [from url1]\n",
    "GPT-3 and BERT are unsupervised learning models that can be used for various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks that require more data, such as summarization or translation, while BERT is better suited for tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]\n",
    "GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks. However, due to their different architectures and training dataset sizes, each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. Ultimately, the choice between the two models depends on the specific needs of the user and the task they are trying to accomplish. [from url1]\n",
    "Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. In the BERT paper, it is learnt that BERT is an encoder-only model, involving only transformer encoder blocks. In the GPT paper, it is learnt that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]\n",
    "This text discusses the differences between GPT-2 and BERT, two models used for natural language processing. GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. Apart from technical differences in hyper-parameters, there are no other conceptual differences between the two models. [from url2]\n",
    "BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). This uses templates to exploit the language modeling abilities of BERT, such as for sentiment analysis. Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [from url2]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[GPT-3 and BERT] GPT-3 and BERT are two popular language models in natural language processing (NLP). GPT-3 is an autoregressive model trained on 45TB of text data, while BERT is a bidirectional transformer model trained on 3TB of data. GPT-3 is better for tasks that require more data, such as summarization or translation, while BERT is better for tasks such as sentiment analysis or natural language understanding (NLU). [url1]\\n\\n[PET] BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [url2]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\n\n",
    "[GPT-3 and BERT] GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data, and GPT-3 has 1.5 billion parameters while BERT has 340 million parameters. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. [url1]\\n\n",
    "\\n\n",
    "[PET] BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [url2]\\n\n",
    "\\n\n",
    "[Irrelevant Text] This text discusses three different topics: replacing weapon die rolls with a character's proficiency bonus, what happens when an insurance company does not mention a critical clause of a policy during a telephone purchase, and how far apart the sun has drifted from Alpha Centari due to the expansion of the universe since its formation. It also mentions the terms of service, privacy policy, and code of conduct for Stack Exchange, as well as the Cookie Policy. [url2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
