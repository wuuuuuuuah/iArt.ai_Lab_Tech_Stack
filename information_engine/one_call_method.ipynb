{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Call Information Engine\n",
    "- Complete the information summarization with one call\n",
    "- I/O\n",
    "    - Input: text to summarization\n",
    "    - Output: Structured Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_to_su.txt', 'r') as f:\n",
    "    text_to_su = f.read()\n",
    "# print(text_to_su)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(encoding.encode(text_to_su))\n",
    "print(num_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Call Information Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.organization = keys['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are a summary writer.\"\"\"\n",
    "\n",
    "instruction1 = \"\"\"Write a summary on those web text. You may make some comparisons on the text from different sources.\"\"\"\n",
    "instruction2 = \"Yes, I will write it with sequence number 1,2,3\"\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "<Web texts: (The content is from web so there might be some noise, you should ignore them)\n",
    "{text_to_su}>\n",
    "\n",
    "<Each point you conclude must include the source and output in Sequence, for example:\n",
    "1 \"Point a\"[url1]\\n\n",
    "1.1 \"Point b\"[url1][url2]\\n\n",
    "2 \"point c\"[url3]>\\n\n",
    "\n",
    "Summary: (Write in detail and with reference, remember to include the source)\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', \"content\": instruction1},\n",
    "        {\"role\": 'assistant', \"content\": instruction2},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    # max_tokens = 2000,\n",
    "    # stream = True\n",
    ")\n",
    "\n",
    "#for chunk in tur_response:\n",
    "#    try:\n",
    "#        print(chunk['choices'][0]['delta']['content'])\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7MApIcPRVqhgjSFEzP8z7FmCln3zF at 0x7fe28e765190> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"1. Both GPT and BERT are natural language processing (NLP) models that can perform a range of NLP tasks, like language generation, text classification, and question answering.\\n1.1 \\\"GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It is a language model that can generate natural language text, answer questions, compose poetry, and write complete articles. It has 175 billion parameters, making it one of the largest language models ever constructed.\\\" [url0]\\n1.2 \\\"BERT (Bidirectional Encoder Representations from Transformers) is a pre-training language representation model that fine-tunes NLP applications created by Google in 2018. BERT uses bidirectional flow, which allows it to use context from both directions during processing to understand the meaning of words in context, and, in turn, better comprehend language structures.\\\" [url0]\\n\\n2. GPT-3 is better suited for language generation tasks, while BERT is better for tasks that require understanding the meaning of text.\\n2.1 \\\"While GPT-3 can generate high-quality text in various styles, such as news articles, essays, and stories, BERT can encode the context of a word in a sentence and generate accurate results based on that context.\\\" [url1]\\n2.2 \\\"GPT-3 is better for language generation tasks such as article writing, story generation, and poem generation. However, GPT-3 may not be the best choice for tasks that require understanding the meaning of text.\\\" [url1]\\n2.3 \\\"BERT is better for NLP tasks that require understanding the meaning of text, such as sentiment analysis, text classification, and question answering.\\\" [url1]\\n\\n3. Both GPT-3 and BERT have their strengths and weaknesses, and choosing the right model for an NLP task depends on the specific requirements of the task.\\n3.1 \\\"Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\\\" [url2]\\n3.2 \\\"Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\\\" [url2]\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685519188,\n",
       "  \"id\": \"chatcmpl-7MApIcPRVqhgjSFEzP8z7FmCln3zF\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 474,\n",
       "    \"prompt_tokens\": 3322,\n",
       "    \"total_tokens\": 3796\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(response['choices'][0]['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
