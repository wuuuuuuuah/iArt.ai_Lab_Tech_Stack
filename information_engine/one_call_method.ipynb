{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Call Information Engine\n",
    "- Complete the information summarization with one call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 3021, 8415]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"I love cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n"
     ]
    }
   ],
   "source": [
    "with open('text1.txt', 'r') as f:\n",
    "    text1_content = f.read()\n",
    "\n",
    "with open('text2.txt', 'r') as f:\n",
    "    text2_content = f.read()\n",
    "\n",
    "\n",
    "num_tokens = len(encoding.encode(text1_content)) + len(encoding.encode(text2_content))\n",
    "\n",
    "print(num_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Original Call\n",
    "- prompt 1765 + completion 226 = \n",
    "    - 'gpt-3.5-turbo': 18s+14s+15s /3 = 16s\n",
    "    - 'text-davinci-003': above 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "openai.organization = keys['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "# openai.Model.list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo\n",
    "- Without Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "Write a concise summary on the content. You may make some comparisons on different source.\n",
    "The point you say must include the source, \n",
    "for example, [url1] or [url2]\"\"\"\n",
    "\n",
    "text = text1_content + '[from url1]' + text2_content + '[from url2]'\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "tur_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    temperature = 0,\n",
    ")\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The article compares two popular language models, GPT-3 and BERT, and discusses their differences and similarities, exploring their capabilities and the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, while BERT is a bidirectional transformer model developed by Google AI. The main difference between the two models lies in their architecture and training dataset size, with GPT-3 being better suited for tasks such as summarization or translation, while BERT is more beneficial for sentiment analysis or natural language understanding (NLU). However, both models have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1685508068,\n",
      "  \"id\": \"chatcmpl-7M7vwE8NQeN2sDGdZBmt35eFvgzdK\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 134,\n",
      "    \"prompt_tokens\": 1750,\n",
      "    \"total_tokens\": 1884\n",
      "  }\n",
      "}\n",
      "9.403381586074829s\n"
     ]
    }
   ],
   "source": [
    "print(str(tur_response) + '\\n' + str(stop - start) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article compares two popular language models, GPT-3 and BERT, and discusses their differences and similarities, exploring their capabilities and the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, while BERT is a bidirectional transformer model developed by Google AI. The main difference between the two models lies in their architecture and training dataset size, with GPT-3 being better suited for tasks such as summarization or translation, while BERT is more beneficial for sentiment analysis or natural language understanding (NLU). However, both models have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tur_response['choices'][0]['message']['content']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo\n",
    "- With Streaming\n",
    "    - use 'stream = True'\n",
    "    - automatically call 'await'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " article\n",
      " compares\n",
      " two\n",
      " popular\n",
      " language\n",
      " models\n",
      ",\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " and\n",
      " B\n",
      "ERT\n",
      ",\n",
      " discussing\n",
      " their\n",
      " differences\n",
      " and\n",
      " similarities\n",
      ",\n",
      " exploring\n",
      " their\n",
      " capabilities\n",
      ",\n",
      " and\n",
      " discussing\n",
      " some\n",
      " of\n",
      " the\n",
      " tools\n",
      " that\n",
      " use\n",
      " them\n",
      ".\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " is\n",
      " an\n",
      " aut\n",
      "ore\n",
      "gressive\n",
      " language\n",
      " model\n",
      " developed\n",
      " by\n",
      " Open\n",
      "AI\n",
      ",\n",
      " while\n",
      " B\n",
      "ERT\n",
      " is\n",
      " a\n",
      " bid\n",
      "irectional\n",
      " transformer\n",
      " model\n",
      " developed\n",
      " by\n",
      " Google\n",
      " AI\n",
      ".\n",
      " The\n",
      " main\n",
      " difference\n",
      " between\n",
      " the\n",
      " two\n",
      " models\n",
      " lies\n",
      " in\n",
      " their\n",
      " architecture\n",
      " and\n",
      " training\n",
      " dataset\n",
      " size\n",
      ",\n",
      " with\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " being\n",
      " better\n",
      " suited\n",
      " for\n",
      " tasks\n",
      " such\n",
      " as\n",
      " summar\n",
      "ization\n",
      " or\n",
      " translation\n",
      ",\n",
      " while\n",
      " B\n",
      "ERT\n",
      " is\n",
      " more\n",
      " beneficial\n",
      " for\n",
      " sentiment\n",
      " analysis\n",
      " or\n",
      " natural\n",
      " language\n",
      " understanding\n",
      " (\n",
      "N\n",
      "LU\n",
      ").\n",
      " However\n",
      ",\n",
      " both\n",
      " models\n",
      " have\n",
      " proven\n",
      " themselves\n",
      " valuable\n",
      " tools\n",
      " for\n",
      " performing\n",
      " various\n",
      " N\n",
      "LP\n",
      " tasks\n",
      " with\n",
      " varying\n",
      " degrees\n",
      " of\n",
      " accuracy\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\"\n",
    "Write a concise summary on the content. You may make some comparisons on different source.\n",
    "The point you say must include the source, \n",
    "for example, [url1] or [url2]\"\"\"\n",
    "\n",
    "text = text1_content + '[from url1]' + text2_content + '[from url2]'\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "tur_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    temperature = 0,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "for chunk in tur_response:\n",
    "    try:\n",
    "        print(chunk['choices'][0]['delta']['content'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text1_content + '[from url1]' + text2_content + '[from url2]'\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "Write a concise summary on the content. You may make some comparisons on different source.\n",
    "The point you say must include the source, \n",
    "for example, [url1] or [url2]\n",
    "content: {text}\n",
    "summary:\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "da_response = openai.Completion.create(\n",
    "    engine = 'text-davinci-003',\n",
    "    prompt = usr_prompt,\n",
    "    max_tokens = 300\n",
    ")\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7M6v1z4mPhssrfNtkAWmzPXFAdDk4 at 0x7f85c38ca150> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"GPT-3 and BERT are two well-known language models used in natural language processing (NLP) applications. GPT-3 is an autoregressive model trained on 45TB of data while BERT is bidirectional and trained on 3TB of data. They have significant differences in architecture and data sizes, but both are Transformer-based and can be used for similar tasks such as question answering, summarization, and translation. GPT-3 typically outperforms BERT in tasks that require more data, while BERT is better for sentiment analysis and natural language understanding. GPT-2 models can be used to generate data from prompts, while BERT can be used for zero- and few-shot learning with the PET method. [from url1 and url2]\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685504167,\n",
       "  \"id\": \"cmpl-7M6v1z4mPhssrfNtkAWmzPXFAdDk4\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 161,\n",
       "    \"prompt_tokens\": 1832,\n",
       "    \"total_tokens\": 1993\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-3 and BERT are two well-known language models used in natural language processing (NLP) applications. GPT-3 is an autoregressive model trained on 45TB of data while BERT is bidirectional and trained on 3TB of data. They have significant differences in architecture and data sizes, but both are Transformer-based and can be used for similar tasks such as question answering, summarization, and translation. GPT-3 typically outperforms BERT in tasks that require more data, while BERT is better for sentiment analysis and natural language understanding. GPT-2 models can be used to generate data from prompts, while BERT can be used for zero- and few-shot learning with the PET method. [from url1 and url2]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_response['choices'][0]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "- give text_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Content Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.makeuseof.com/gpt-vs-bert/', 'https://aitechtrend.com/gpt-3-vs-bert-which-one-wins-for-nlp-tasks/', 'https://blog.invgate.com/gpt-3-vs-bert', 'https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/']\n"
     ]
    }
   ],
   "source": [
    "with open('web_link.txt', 'r') as f:\n",
    "    web_link = f.readlines()\n",
    "\n",
    "web_link = [web.strip() for web in web_link]\n",
    "\n",
    "print(web_link)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "url_loader = UnstructuredURLLoader(\n",
    "    urls = web_link,\n",
    "    continue_on_failure = True,\n",
    "    mode = 'single'\n",
    ")\n",
    "\n",
    "url_data = url_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"ITSM\\nTicket Volume\\nWorkplace\\nProduct\\nCase Studies\\nGuides\\n\\n\\nMeet InvGate\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\nEspañol\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\nEspañol\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCategories\\n\\n\\n\\n\\nITSM\\n\\n\\n\\n\\n\\nWorkplace\\n\\n\\n\\n\\n\\nProduct\\n\\n\\n\\n\\n\\nCase Study\\n\\n\\n\\n\\n\\n\\n\\nLanguages\\n\\n\\nEnglish\\n \\nEspañol\\n\\nGPT-3 vs. BERT: Comparing the Two Most Popular Language Models\\n\\nCeleste Mottesi\\n            February 9, 2023\\n\\n3 min read\\n\\nNatural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\\n\\nHowever, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\\n\\nWhat is GPT-3?\\n\\nGPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\\n\\nExamples of AI-writing tools based on GPT-3\\n\\nSeveral AI content writing tools currently use GPT-3, such as:\\n\\nJasper\\n\\nChibiAI\\n\\nWriteSonic\\n\\nSimplified\\n\\nKafkai\\n\\nCopysmith\\n\\nWhat is BERT?\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\\n\\nBERT use cases\\n\\nBERT serves as the base for a number of services, like:\\n\\nGoogle Search Engine\\n\\nHuggingface Transformer Library\\n\\nMicrosoft Azure Cognitive Services\\n\\nGoogle Natural Language API\\n\\nDifferences between GPT-3 and BERT\\n\\nThe most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\\n\\nAnother difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\\n\\nFinally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\\n\\nSimilarities between GPT-3 and BERT\\n\\nDespite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\\n\\nThey use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\\n\\nThey are unsupervised learning models (they don’t require labeled data for training).\\n\\nThey can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\\n\\nGPT-3 vs. BERT: capabilities comparison\\n\\nBoth GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\\n\\nHowever, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\\n\\nOn other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\\n\\nConclusion\\n\\nThe bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\\n\\nFor example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\\n\\nRead other articles like this :\\n            \\n            AI\\n\\nRead other articles like this:\\n\\nEvaluate InvGate as Your ITSM Solution\\n30-day free trial - No credit card needed\\n\\nGet Started\\n\\nService Desk\\n\\nIT Service Management\\nEnterprise Service Management\\nTicket Management\\nSelf-Service\\nWorkflows\\n\\n\\n\\nITAM\\n\\nIT Asset Management\\nAsset Inventory\\nCMDB\\nNetwork Discovery\\nMetering\\n\\n\\n\\nLearn\\n\\nITSM\\nITIL\\nESM\\nSLAs\\nHelp Desks\\nTicket Volume\\n\\n\\n\\nInvGate\\n\\nAbout Us\\nWhy InvGate?\\nPartners\\nCareers\\nContact Us\\nLeave a Review\\n\\n\\n\\nCompare With\\n\\nJira\\nService Now\\nClarity\\nFreshservice\\nManage Engine\\nOther Alternatives\\n\\n\\n\\n\\n\\n\\n\\nPrivacy Policy\\nQuality Policy\\nCookie Policy\\nTerms and Conditions\\nSolutions Terms and Conditions\\n© InvGate\", metadata={'source': 'https://blog.invgate.com/gpt-3-vs-bert'}),\n",
       " Document(page_content='403 Forbidden', metadata={'source': 'https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/'}),\n",
       " Document(page_content='Stack Exchange Network\\n\\nStack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\\n\\nVisit Stack Exchange\\n\\nLoading…\\n\\n\\n\\nTour\\r\\n                                \\r\\n                                    Start here for a quick overview of the site\\n\\nHelp Center\\r\\n                            \\r\\n                                Detailed answers to any questions you might have\\n\\nMeta\\r\\n                                        \\r\\n                                            Discuss the workings and policies of this site\\n\\nAbout Us\\r\\n                                    \\r\\n                                        Learn more about Stack Overflow the company, and our products.\\n\\n\\n\\ncurrent community\\r\\n            \\r\\n            \\r\\n                \\r\\n                    \\r\\n                        \\r\\n                    \\r\\n                \\r\\n            \\r\\n        \\r\\n        \\r\\n            \\r\\n                    \\r\\n                            \\r\\n                \\r\\n        \\r\\n        \\r\\n            Data Science\\r\\n        \\r\\n    \\r\\n\\r\\n    \\r\\n    \\r\\n            help\\r\\n            chat\\r\\n    \\r\\n\\r\\n                    \\r\\n                    \\r\\n                            \\r\\n        \\r\\n    \\r\\n\\r\\n                            \\r\\n        \\r\\n        \\r\\n            Data Science Meta\\r\\n        \\r\\n    \\r\\n\\r\\n                    \\r\\n            \\r\\n        \\r\\n\\r\\n        \\r\\n            \\r\\nyour communities            \\r\\n\\r\\n        \\r\\n        \\r\\n\\r\\n                \\r\\nSign up or log in to customize your list.                \\r\\n        \\r\\n\\r\\n        \\r\\n            more stack exchange communities\\r\\n            \\r\\n            company blog\\n\\n\\n\\nLog in\\n\\nSign up\\n\\nData Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\\n\\nSign up to join this community\\n\\nAnybody can ask a question\\n\\nAnybody can answer\\n\\nThe best answers are voted up and rise to the top\\n\\nHome\\n\\nPublic\\r\\n\\r\\n                            \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    \\r\\n            Questions\\r\\n    \\r\\n\\r\\n\\r\\n                        \\r\\n\\r\\n                        \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    \\r\\n            \\r\\n                \\r\\n                    Tags\\r\\n                \\r\\n            \\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n                        \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    \\r\\n            \\r\\n                \\r\\n                    Users\\r\\n                \\r\\n            \\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n                            \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    \\r\\n            \\r\\n                \\r\\n                    Companies\\r\\n                \\r\\n            \\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n                            \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    \\r\\n            \\r\\n                \\r\\n                    Unanswered\\n\\nTeams\\r\\n\\r\\n    \\r\\n        Stack Overflow for Teams\\r\\n        – Start collaborating and sharing organizational knowledge.\\r\\n        \\r\\n        \\r\\n\\r\\n        Create a free Team\\r\\n        Why Teams?\\n\\nTeams\\n\\nCreate free Team\\n\\nTeams\\n\\nQ&A for work\\n\\nConnect and share knowledge within a single location that is structured and easy to search.\\n\\nLearn more about Teams\\n\\nBERT vs GPT architectural, conceptual and implemetational differences\\n\\nAsk Question\\n\\nAsked\\n\\nModified\\n\\n1 year, 6 months ago\\n\\nViewed\\r\\n                        6k times\\n\\n$\\\\begingroup$\\n\\nIn the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\\n\\nIn the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\\n\\nI was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\\n\\nBut I have following doubts:\\n\\nQ1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\\n\\nQ2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\\n\\nGpt2Model.forward does indeed contain\\n\\nBertModel.forward. So, I guess swapping out\\n\\nQ4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\\n\\nmachine-learning\\n\\nnlp\\n\\nbert\\n\\ntransformer\\n\\ngpt\\n\\nShare\\n\\nImprove this question\\n\\n\\r\\n        asked \\n\\nNov 26, 2021 at 21:22\\n\\nRnj\\n\\nRnj\\n\\n205\\n\\n2 silver badges\\n\\n7 bronze badges\\n\\n$\\\\endgroup$\\n\\nAdd a comment\\n\\n1 Answer\\r\\n                                    1\\n\\nReset to default\\n\\n$\\\\begingroup$\\n\\nTo start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\\n\\nBERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\\nExploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\\n\\nThen you check what score was would good and bad get at the position of the [MASK] token.\\n\\nWorking with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\\n\\nThe usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\\n\\nShare\\n\\nImprove this answer\\n\\n\\r\\n        answered \\n\\nNov 30, 2021 at 10:46\\n\\nJindřich\\n\\nJindřich\\n\\n1,631\\n\\n4 silver badges\\n\\n8 bronze badges\\n\\n$\\\\endgroup$\\n\\nAdd a comment\\n\\nYour Answer\\n\\nThanks for contributing an answer to Data Science Stack Exchange!\\n\\nPlease be sure to answer the question. Provide details and share your research!\\n\\nBut avoid …\\n\\nAsking for help, clarification, or responding to other answers.\\n\\nMaking statements based on opinion; back them up with references or personal experience.\\n\\nUse MathJax to format equations. MathJax reference.\\n\\nTo learn more, see our tips on writing great answers.\\n\\nDraft saved\\n\\nDraft discarded\\n\\nSign up or log in\\n\\nPost as a guest\\n\\nRequired, but never shown\\n\\nPost as a guest\\n\\nRequired, but never shown\\n\\nBy clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\\n\\nNot the answer you\\'re looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\\n\\nThe Overflow Blog\\n\\nMore on our AI future: building course recommendations and a new data platform\\n\\nThis product could help build a more equitable workplace (Ep. 575)\\n\\nFeatured on Meta\\n\\nAI/ML Tool examples part 3 - Title-Drafting Assistant\\n\\nWe are graduating the updated button styling for vote arrows\\n\\nMiddle Sidebar Advertising\\n\\nRelated\\n\\nHow Transformer is Bidirectional - Machine Learning\\n\\nDoes the transformer decoder reuse previous tokens\\' intermediate states like GPT2?\\n\\nTransformer-based architectures for regression tasks\\n\\nWhat is the difference between GPT blocks and Transformer Decoder blocks?\\n\\nWhat is the difference between GPT blocks and BERT blocks\\n\\nWhat are the inputs to the first decoder layer in a Transformer model during the training phase?\\n\\nChatGPT\\'s Architecture - Decoder Only? Or Encoder-Decoder?\\n\\nDo transformers (e.g. BERT) have an unlimited input size?\\n\\nHot Network Questions\\n\\nWhat would\\'ve given more accurate consonant ratios than 12?\\n\\nKazhdan-Lusztig theory for quantizations of symplectic resolutions/the rational Cherednik algebra?\\n\\nHow would you translate \"too\" and \"to\" as in \"The science is *too* bookish and nerdy *to* understand, oh no!\" to Latin?\\n\\nIs there a system of logic which denies DNI?\\n\\nWhat does the sentence \"it\\'s only dying a bit later than I would have, because I\\'m never going over to the Dark Side!\" mean, especially its last part?\\n\\nNon-rigorous proof in an article\\n\\nPythonic way for validating and categorizing user input\\n\\nCan this be a better way of defining subsets?\\n\\nRoots of this equation in x\\n\\nWhat is the gender of the word \"Haec\" in Latin?\\n\\nWhat are these keys on the БК-0010-series keyboards, and what do they do?\\n\\nHow do I go about apologizing to my ex-professor for putting her in an uncomfortable position?\\n\\nLogarithmic Incrementation\\n\\nDo stars become more metal-rich as they evolve?\\n\\nWhat is the time difference between Breath of the Wild and Tears of the Kingdom?\\n\\nComplex Gaussian wave packet visualized as a 3D\\n\\nEvaluate a function with two lists and table\\n\\nIdentify cluster of circular growths on tree branch\\n\\nSchrödinger equation in QFT\\n\\nA triangular puzzle\\n\\nAre legislators ever asked to explain their intent in Supreme Court cases?\\n\\nWhy does sending a trapped signal cause `read` to return in POSIX shell but not in Bash?\\n\\nCould a Nuclear-Thermal turbine keep a winged craft aloft on Titan at 5000m ASL?\\n\\nUsing two passports on single itinerary\\n\\nmore hot questions\\n\\nQuestion feed\\n\\nSubscribe to RSS\\n\\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\\n\\nData Science\\n\\nTour\\n\\nHelp\\n\\nChat\\n\\nContact\\n\\nFeedback\\n\\nCompany\\n\\nStack Overflow\\n\\nTeams\\n\\nAdvertising\\n\\nCollectives\\n\\nTalent\\n\\nAbout\\n\\nPress\\n\\nLegal\\n\\nPrivacy Policy\\n\\nTerms of Service\\n\\nCookie Settings\\n\\nCookie Policy\\n\\nStack Exchange Network\\n\\nTechnology\\n\\nCulture & recreation\\n\\nLife & arts\\n\\nScience\\n\\nProfessional\\n\\nBusiness\\n\\nAPI\\n\\nData\\n\\nBlog\\n\\nFacebook\\n\\nTwitter\\n\\nLinkedIn\\n\\nInstagram\\n\\nSite design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev\\xa02023.5.30.43465\\n\\nYour privacy\\n\\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.', metadata={'source': 'https://datascience.stackexchange.com/questions/104536/bert-vs-gpt-architectural-conceptual-and-implemetational-differences'})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using BeautifulSoup\n",
    "- Maybe Scrapy Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text_content_from_link(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the desired elements and extract their text content\n",
    "        text_content = ''\n",
    "        for element in soup.find_all('p'):  # Example: Extract text from all <p> tags\n",
    "            text_content += element.get_text() + '\\n'\n",
    "\n",
    "        return text_content\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from {url}. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Failed to retrieve content from https://symbl.ai/blog/gpt-3-versus-bert-a-high-level-comparison/. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "text_content = ''\n",
    "for i in range(len(web_link)):\n",
    "    try:\n",
    "        text_content += ''.join(['\\n',f'[this text is from url{i}]', '\\n', get_text_content_from_link(web_link[i])])\n",
    "        print(i)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.txt', 'w') as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "# print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3163\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(encoding.encode(text_content))\n",
    "print(num_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Call Information Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.organization = keys['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are a summary writer.\"\"\"\n",
    "\n",
    "prompt1 = \"\"\"Write a summary on those web text. You may make some comparisons on the text from different sources.\"\"\"\n",
    "prompt2 = \"Yes, I will write it with sequence number 1,2,3\"\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "<Web texts: (The content is from web so there might be some noise, you should ignore them)\n",
    "{text_content}>\n",
    "\n",
    "<Each point you conclude must include the source and output in Sequence, for example:\n",
    "1 \"Point a\"[url1]\\n\n",
    "1.1 \"Point b\"[url1][url2]\\n\n",
    "2 \"point c\"[url3]>\\n\n",
    "\n",
    "Summary: (Write in detail and with reference, remember to include the source)\n",
    "\"\"\"\n",
    "\n",
    "tur_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', \"content\": prompt1},\n",
    "        {\"role\": 'assistant', \"content\": prompt2},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    # max_tokens = 2000,\n",
    "    # stream = True\n",
    ")\n",
    "\n",
    "#for chunk in tur_response:\n",
    "#    try:\n",
    "#        print(chunk['choices'][0]['delta']['content'])\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7MApIcPRVqhgjSFEzP8z7FmCln3zF at 0x7fe28e765190> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"1. Both GPT and BERT are natural language processing (NLP) models that can perform a range of NLP tasks, like language generation, text classification, and question answering.\\n1.1 \\\"GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It is a language model that can generate natural language text, answer questions, compose poetry, and write complete articles. It has 175 billion parameters, making it one of the largest language models ever constructed.\\\" [url0]\\n1.2 \\\"BERT (Bidirectional Encoder Representations from Transformers) is a pre-training language representation model that fine-tunes NLP applications created by Google in 2018. BERT uses bidirectional flow, which allows it to use context from both directions during processing to understand the meaning of words in context, and, in turn, better comprehend language structures.\\\" [url0]\\n\\n2. GPT-3 is better suited for language generation tasks, while BERT is better for tasks that require understanding the meaning of text.\\n2.1 \\\"While GPT-3 can generate high-quality text in various styles, such as news articles, essays, and stories, BERT can encode the context of a word in a sentence and generate accurate results based on that context.\\\" [url1]\\n2.2 \\\"GPT-3 is better for language generation tasks such as article writing, story generation, and poem generation. However, GPT-3 may not be the best choice for tasks that require understanding the meaning of text.\\\" [url1]\\n2.3 \\\"BERT is better for NLP tasks that require understanding the meaning of text, such as sentiment analysis, text classification, and question answering.\\\" [url1]\\n\\n3. Both GPT-3 and BERT have their strengths and weaknesses, and choosing the right model for an NLP task depends on the specific requirements of the task.\\n3.1 \\\"Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\\\" [url2]\\n3.2 \\\"Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\\\" [url2]\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685519188,\n",
       "  \"id\": \"chatcmpl-7MApIcPRVqhgjSFEzP8z7FmCln3zF\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 474,\n",
       "    \"prompt_tokens\": 3322,\n",
       "    \"total_tokens\": 3796\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tur_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write(tur_response['choices'][0]['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
