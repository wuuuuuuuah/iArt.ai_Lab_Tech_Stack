{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to load your OPENAI_API_KEY into your environment\n",
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore Construction\n",
    "- I/O\n",
    "    - Input: text.txt\n",
    "    - Output: text_chunks (with document insides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text.split('++-------------------++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Change the text string into a Document object\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = []\n",
    "titles = []\n",
    "for text in texts:\n",
    "    titles.append(text.split('\\n')[0])\n",
    "    docs.append(Document(page_content=''.join(text.split('\\n')[1:])))\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[this text is from url0]', '[from pdf0]', '[from docx0]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135660\n",
      "32736\n"
     ]
    }
   ],
   "source": [
    "print(len(str(text)))\n",
    "print(len(encoding.encode(str(text))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter\n",
    "- I/O\n",
    "    - Input: text.txt\n",
    "    - Output: text_chunks (with document insides)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "- Using len() to count chunk_size -> 0.003s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003742218017578125\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "text_chunks = text_splitter.split_documents([text])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text_chunks), type(text_chunks[0]), len(text_chunks))\n",
    "for chunk in text_chunks:\n",
    "    print('---', chunk.page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TiktokenSplitter\n",
    "- Using tiktoken to count chunk_size -> 0.01s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import time\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018354177474975586\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "text_chunks = []\n",
    "for i in range(len(docs)):\n",
    "    chunks = token_text_splitter.split_documents([docs[i]])\n",
    "    for chunk in chunks:\n",
    "        chunk.page_content = f'{titles[i]}\\n' + chunk.page_content\n",
    "        text_chunks.append(chunk)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'langchain.schema.Document'> 158\n",
      "--- [this text is from url0]\n",
      "Pokémon[a][1][2][3] (an abbreviation for Pocket Monsters[b] in Japanese) is a Japanese media franchise managed by The Pokémon Company, founded by Nintendo, Game Freak, and Creatures. The franchise was created by Satoshi Tajiri in 1996,[4] and is centered around fictional creatures called \"Pokémon\". In Pokémon, Pokémon Trainers are people who catch, train, care for, and battle with Pokémon. The English slogan for the franchise is \"Gotta Catch 'Em All!\".[5][6] There are currently 1015 Pokémon species.[7]The franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996. Pokémon soon became a media mix franchise adapted into various different media.[8] Pokémon is one of the highest-grossing media franchises of all time. The Pokémon video game\n",
      "--- [this text is from url0]\n",
      " series is the third best-selling video game franchise of all time with more than 480 million copies sold[9] and one billion mobile downloads.[10] The Pokémon video game series spawned an anime television series that has become the most successful video game adaptation of all time[11] with over 20 seasons and 1,000 episodes in 192 countries.[9] The Pokémon Trading Card Game is the highest-selling trading card game of all time[12] with over 52.9 billion cards sold. In addition, the Pokémon franchise includes the world's top-selling toy brand,[13] an anime film series, a live-action film (Detective Pikachu), books, manga comics, music, merchandise, and a temporary theme park. The franchise is also represented in other Nintendo media, such as the Super Smash Bros. series, where various Pokémon characters appear as both playable and non-playable characters.The Pokémon franchise began as Pocket Monsters: Red and Green (later released outside of Japan\n",
      "--- [this text is from url0]\n",
      " as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996.[14] Pokémon soon became a media mix franchise adapted into various different media, with the Pokémon Trading Card Game released in October 1996, the Pokémon Adventures manga first released in Japan in March 1997, and the Pocket Monsters: Original Series released in April 1997. Pocket Pikachu was released in Japan in March 1998, with the first ever Pokémon film, Pokémon: The First Movie, first released in Japan in July 1998.[15]In 1998, Nintendo spent $25 million promoting Pokémon in the United States in partnership with Hasbro, KFC, and others.[16] Nintendo initially feared that Pokémon was too Japanese for Western tastes but Alfred Kahn, then CEO of 4Kids Entertainment, convinced the company otherwise.[17] The one who spotted Pokémon's potential in the United States was Kahn's colleague Thomas Kenney.[18]In November 2005, 4Kids Entertainment\n",
      "--- [this text is from url0]\n",
      ", which had managed the non-game related licensing of Pokémon, announced that it had agreed not to renew the Pokémon representation agreement. The Pokémon Company International oversees all Pokémon licensing outside Asia.[19] In 2006, the franchise celebrated its tenth anniversary with the release of Pokémon Diamond and Pearl.[20] In 2016, the Pokémon Company celebrated Pokémon's 20th anniversary by airing an ad during Super Bowl 50 in January and re-releasing the first Pokémon video games 1996 Game Boy games Pokémon Red, Green (only in Japan), and Blue, and the 1998 Game Boy Color game Pokémon Yellow for the Nintendo 3DS on February 26, 2016.[21][22] The mobile augmented reality game Pokémon Go was released in July 2016.[23] Pokémon Sun and Moon also released in the same year. The first live-action film in the franchise, Pokémon Detective Pikachu, based on the 2018 Nintendo 3DS spin-off game Detective Pikachu, was released in 2019.[24] The eighth generation of core series games began\n",
      "--- [this text is from url0]\n",
      " with Pokémon Sword and Shield, released worldwide on the Nintendo Switch on November 15, 2019.To celebrate its 25th anniversary, the company released two additional titles for the Nintendo Switch: Pokémon Brilliant Diamond and Shining Pearl, remakes of the Nintendo DS Pokémon Diamond and Pearl games, on November 19, 2021, and its \"premake\" Pokémon Legends: Arceus, which was subsequently released on January 28, 2022.[25][26]Pokémon Scarlet and Violet began the ninth generation of the game series when they released worldwide for the Nintendo Switch on November 18, 2022.[27]The name \"Pokémon\" is a syllabic abbreviation of the Japanese brand Pocket Monsters.[28] The term \"Pokémon\", in addition to referring to the Pokémon franchise itself, also collectively refers to the many fictional species that have made appearances in Pokémon media. \"Pokémon\" is identical in the singular and plural, as is each individual species name; it is and would be grammatically correct to say \"one Pokémon\"\n",
      "--- [this text is from url0]\n",
      " and \"many Pokémon\", as well as \"one Pikachu\" and \"many Pikachu\".[29]Pokémon executive director Satoshi Tajiri first thought of Pokémon, albeit with a different concept and name, around 1989, when the Game Boy was released. The concept of the Pokémon universe, in both the video games and the general fictional world of Pokémon, stems from the hobby of insect collecting, a popular pastime which Tajiri enjoyed as a child.[30] Players are designated as Pokémon Trainers and have three general goals: to complete the regional Pokédex by collecting all of the available Pokémon species found in the fictional region where a game takes place, to complete the national Pokédex by transferring Pokémon from other regions, and to train a team of powerful Pokémon from those they have caught to compete against teams owned by other Trainers so they may eventually win the Pokémon League and become the regional Champion. These themes of collecting, training, and battling are present in almost every version of the Pokémon franchise, including\n",
      "--- [this text is from url0]\n",
      " the video games, the anime and manga series, and the Pokémon Trading Card Game (also known as TCG).In most incarnations of the Pokémon universe, a Trainer who encounters a wild Pokémon has the ability to capture that Pokémon by throwing a specially designed, mass-producible spherical tool called a Poké Ball at it. If the Pokémon is unable to escape the confines of the Poké Ball, it is considered to be under the ownership of that Trainer. Afterwards, it will obey whatever commands it receives from its new Trainer, unless the Trainer demonstrates such a lack of experience that the Pokémon would rather act on its own accord. Trainers can send out any of their Pokémon to wage non-lethal battles against other Pokémon; if the opposing Pokémon is wild, the Trainer can capture that Pokémon with a Poké Ball, increasing their collection of creatures. In Pokémon Go, and in Pokémon: Let's Go, Pikachu! and Let's Go, Eevee!, wild Pokémon encountered by players can be caught\n",
      "--- [this text is from url0]\n",
      " in Poké Balls, but most cannot be battled. Pokémon already owned by other Trainers cannot be captured, except under special circumstances in certain side games. If a Pokémon fully defeats an opponent in battle so that the opponent is knocked out (\"faints\"), the winning Pokémon gains experience points and may level up. Beginning with Pokémon X and Y, experience points are also gained from catching Pokémon in Poké Balls. When leveling up, the Pokémon's battling aptitude statistics (\"stats\", such as \"Attack\" and \"Speed\") increase. At certain levels, the Pokémon may also learn new moves, which are techniques used in battle. In addition, many species of Pokémon can undergo a form of metamorphosis and transform into a similar but stronger species of Pokémon, a process called evolution; this process occurs spontaneously under differing circumstances, and is itself a central theme of the series. Some species of Pokémon may undergo a maximum of two evolutionary transformations, while others may undergo only one, and others may not evolve at all\n",
      "--- [this text is from url0]\n",
      ". For example, the Pokémon Pichu may evolve into Pikachu, which in turn may evolve into Raichu, following which no further evolutions may occur. Pokémon X and Y introduced the concept of \"Mega Evolution,\" by which certain fully evolved Pokémon may temporarily undergo an additional evolution into a stronger form for the purpose of battling; this evolution is considered a special case, and unlike other evolutionary stages, is reversible.In the main series, each game's single-player mode requires the Trainer to raise a team of Pokémon to defeat many non-player character (NPC) Trainers and their Pokémon. Each game lays out a somewhat linear path through a specific region of the Pokémon world for the Trainer to journey through, completing events and battling opponents along the way (including foiling the plans of an evil team of Pokémon Trainers who serve as antagonists to the player). Excluding Pokémon Sun and Moon and Pokémon Ultra Sun and Ultra Moon, the games feature eight powerful Trainers, referred to as\n",
      "--- [this text is from url0]\n",
      " Gym Leaders, that the Trainer must defeat in order to progress. As a reward, the Trainer receives a Gym Badge, and once all eight badges are collected, the Trainer is eligible to challenge the region's Pokémon League, where four talented trainers (referred to collectively as the \"Elite Four\") challenge the Trainer to four Pokémon battles in succession. If the trainer can overcome this gauntlet, they must challenge the Regional Champion, the master Trainer who had previously defeated the Elite Four. Any Trainer who wins this last battle becomes the new champion.Pokémon is set in the fictional Pokémon universe. There are numerous regions that have appeared in the various media of the Pokémon franchise. There are 9 main series regions set in the main series games: Kanto, Johto, Hoenn, Sinnoh/Hisui, Unova, Kalos, Alola, Galar, and Paldea. Each of the nine generations of the main series releases focuses on a new region. Every region consists of several\n",
      "--- [this text is from url0]\n",
      " cities and towns that the player must explore in order to overcome many waiting challenges, such as Gyms, Contests and villainous teams. At different locations within each region, the player can find different types of Pokémon, as well as helpful items and characters. Different regions are not accessible from one another at all within a single game, only with the exception of Kanto and Johto being linked together in Pokémon Gold, Silver, Crystal, HeartGold and SoulSilver versions. There are also regions set in spinoff games and two islands in the Pokémon anime (Orange Islands and Decolore Islands), all still set within the same fictional universe.Each main series region in the Pokémon universe is based on a real world location. The first four regions introduced are based on locations in Japan, being Kantō, Kansai, Kyushu, and Hokkaidō, with later regions being based on parts of New York City, France, Hawaii, the United Kingdom, and the Iberian\n",
      "--- [this text is from url0]\n",
      " Peninsula.[31][32]All of the licensed Pokémon properties overseen by the Pokémon Company International are divided roughly by generation. These generations are roughly chronological divisions by release; every several years, when a sequel to the 1996 role-playing video games Pokémon Red and Green is released that features new Pokémon, characters, and gameplay concepts, that sequel is considered the start of a new generation of the franchise. The main Pokémon video games and their spin-offs, the anime, manga, and trading card game are all updated with the new Pokémon properties each time a new generation begins.[33] Some Pokémon from the newer games appear in anime episodes or films months, or even years, before the game they were programmed for came out. The first generation began in Japan with Pokémon Red and Green on the Game Boy. As of 2022, there are nine generations of main series video games. The most recent games in the main series, Pokémon Scarlet and Violet began the ninth and latest generation when they released worldwide for the Nintendo\n",
      "--- [this text is from url0]\n",
      " Switch on November 18, 2022.[34][35][36]Kanto regionJohto regionKanto region Hoenn regionKanto regionSinnoh regionJohto regionKanto regionUnova regionKalos regionHoenn regionAlola regionKanto regionGalar regionSinnoh/Hisui regionPaldea regionPokémon, also known as Pokémon the Series to Western audiences since the year 2013, is an anime television series based on the Pokémon video game series. It was originally broadcast on TV Tokyo in 1997. More than 1,200 episodes of the anime has been produced and aired,[42] divided into 8 series in Japan and 26 seasons internationally. It is one of the longest currently running anime series.[42]The anime follows the quest of the main character, Ash Ketchum, a Pokémon Trainer, as he and a small group of friends travel around the world of Pokémon along with their Pokémon partners.[43]Various children's books, collectively known as Pokémon Junior\n",
      "--- [this text is from url0]\n",
      ", are also based on the anime.[44]An eight-part anime series called Pokémon: Twilight Wings aired on YouTube in 2020.[45] The series was animated by Studio Colorido.[46]An eight part anime series in celebration of the Pokémon 25th anniversary called Pokémon Evolutions aired on YouTube in 2021.[47]In July 2021, it was announced that a live action Pokémon series is in early development at Netflix with Joe Henderson attached to write and executive produce.[48]There have been 23 animated theatrical Pokémon films, which have been directed by Kunihiko Yuyama and Tetsuo Yajima, and distributed in Japan by Toho since 1998. The pair of films, Pokémon the Movie: Black—Victini and Reshiram and White—Victini and Zekrom are considered together as one film. Collectibles, such as promotional trading cards, have been available with some of the films. Since the 20th film, the films have been set in an alternate\n",
      "--- [this text is from url0]\n",
      " continuity separate from the anime series.Pokémon CDs have been released in North America, some of them in conjunction with the theatrical releases of the first three and the 20th Pokémon films. These releases were commonplace until late 2001. On March 27, 2007, a tenth anniversary CD was released containing 18 tracks from the English dub; this was the first English-language release in over five years. Soundtracks of the Pokémon feature films have been released in Japan each year in conjunction with the theatrical releases. In 2017, a soundtrack album featuring music from the North American versions of the 17th through 20th movies was released.^  The exact date of release is unknown.^  Featuring music from Pokémon the Movie: Diancie and the Cocoon of Destruction, Pokémon the Movie: Hoopa and the Clash of Ages, Pokémon the Movie: Volcanion and the Mechanical Marvel, and Pokémon the Movie: I Choose You! The Pokémon Trading Card Game (TCG) is a collectible card game with\n",
      "--- [this text is from url0]\n",
      " a goal similar to a Pokémon battle in the video game series. Players use Pokémon cards, with individual strengths and weaknesses, in an attempt to defeat their opponent by \"knocking out\" their Pokémon cards.[51] The game was published in North America by Wizards of the Coast in 1999.[52] With the release of the Game Boy Advance video games Pokémon Ruby and Sapphire, the Pokémon Company took back the card game from Wizards of the Coast and started publishing the cards themselves.[52] The Expedition expansion introduced the Pokémon-e Trading Card Game, where the cards (for the most part) were compatible with the Nintendo e-Reader. Nintendo discontinued its production of e-Reader compatible cards with the release of FireRed and LeafGreen. In 1998, Nintendo released a Game Boy Color version of the trading card game in Japan; Pokémon Trading Card Game was subsequently released to the US and Europe in 2000. The game included digital versions of cards from the original set of cards and the first two expansions (J\n",
      "--- [this text is from url0]\n",
      "ungle and Fossil), as well as several cards exclusive to the game. A sequel was released in Japan in 2001.[53]There are various Pokémon manga series, four of which were released in English by Viz Media, and seven of them released in English by Chuang Yi. The manga series vary from game-based series to being based on the anime and the Trading Card Game. Original stories have also been published. As there are several series created by different authors, most Pokémon manga series differ greatly from each other and other media, such as the anime.[example  needed] Pokémon Pocket Monsters and Pokémon Adventures are the two manga in production since the first generation.In July 2021, it was announced that a live-action Pokémon series is reportedly in development at Netflix. Joe Henderson, showrunner of Lucifer, is signed on as writer and executive producer.[62]Pokémon has been criticized by some fundamentalist Christians over perceived occult and violent themes and the concept of \"Pokémon evolution\", which they feel goes against\n",
      "--- [this text is from url0]\n",
      " the Biblical creation account in Genesis.[63] Sat2000, a satellite television station based in Vatican City, has countered that the Pokémon Trading Card Game and video games are \"full of inventive imagination\" and have no \"harmful moral side effects\".[64][65] In the United Kingdom, the \"Christian Power Cards\" game was introduced in 1999 by David Tate who stated, \"Some people aren't happy with Pokémon and want an alternative, others just want Christian games.\" The game was similar to the Pokémon Trading Card Game but used Biblical figures.[66]In 1999, Nintendo stopped manufacturing the Japanese version of the \"Koga's Ninja Trick\" trading card because it depicted a manji, a traditionally Buddhist symbol with no negative connotations. The Jewish civil rights group Anti-Defamation League complained because the symbol is the reverse of a swastika, a Nazi symbol. The cards were intended for sale in Japan only, but the popularity of Pokémon led to import into the United States with approval from Nintendo\n",
      "--- [this text is from url0]\n",
      ". The Anti-Defamation League understood that the portrayed symbol was not intended to offend and acknowledged the sensitivity that Nintendo showed by removing the product.[67][68]In 1999, two nine-year-old boys from Merrick, New York, sued Nintendo because they claimed the Pokémon Trading Card Game caused their problematic gambling.[69]In 2001, Saudi Arabia banned Pokémon games and the trading cards, alleging that the franchise promoted Zionism by displaying the Star of David in the trading cards (the Colorless energy from the Pokémon Trading Card Game resembles a six-pointed star) as well as other religious symbols such as crosses they associated with Christianity and triangles they associated with Freemasonry; the games also involved gambling, which is in violation of Muslim doctrine.[70][71]Pokémon has also been accused of promoting materialism.[72]In 2012, PETA criticized the concept of Pokémon as supporting cruelty to animals. PETA compared the game's concept, of capturing animals and forcing them to fight, to\n",
      "--- [this text is from url0]\n",
      " cockfights, dog fighting rings and circuses, events frequently criticized for cruelty to animals. PETA released a game spoofing Pokémon where the Pokémon battle their trainers to win their freedom.[73] PETA reaffirmed their objections in 2016 with the release of Pokémon Go, promoting the hashtag #GottaFreeThemAll.[74]On December 16, 1997, more than 635 Japanese children were admitted to hospitals with epileptic seizures.[75] It was determined the seizures were caused by watching an episode of Pokémon \"Dennō Senshi Porygon\", (most commonly translated \"Electric Soldier Porygon\", season 1, episode 38); as a result, this episode has not been aired since. In this particular episode, there were bright explosions with rapidly alternating blue and red color patterns.[76] It was determined in subsequent research that these strobing light effects cause some individuals to have epileptic seizures, even if the person had no previous history of epilepsy.[77] This incident is a\n",
      "--- [this text is from url0]\n",
      " common focus of Pokémon-related parodies in other media, and was lampooned by The Simpsons episode \"Thirty Minutes over Tokyo\" in a short cameo[78] and the South Park episode \"Chinpokomon\",[79] among others.Within its first two days of release, Pokémon Go raised safety concerns among players. Multiple people also suffered minor injuries from falling while playing the game due to being distracted.[80]Multiple police departments in various countries have issued warnings, some tongue-in-cheek, regarding inattentive driving, trespassing, and being targeted by criminals due to being unaware of one's surroundings.[81][82] People have suffered various injuries from accidents related to the game,[83][84][85][86] and Bosnian players have been warned to stay out of minefields left over from the 1990s Bosnian War.[87] On July 20, 2016, it was reported that an 18-year-old boy in Chiquimula\n",
      "--- [this text is from url0]\n",
      ", Guatemala, was shot and killed while playing the game in the late evening hours. This was the first reported death in connection with the app. The boy's 17-year-old cousin, who was accompanying the victim, was shot in the foot. Police speculated that the shooters used the game's GPS capability to find the two.[88]Pokémon, being a globally popular franchise, has left a significant mark on today's popular culture. The various species of Pokémon have become pop culture icons; examples include two different Pikachu balloons in the Macy's Thanksgiving Day Parade, Pokémon-themed airplanes operated by All Nippon Airways, merchandise items, and a traveling theme park that was in Nagoya, Japan in 2005 and in Taipei in 2006. Pokémon also appeared on the cover of the U.S. magazine Time in 1999.[89] The Comedy Central show Drawn Together has a character named Ling-Ling who is a parody of Pikachu.[90] Several other shows such as The Simpsons[91\n",
      "--- [this text is from url0]\n",
      "] South Park[92] and Robot Chicken[93] and The Flash have made references and spoofs of Pokémon, among other series. Pokémon was featured on VH1's I Love the '90s: Part Deux. A live action show based on the anime called Pokémon Live! toured the United States in late 2000.[94] Jim Butcher cites Pokémon as one of the inspirations for the Codex Alera series of novels.[95]Pokémon has even made its mark in the realm of science. This includes animals named after Pokémon, such as Stentorceps weedlei (named after the Pokémon Weedle for its resemblance) and Chilicola charizard (named after the Pokémon Charizard) as well as Binburrum articuno, Binburrum zapdos, and Binburrum moltres (named after the Pokémon Articuno, Zapdos, and Moltres, respectively).[96][97] There is also a protein named after Pikachu, called Pik\n",
      "--- [this text is from url0]\n",
      "achurin.In November 2001, Nintendo opened a store called the Pokémon Center in New York, in Rockefeller Center,[98] modeled after the two other Pokémon Center stores in Tokyo and Osaka and named after a staple of the video game series. Pokémon Centers are fictional buildings where Trainers take their injured Pokémon to be healed after combat.[99] The store sold Pokémon merchandise on a total of two floors, with items ranging from collectible shirts to stuffed Pokémon plushies.[100] The store also featured a Pokémon Distributing Machine in which players would place their game to receive an egg of a Pokémon that was being given out at that time. The store also had tables that were open for players of the Pokémon Trading Card Game to duel each other or an employee. The store was closed and replaced by the Nintendo World Store on May 14, 2005.[101] Four Pokémon Center kiosks were put in malls in the Seattle area.[102] The Pokémon Center online store was relaunched on August 6,\n",
      "--- [this text is from url0]\n",
      " 2014.[103]Professor of education Joseph Tobin theorizes that the success of the franchise was due to the long list of names that could be learned by children and repeated in their peer groups. Its rich fictional universe provides opportunities for discussion and demonstration of knowledge in front of their peers. The names of the creatures were linked to its characteristics, which converged with the children's belief that names have symbolic power. Children can pick their favourite Pokémon and affirm their individuality while at the same time affirming their conformance to the values of the group, and they can distinguish themselves from others by asserting what they liked and what they did not like from every chapter. Pokémon gained popularity because it provides a sense of identity to a wide variety of children, and lost it quickly when many of those children found that the identity groups were too big and searched for identities that would distinguish them into smaller groups.[104][page needed]Pokémon's history has been marked at times by rivalry with the Digimon media franchise that\n",
      "--- [this text is from url0]\n",
      " debuted at a similar time. Described as \"the other 'mon'\" by IGN's Juan Castro, Digimon has not enjoyed Pokémon's level of international popularity or success, but has maintained a dedicated fanbase.[105] IGN's Lucas M. Thomas stated that Pokémon is Digimon's \"constant competition and comparison\", attributing the former's relative success to the simplicity of its evolution mechanic as opposed to Digivolution.[106] The two have been noted for conceptual and stylistic similarities by sources such as GameZone.[107] A debate among fans exists over which of the two franchises came first.[108] In actuality, the first Pokémon media, Pokémon Red and Green, were released initially on February 27, 1996;[109] whereas the Digimon virtual pet was released on June 26, 1997.While Pokémon's target demographic is children, early purchasers of Pokémon Omega Ruby and Alpha Sapphire were in their 20s.[110] Many fans are adults who originally played the games as children\n",
      "--- [this text is from url0]\n",
      " and had later returned to the series.[110]Numerous fan sites exist for the Pokémon franchise, including Bulbagarden, a site hosting the wiki-based encyclopedia Bulbapedia,[111][112][113] and Serebii,[114] a news and reference website.[115] Large fan communities exist on other platforms, such as the subreddit r/pokemon, which has over 4 million subscribers.[116]A significant community around the Pokémon video games' metagame has existed for a long time, analyzing the best ways to use each Pokémon to their full potential in competitive battles. The most prolific competitive community is Smogon University, which has created a widely accepted tier-based battle system.[117]Smogon is affiliated with an online Pokémon game called Pokémon Showdown, in which players create a team and battle against other players around the world using the competitive tiers created by Smogon.[118]In early 2014, an anonymous video streamer on Twitch launched Twitch Plays Pokémon,\n",
      "--- [this text is from url0]\n",
      " a small experiment trying to crowdsource playing subsequent Pokémon games, that started with the game Pokémon Red and has since included subsequent games in the series.[119][120]A study at Stanford Neurosciences published in Nature performed magnetic resonance imaging scans of 11 Pokémon experts and 11 controls, finding that seeing Pokémon stimulated activity in the visual cortex, in a different place than is triggered by recognizing faces, places, or words, demonstrating the brain's ability to create such specialized areas.[121][122]Pokémon[a][1][2][3] (an abbreviation for Pocket Monsters[b] in Japanese) is a Japanese media franchise managed by The Pokémon Company, founded by Nintendo, Game Freak, and Creatures. The franchise was created by Satoshi Tajiri in 1996,[4] and is centered around fictional creatures called \"Pokémon\". In Pokémon, Pokémon Trainers are people who catch, train, care for, and battle with Pokémon. The English slogan for the franchise is \"Gotta Catch 'Em All\n",
      "--- [this text is from url0]\n",
      "!\".[5][6] There are currently 1015 Pokémon species.[7]The franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996. Pokémon soon became a media mix franchise adapted into various different media.[8] Pokémon is one of the highest-grossing media franchises of all time. The Pokémon video game series is the third best-selling video game franchise of all time with more than 480 million copies sold[9] and one billion mobile downloads.[10] The Pokémon video game series spawned an anime television series that has become the most successful video game adaptation of all time[11] with over 20 seasons and 1,000 episodes in 192 countries.[9] The Pokémon Trading Card Game is the highest-selling trading card game of all time[12] with over 52.9 billion cards sold. In\n",
      "--- [this text is from url0]\n",
      " addition, the Pokémon franchise includes the world's top-selling toy brand,[13] an anime film series, a live-action film (Detective Pikachu), books, manga comics, music, merchandise, and a temporary theme park. The franchise is also represented in other Nintendo media, such as the Super Smash Bros. series, where various Pokémon characters appear as both playable and non-playable characters.The Pokémon franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996.[14] Pokémon soon became a media mix franchise adapted into various different media, with the Pokémon Trading Card Game released in October 1996, the Pokémon Adventures manga first released in Japan in March 1997, and the Pocket Monsters: Original Series released in April 1997. Pocket Pikachu was released in Japan in March 1998, with the first ever Pokémon film, Pokémon: The\n",
      "--- [this text is from url0]\n",
      " First Movie, first released in Japan in July 1998.[15]In 1998, Nintendo spent $25 million promoting Pokémon in the United States in partnership with Hasbro, KFC, and others.[16] Nintendo initially feared that Pokémon was too Japanese for Western tastes but Alfred Kahn, then CEO of 4Kids Entertainment, convinced the company otherwise.[17] The one who spotted Pokémon's potential in the United States was Kahn's colleague Thomas Kenney.[18]In November 2005, 4Kids Entertainment, which had managed the non-game related licensing of Pokémon, announced that it had agreed not to renew the Pokémon representation agreement. The Pokémon Company International oversees all Pokémon licensing outside Asia.[19] In 2006, the franchise celebrated its tenth anniversary with the release of Pokémon Diamond and Pearl.[20] In 2016, the Pokémon Company celebrated Pokémon's 20th anniversary by airing an ad during Super Bowl 50 in January and re-releasing the first Pokémon video games 1996 Game Boy games Pokémon Red, Green (only in\n",
      "--- [this text is from url0]\n",
      " Japan), and Blue, and the 1998 Game Boy Color game Pokémon Yellow for the Nintendo 3DS on February 26, 2016.[21][22] The mobile augmented reality game Pokémon Go was released in July 2016.[23] Pokémon Sun and Moon also released in the same year. The first live-action film in the franchise, Pokémon Detective Pikachu, based on the 2018 Nintendo 3DS spin-off game Detective Pikachu, was released in 2019.[24] The eighth generation of core series games began with Pokémon Sword and Shield, released worldwide on the Nintendo Switch on November 15, 2019.To celebrate its 25th anniversary, the company released two additional titles for the Nintendo Switch: Pokémon Brilliant Diamond and Shining Pearl, remakes of the Nintendo DS Pokémon Diamond and Pearl games, on November 19, 2021, and its \"premake\" Pokémon Legends: Arceus, which was subsequently released on January 28, 2022.[25][26]Pokémon Scarlet and Violet began the ninth generation of the game series when they\n",
      "--- [this text is from url0]\n",
      " released worldwide for the Nintendo Switch on November 18, 2022.[27]The name \"Pokémon\" is a syllabic abbreviation of the Japanese brand Pocket Monsters.[28] The term \"Pokémon\", in addition to referring to the Pokémon franchise itself, also collectively refers to the many fictional species that have made appearances in Pokémon media. \"Pokémon\" is identical in the singular and plural, as is each individual species name; it is and would be grammatically correct to say \"one Pokémon\" and \"many Pokémon\", as well as \"one Pikachu\" and \"many Pikachu\".[29]Pokémon executive director Satoshi Tajiri first thought of Pokémon, albeit with a different concept and name, around 1989, when the Game Boy was released. The concept of the Pokémon universe, in both the video games and the general fictional world of Pokémon, stems from the hobby of insect collecting, a popular pastime which Tajiri enjoyed as a child.[30] Players are designated as Pokémon Trainers and have three general goals\n",
      "--- [this text is from url0]\n",
      ": to complete the regional Pokédex by collecting all of the available Pokémon species found in the fictional region where a game takes place, to complete the national Pokédex by transferring Pokémon from other regions, and to train a team of powerful Pokémon from those they have caught to compete against teams owned by other Trainers so they may eventually win the Pokémon League and become the regional Champion. These themes of collecting, training, and battling are present in almost every version of the Pokémon franchise, including the video games, the anime and manga series, and the Pokémon Trading Card Game (also known as TCG).In most incarnations of the Pokémon universe, a Trainer who encounters a wild Pokémon has the ability to capture that Pokémon by throwing a specially designed, mass-producible spherical tool called a Poké Ball at it. If the Pokémon is unable to escape the confines of the Poké Ball, it is considered to be under the ownership of that Trainer. Afterwards, it will obey whatever commands it receives from\n",
      "--- [this text is from url0]\n",
      " its new Trainer, unless the Trainer demonstrates such a lack of experience that the Pokémon would rather act on its own accord. Trainers can send out any of their Pokémon to wage non-lethal battles against other Pokémon; if the opposing Pokémon is wild, the Trainer can capture that Pokémon with a Poké Ball, increasing their collection of creatures. In Pokémon Go, and in Pokémon: Let's Go, Pikachu! and Let's Go, Eevee!, wild Pokémon encountered by players can be caught in Poké Balls, but most cannot be battled. Pokémon already owned by other Trainers cannot be captured, except under special circumstances in certain side games. If a Pokémon fully defeats an opponent in battle so that the opponent is knocked out (\"faints\"), the winning Pokémon gains experience points and may level up. Beginning with Pokémon X and Y, experience points are also gained from catching Pokémon in Poké Balls. When leveling up, the Pokémon's battling aptitude statistics (\"stats\", such as \"Attack\" and \"Speed\n",
      "--- [this text is from url0]\n",
      "\") increase. At certain levels, the Pokémon may also learn new moves, which are techniques used in battle. In addition, many species of Pokémon can undergo a form of metamorphosis and transform into a similar but stronger species of Pokémon, a process called evolution; this process occurs spontaneously under differing circumstances, and is itself a central theme of the series. Some species of Pokémon may undergo a maximum of two evolutionary transformations, while others may undergo only one, and others may not evolve at all. For example, the Pokémon Pichu may evolve into Pikachu, which in turn may evolve into Raichu, following which no further evolutions may occur. Pokémon X and Y introduced the concept of \"Mega Evolution,\" by which certain fully evolved Pokémon may temporarily undergo an additional evolution into a stronger form for the purpose of battling; this evolution is considered a special case, and unlike other evolutionary stages, is reversible.In the main series, each game's single-player mode requires the Trainer to raise a\n",
      "--- [this text is from url0]\n",
      " team of Pokémon to defeat many non-player character (NPC) Trainers and their Pokémon. Each game lays out a somewhat linear path through a specific region of the Pokémon world for the Trainer to journey through, completing events and battling opponents along the way (including foiling the plans of an evil team of Pokémon Trainers who serve as antagonists to the player). Excluding Pokémon Sun and Moon and Pokémon Ultra Sun and Ultra Moon, the games feature eight powerful Trainers, referred to as Gym Leaders, that the Trainer must defeat in order to progress. As a reward, the Trainer receives a Gym Badge, and once all eight badges are collected, the Trainer is eligible to challenge the region's Pokémon League, where four talented trainers (referred to collectively as the \"Elite Four\") challenge the Trainer to four Pokémon battles in succession. If the trainer can overcome this gauntlet, they must challenge the Regional Champion, the master Trainer who had previously defeated the Elite Four. Any Trainer who wins this\n",
      "--- [this text is from url0]\n",
      " last battle becomes the new champion.Pokémon is set in the fictional Pokémon universe. There are numerous regions that have appeared in the various media of the Pokémon franchise. There are 9 main series regions set in the main series games: Kanto, Johto, Hoenn, Sinnoh/Hisui, Unova, Kalos, Alola, Galar, and Paldea. Each of the nine generations of the main series releases focuses on a new region. Every region consists of several cities and towns that the player must explore in order to overcome many waiting challenges, such as Gyms, Contests and villainous teams. At different locations within each region, the player can find different types of Pokémon, as well as helpful items and characters. Different regions are not accessible from one another at all within a single game, only with the exception of Kanto and Johto being linked together in Pokémon Gold, Silver, Crystal, HeartGold and SoulSilver versions. There are also regions set in spin\n",
      "--- [this text is from url0]\n",
      "off games and two islands in the Pokémon anime (Orange Islands and Decolore Islands), all still set within the same fictional universe.Each main series region in the Pokémon universe is based on a real world location. The first four regions introduced are based on locations in Japan, being Kantō, Kansai, Kyushu, and Hokkaidō, with later regions being based on parts of New York City, France, Hawaii, the United Kingdom, and the Iberian Peninsula.[31][32]All of the licensed Pokémon properties overseen by the Pokémon Company International are divided roughly by generation. These generations are roughly chronological divisions by release; every several years, when a sequel to the 1996 role-playing video games Pokémon Red and Green is released that features new Pokémon, characters, and gameplay concepts, that sequel is considered the start of a new generation of the franchise. The main Pokémon video games and their spin-offs, the anime, manga, and trading card game are all updated\n",
      "--- [this text is from url0]\n",
      " with the new Pokémon properties each time a new generation begins.[33] Some Pokémon from the newer games appear in anime episodes or films months, or even years, before the game they were programmed for came out. The first generation began in Japan with Pokémon Red and Green on the Game Boy. As of 2022, there are nine generations of main series video games. The most recent games in the main series, Pokémon Scarlet and Violet began the ninth and latest generation when they released worldwide for the Nintendo Switch on November 18, 2022.[34][35][36]Pokémon, also known as Pokémon the Series to Western audiences since the year 2013, is an anime television series based on the Pokémon video game series. It was originally broadcast on TV Tokyo in 1997. More than 1,200 episodes of the anime has been produced and aired,[42] divided into 8 series in Japan and 26 seasons internationally. It is one of the longest currently running anime series.[42]The anime follows the quest of the main character\n",
      "--- [this text is from url0]\n",
      ", Ash Ketchum, a Pokémon Trainer, as he and a small group of friends travel around the world of Pokémon along with their Pokémon partners.[43]An eight-part anime series called Pokémon: Twilight Wings aired on YouTube in 2020.[45] The series was animated by Studio Colorido.[46]An eight part anime series in celebration of the Pokémon 25th anniversary called Pokémon Evolutions aired on YouTube in 2021.[47]In July 2021, it was announced that a live action Pokémon series is in early development at Netflix with Joe Henderson attached to write and executive produce.[48]There have been 23 animated theatrical Pokémon films, which have been directed by Kunihiko Yuyama and Tetsuo Yajima, and distributed in Japan by Toho since 1998. The pair of films, Pokémon the Movie: Black—Victini and Reshiram and White—Victini and Zekrom are considered together as one film. Collectibles, such as promotional trading cards, have\n",
      "--- [this text is from url0]\n",
      " been available with some of the films. Since the 20th film, the films have been set in an alternate continuity separate from the anime series.Pokémon CDs have been released in North America, some of them in conjunction with the theatrical releases of the first three and the 20th Pokémon films. These releases were commonplace until late 2001. On March 27, 2007, a tenth anniversary CD was released containing 18 tracks from the English dub; this was the first English-language release in over five years. Soundtracks of the Pokémon feature films have been released in Japan each year in conjunction with the theatrical releases. In 2017, a soundtrack album featuring music from the North American versions of the 17th through 20th movies was released.^  Featuring music from Pokémon the Movie: Diancie and the Cocoon of Destruction, Pokémon the Movie: Hoopa and the Clash of Ages, Pokémon the Movie: Volcanion and the Mechanical Marvel, and Pokémon the Movie: I Choose You! The Pokémon Trading Card\n",
      "--- [this text is from url0]\n",
      " Game (TCG) is a collectible card game with a goal similar to a Pokémon battle in the video game series. Players use Pokémon cards, with individual strengths and weaknesses, in an attempt to defeat their opponent by \"knocking out\" their Pokémon cards.[51] The game was published in North America by Wizards of the Coast in 1999.[52] With the release of the Game Boy Advance video games Pokémon Ruby and Sapphire, the Pokémon Company took back the card game from Wizards of the Coast and started publishing the cards themselves.[52] The Expedition expansion introduced the Pokémon-e Trading Card Game, where the cards (for the most part) were compatible with the Nintendo e-Reader. Nintendo discontinued its production of e-Reader compatible cards with the release of FireRed and LeafGreen. In 1998, Nintendo released a Game Boy Color version of the trading card game in Japan; Pokémon Trading Card Game was subsequently released to the US and Europe in 2000. The game included digital versions of cards from\n",
      "--- [this text is from url0]\n",
      " the original set of cards and the first two expansions (Jungle and Fossil), as well as several cards exclusive to the game. A sequel was released in Japan in 2001.[53]There are various Pokémon manga series, four of which were released in English by Viz Media, and seven of them released in English by Chuang Yi. The manga series vary from game-based series to being based on the anime and the Trading Card Game. Original stories have also been published. As there are several series created by different authors, most Pokémon manga series differ greatly from each other and other media, such as the anime.[example  needed] Pokémon Pocket Monsters and Pokémon Adventures are the two manga in production since the first generation.In July 2021, it was announced that a live-action Pokémon series is reportedly in development at Netflix. Joe Henderson, showrunner of Lucifer, is signed on as writer and executive producer.[62]Pokémon has been criticized by some fundamentalist Christians over perceived occult and violent themes and\n",
      "--- [this text is from url0]\n",
      " the concept of \"Pokémon evolution\", which they feel goes against the Biblical creation account in Genesis.[63] Sat2000, a satellite television station based in Vatican City, has countered that the Pokémon Trading Card Game and video games are \"full of inventive imagination\" and have no \"harmful moral side effects\".[64][65] In the United Kingdom, the \"Christian Power Cards\" game was introduced in 1999 by David Tate who stated, \"Some people aren't happy with Pokémon and want an alternative, others just want Christian games.\" The game was similar to the Pokémon Trading Card Game but used Biblical figures.[66]In 1999, Nintendo stopped manufacturing the Japanese version of the \"Koga's Ninja Trick\" trading card because it depicted a manji, a traditionally Buddhist symbol with no negative connotations. The Jewish civil rights group Anti-Defamation League complained because the symbol is the reverse of a swastika, a Nazi symbol. The cards were intended for sale in Japan only, but the popularity of\n",
      "--- [this text is from url0]\n",
      " Pokémon led to import into the United States with approval from Nintendo. The Anti-Defamation League understood that the portrayed symbol was not intended to offend and acknowledged the sensitivity that Nintendo showed by removing the product.[67][68]In 1999, two nine-year-old boys from Merrick, New York, sued Nintendo because they claimed the Pokémon Trading Card Game caused their problematic gambling.[69]In 2001, Saudi Arabia banned Pokémon games and the trading cards, alleging that the franchise promoted Zionism by displaying the Star of David in the trading cards (the Colorless energy from the Pokémon Trading Card Game resembles a six-pointed star) as well as other religious symbols such as crosses they associated with Christianity and triangles they associated with Freemasonry; the games also involved gambling, which is in violation of Muslim doctrine.[70][71]In 2012, PETA criticized the concept of Pokémon as supporting cruelty to animals. PETA compared the game's concept, of capturing animals and forcing them to fight, to\n",
      "--- [this text is from url0]\n",
      " cockfights, dog fighting rings and circuses, events frequently criticized for cruelty to animals. PETA released a game spoofing Pokémon where the Pokémon battle their trainers to win their freedom.[73] PETA reaffirmed their objections in 2016 with the release of Pokémon Go, promoting the hashtag #GottaFreeThemAll.[74]On December 16, 1997, more than 635 Japanese children were admitted to hospitals with epileptic seizures.[75] It was determined the seizures were caused by watching an episode of Pokémon \"Dennō Senshi Porygon\", (most commonly translated \"Electric Soldier Porygon\", season 1, episode 38); as a result, this episode has not been aired since. In this particular episode, there were bright explosions with rapidly alternating blue and red color patterns.[76] It was determined in subsequent research that these strobing light effects cause some individuals to have epileptic seizures, even if the person had no previous history of epilepsy.[77] This incident is a\n",
      "--- [this text is from url0]\n",
      " common focus of Pokémon-related parodies in other media, and was lampooned by The Simpsons episode \"Thirty Minutes over Tokyo\" in a short cameo[78] and the South Park episode \"Chinpokomon\",[79] among others.Within its first two days of release, Pokémon Go raised safety concerns among players. Multiple people also suffered minor injuries from falling while playing the game due to being distracted.[80]Multiple police departments in various countries have issued warnings, some tongue-in-cheek, regarding inattentive driving, trespassing, and being targeted by criminals due to being unaware of one's surroundings.[81][82] People have suffered various injuries from accidents related to the game,[83][84][85][86] and Bosnian players have been warned to stay out of minefields left over from the 1990s Bosnian War.[87] On July 20, 2016, it was reported that an 18-year-old boy in Chiquimula\n",
      "--- [this text is from url0]\n",
      ", Guatemala, was shot and killed while playing the game in the late evening hours. This was the first reported death in connection with the app. The boy's 17-year-old cousin, who was accompanying the victim, was shot in the foot. Police speculated that the shooters used the game's GPS capability to find the two.[88]Pokémon, being a globally popular franchise, has left a significant mark on today's popular culture. The various species of Pokémon have become pop culture icons; examples include two different Pikachu balloons in the Macy's Thanksgiving Day Parade, Pokémon-themed airplanes operated by All Nippon Airways, merchandise items, and a traveling theme park that was in Nagoya, Japan in 2005 and in Taipei in 2006. Pokémon also appeared on the cover of the U.S. magazine Time in 1999.[89] The Comedy Central show Drawn Together has a character named Ling-Ling who is a parody of Pikachu.[90] Several other shows such as The Simpsons[91\n",
      "--- [this text is from url0]\n",
      "] South Park[92] and Robot Chicken[93] and The Flash have made references and spoofs of Pokémon, among other series. Pokémon was featured on VH1's I Love the '90s: Part Deux. A live action show based on the anime called Pokémon Live! toured the United States in late 2000.[94] Jim Butcher cites Pokémon as one of the inspirations for the Codex Alera series of novels.[95]Pokémon has even made its mark in the realm of science. This includes animals named after Pokémon, such as Stentorceps weedlei (named after the Pokémon Weedle for its resemblance) and Chilicola charizard (named after the Pokémon Charizard) as well as Binburrum articuno, Binburrum zapdos, and Binburrum moltres (named after the Pokémon Articuno, Zapdos, and Moltres, respectively).[96][97] There is also a protein named after Pikachu, called Pik\n",
      "--- [this text is from url0]\n",
      "achurin.In November 2001, Nintendo opened a store called the Pokémon Center in New York, in Rockefeller Center,[98] modeled after the two other Pokémon Center stores in Tokyo and Osaka and named after a staple of the video game series. Pokémon Centers are fictional buildings where Trainers take their injured Pokémon to be healed after combat.[99] The store sold Pokémon merchandise on a total of two floors, with items ranging from collectible shirts to stuffed Pokémon plushies.[100] The store also featured a Pokémon Distributing Machine in which players would place their game to receive an egg of a Pokémon that was being given out at that time. The store also had tables that were open for players of the Pokémon Trading Card Game to duel each other or an employee. The store was closed and replaced by the Nintendo World Store on May 14, 2005.[101] Four Pokémon Center kiosks were put in malls in the Seattle area.[102] The Pokémon Center online store was relaunched on August 6,\n",
      "--- [this text is from url0]\n",
      " 2014.[103]Professor of education Joseph Tobin theorizes that the success of the franchise was due to the long list of names that could be learned by children and repeated in their peer groups. Its rich fictional universe provides opportunities for discussion and demonstration of knowledge in front of their peers. The names of the creatures were linked to its characteristics, which converged with the children's belief that names have symbolic power. Children can pick their favourite Pokémon and affirm their individuality while at the same time affirming their conformance to the values of the group, and they can distinguish themselves from others by asserting what they liked and what they did not like from every chapter. Pokémon gained popularity because it provides a sense of identity to a wide variety of children, and lost it quickly when many of those children found that the identity groups were too big and searched for identities that would distinguish them into smaller groups.[104][page needed]Pokémon's history has been marked at times by rivalry with the Digimon media franchise that\n",
      "--- [this text is from url0]\n",
      " debuted at a similar time. Described as \"the other 'mon'\" by IGN's Juan Castro, Digimon has not enjoyed Pokémon's level of international popularity or success, but has maintained a dedicated fanbase.[105] IGN's Lucas M. Thomas stated that Pokémon is Digimon's \"constant competition and comparison\", attributing the former's relative success to the simplicity of its evolution mechanic as opposed to Digivolution.[106] The two have been noted for conceptual and stylistic similarities by sources such as GameZone.[107] A debate among fans exists over which of the two franchises came first.[108] In actuality, the first Pokémon media, Pokémon Red and Green, were released initially on February 27, 1996;[109] whereas the Digimon virtual pet was released on June 26, 1997.While Pokémon's target demographic is children, early purchasers of Pokémon Omega Ruby and Alpha Sapphire were in their 20s.[110] Many fans are adults who originally played the games as children\n",
      "--- [this text is from url0]\n",
      " and had later returned to the series.[110]Numerous fan sites exist for the Pokémon franchise, including Bulbagarden, a site hosting the wiki-based encyclopedia Bulbapedia,[111][112][113] and Serebii,[114] a news and reference website.[115] Large fan communities exist on other platforms, such as the subreddit r/pokemon, which has over 4 million subscribers.[116]A significant community around the Pokémon video games' metagame has existed for a long time, analyzing the best ways to use each Pokémon to their full potential in competitive battles. The most prolific competitive community is Smogon University, which has created a widely accepted tier-based battle system.[117]Smogon is affiliated with an online Pokémon game called Pokémon Showdown, in which players create a team and battle against other players around the world using the competitive tiers created by Smogon.[118]In early 2014, an anonymous video streamer on Twitch launched Twitch Plays Pokémon,\n",
      "--- [this text is from url0]\n",
      " a small experiment trying to crowdsource playing subsequent Pokémon games, that started with the game Pokémon Red and has since included subsequent games in the series.[119][120]A study at Stanford Neurosciences published in Nature performed magnetic resonance imaging scans of 11 Pokémon experts and 11 controls, finding that seeing Pokémon stimulated activity in the visual cortex, in a different place than is triggered by recognizing faces, places, or words, demonstrating the brain's ability to create such specialized areas.[121][122]\n",
      "--- [from pdf0]\n",
      "VisionLLM: Large Language Model is alsoan Open-Ended Decoder for Vision-Centric TasksWenhai Wang∗1, Zhe Chen∗2,1, Xiaokang Chen∗3,1, Jiannan Wu∗4,1, Xizhou Zhu5,1Gang Zeng3, Ping Luo4,1, Tong Lu2, Jie Zhou6, Yu Qiao1, Jifeng Dai†6,11OpenGVLab, Shanghai AI Laboratory2Nanjing University3Peking University4The University of HongKong5SenseTime Research6Tsinghua UniversityCode: https://github.com/OpenGVLab/VisionLLMDemo: https://github.com/OpenGVLab/InternGPTAbstractLarge language models (LLMs) have notably accelerated progress towards artificialgeneral intelligence (AGI), with their impressive zero-shot capacity for user-tailoredtasks, endowing them with\n",
      "--- [from pdf0]\n",
      " immense potential across a range of applications.However, in the field of computer vision, despite the availability of numerouspowerful vision foundation models (VFMs), they are still restricted to tasks in apre-defined form, struggling to match the open-ended task capabilities of LLMs.In this work, we present an LLM-based framework for vision-centric tasks, termedVisionLLM. This framework provides a unified perspective for vision and languagetasks by treating images as a foreign language and aligning vision-centric taskswith language tasks that can be flexibly defined and managed using languageinstructions. An LLM-based decoder can then make appropriate predictions basedon these instructions for open-ended tasks. Extensive experiments show that theproposed VisionLLM can achieve different levels of task customization throughlanguage instructions, from fine-grained object-level to coarse-grained task-levelcustomization, all with good results. It’s noteworthy that, with\n",
      "--- [from pdf0]\n",
      " a generalist LLM-based framework, our model can achieve over 60% mAP on COCO, on par withdetection-specific models. We hope this model can set a new baseline for generalistvision and language models. The code and demo shall be released.1 IntroductionThe emergence of large language models (LLMs) like ChatGPT [ 41] has revolutionized the landscapeof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-dressing various natural language processing (NLP) tasks through user-tailored prompts or languageinstructions. Despite these advancements, it’s essential to note that the triumph of LLMs does noteffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities betweenmodalities and task formats.The field of computer vision presents a unique set of challenges and paradigms that differ fromthose of NLP. The traditional paradigm of vision foundation models is pre-training\n",
      "--- [from pdf0]\n",
      " followed byfine-tuning [ 59,12,51,61,18,52], which is effective but comes with significant marginal costswhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such asmulti-task unification [ 44,58,1,57,81] have been used to achieve generalist capability, they oftenstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended∗Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns atShanghai AI Laboratory.†Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.arXiv:2305.11175v2  [cs.CV]  25 May 2023VisionGeneralistModelPre-defined tasks:detection, captioning, VQA, grounding, ...(a) Vision general\n",
      "--- [from pdf0]\n",
      "ist models [ 59,61,83] are constrained by the for-mat of pre-defined tasks.VisualPromptTuning(b) Visual prompt tuning [ 26,64,62] are inconsistent with the for-mat of LLMs.Vision + LLMTask deﬁned by  instruc6onsDesired output:<c1> <p1> <p3> ...(c) VisionLLM (ours) can flexiblymanage vision-centric tasks usinglanguage instructions like LLMs .Figure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalistmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistentwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended visiontasks by using language instructions.task capabilities compared to LLMs. Recently, visual prompt tuning [ 26,74,79,76,62]\n",
      "--- [from pdf0]\n",
      " has emergedas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instancesegmentation, and pose estimation, using visual masking. However, the format of visual promptsconsiderably deviates from that of language instructions, making it challenging to directly apply thereasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgentneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with thespecific requirements of vision-centric tasks.In this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centrictasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,VisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,it comprises three core components: (1) a unified language instruction designed for vision andvision-language tasks, (2) a language-guided\n",
      "--- [from pdf0]\n",
      " image tokenizer, and (3) an LLM-based open-endedtask decoder that orchestrates various tasks using language instructions. With this framework, awide range of vision-centric tasks can be seamlessly integrated, including object detection, instancesegmentation, image captioning, and visual grounding. In addition, the framework also facilitatestask customization at different levels of granularity, allowing for the customization of target objects,output formats, task descriptions, etc.Compared to current popular API-based applications [ 68,73,50,35,30], our model takes a unified,end-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiencyof the overall process, and leveraging the strengths and data of both VFMs and LLMs within asingle, cohesive system. Furthermore, our model surpasses the limitations of generalist vision modelspre-trained on pre-defined tasks. VisionLLM can effectively manage vision-centric tasks\n",
      "--- [from pdf0]\n",
      " throughlanguage instructions, embodying a flexible and open-ended approach that is not constrained bypre-set tasks. This versatility makes VisionLLM a robust and powerful generalist model for visionand vision-language tasks, opening up new possibilities for the development of unified generalistmodels that bridge the domains of vision and language.In summary, our main contributions are as follows:(1) We propose VisionLLM, the first framework that leverages the power of LLMs to address vision-centric tasks in an open-ended and customizable manner. By aligning the definitions of vision-centrictasks with LLM methodologies, VisionLLM breaks new ground in enabling the unified modeling ofvision and language, opening up possibilities for advancing the field.(2) We overcome many difficulties when porting LLMs to vision-centric tasks, by designing unifiedlanguage instruction that matches the format of language models and covers various vision-centrictasks including visual perception. Correspondingly, we develop a language-guided\n",
      "--- [from pdf0]\n",
      " image tokenizerand an LLM-based task decoder that can handle open-ended tasks according to the given languageinstructions based on the LLMs’ reasoning and parsing capabilities.2Human: “Identify the objects in <image>that belong to {‘What is the child eating?’: <c0>, ‘red gamepad': <c1>}and draw a bounding box around eachone. The output should be a list oftuples in  the format (c, x1, y1, x2, y2), where c is the class label and x1, y1, x2, y2 are the coordinates of the top-left and bottom-right corners of the bounding box. The coordinates should be within the range 1024.\"VisionLLM: \"Thebounding boxesare[(<c0>, 226.4, 229.8, 363.1, 347.4), (<c\n",
      "--- [from pdf0]\n",
      "1>, 441.1, 183.5, 538.6, 269.9)].\"<c1><c0>(a) Object Detection with Customized Class Set (e.g.,question, reasoning text, open-vocabulary descrip-tion)Human: \"For each object in <image>that is a member of {'motorbike': <c0>}, output a tuple with the class label and the coordinates of a polygon with 16 pointsthatencloses the object.  The coordinates should be within the range 1024. The output format should be (c, x1, y1, x2, y2, ..., x16, y16).\"VisionLLM: \"The detectedpolygonsare[(<c0>, 135.3, 95.7, 123.4, 53.4, 84.9, 57.6, 66.8, 60.5, 60.1, 72.\n",
      "--- [from pdf0]\n",
      "3, 34.2, 71.4, ..., 124.9, 119.3)].\"<c0> (b) Object Location with Editable Output Formatwhich is placed on top of a wooden table. The dog has its head in the middle of the cooling pad, and a fan is positioned behind it, blowing air onto the pad. The table is outdoors, in a grassy area, and there are two chairs placed around it, one on each side.The scene appears to be set in a sunny weather, with a bright and sunny sky, which contributes to the overall outdoor atmosphere.\"Human: \"Provide a long descriptionof the <image>.\"VisionLLM:  \"The image features a white doglaying down on a foldable outdoor cooling pad, (c) Image Description with Controllable Text LengthHuman: \"With the objects in the <image>, if you feel hot, what will you do?\"VisionLLM: \"First, You\n",
      "--- [from pdf0]\n",
      " can also try turning off the computer and other electrical appliancesin theroom to reduce the heat generated by their operation.As an alternative, you can use the desk fanprovided in the image to create a cooling effect in the room. You can plug in the fan and adjust its settings to ensure it blows air in a direction that reaches the room’s occupants effectively.\" (d) Visual Question Answer with Complex ReasoningFigure 2: Results and visualizations of our VisionLLM . Guided by language instructions, ourunified generalist framework showcases its effectiveness on diverse open-ended vision-centric tasks.The text marked with a gray background indicates the customized instructions and the desired outputs.(3) We construct a series of tasks with different granularities to verify the effectiveness of ourmodels, ranging from easy to hard, and from pre-defined to flexible. Through these validations, wedemonstrate the remarkable generality of our models, showcasing their ability to handle diversescenarios, including\n",
      "--- [from pdf0]\n",
      " random object categories, random output formats, and random task descriptions,as shown in Figure 2. The successful outcomes of these validations underscore the tremendouspotential of our model in harnessing the capabilities of LLMs to control and guide vision-centrictasks. In addition, with a generalist LLM-based framework, our model also yields promising resultson various vision-centric tasks. Notably, our generalist model achieves an impressive mAP score of60+% on the COCO dataset, surpassing many detection-specific models [ 82,7,22] and approachingthe state-of-the-art record.2 Related Work2.1 Large Language ModelLarge language models (LLMs) have gained significant attention in the field of natural languageprocessing (NLP) and artificial general intelligence (AGI), due to their impressive capabilitiesin language generation, in-context learning, world knowledge, and reasoning. The GPT family,including GPT-3 [ 6], Chat\n",
      "--- [from pdf0]\n",
      "GPT [ 41], GPT-4 [ 40], and InstructGPT [ 42] are most representativeworks of LLMs. Other LLMs like OPT [ 78], LLaMA [ 54], MOSS [ 15], and GLM [ 77] havealso made substantial contributions to the field. These models achieve high performance and areopen-sourced, serving as valuable resources for training large models and as foundations for furtherfine-tuning for specific purposes. For instance, Alpaca [ 53] introduces a self-instruct frameworkthat facilitates instruction tuning of the LLaMA model, reducing the reliance on human-written3instruction data. Recently, the emergence of these LLMs has also opened up API-based applicationsfor solving vision-centric tasks. These applications have integrated visual APIs with language modelsto enable decision-making or planning based on visual information, such as Visual ChatGPT [ 68],MM-REACT [ 73], HuggingGPT [ 50\n",
      "--- [from pdf0]\n",
      "], InternGPT [ 35], and VideoChat [ 30]. However, despite theconvenience of using language-based instructions to define tasks and describe visual elements, theseinteractive systems [ 68,73,50,35,30] still face limitations in capturing fine-grained visual detailsand understanding complex visual contexts, which hinder their ability to effectively connecting visionand language models. In summary, while LLMs have shown tremendous potential in various NLPapplications, their applicability to vision-centric tasks has been limited by the challenges posed bymodalities and task formats.2.2 Vision Generalist ModelThe pursuit of generalist models [ 83,38,70], which aim to handle a wide range of tasks using ashared architecture and parameters, has been a long-standing goal in the machine learning community.Inspired by the success of sequence-to-sequence (seq2seq) models in the field of NLP [ 44], recentadvancements such as OF\n",
      "--- [from pdf0]\n",
      "A [ 58], Flamingo [ 1], and GIT [ 57] propose modeling diverse tasks assequence generation tasks. Unified-IO [ 38], Pix2Seq v2 [ 9], and UniTab [ 71] extend this idea by usingdiscrete coordinate tokens to encode and decode spatial information for more tasks. Gato [ 47] alsoincorporates reinforcement learning tasks into the seq2seq framework, while GPV [ 21] develops ageneral-purpose vision system by combining a seq2seq module with a DETR-based visual encoder [ 7].However, these methods suffer from some limitations, such as slow inference speed and performancedegradation due to the non-parallel auto-regressive decoding process. Uni-Perceivers [ 83,81,28]solve these issues by unifying different tasks using the maximum likelihood target for each inputbased on representation similarity, regardless of their modality, making it possible to support bothgeneration and non\n",
      "--- [from pdf0]\n",
      "-generation tasks in a unified framework. Nevertheless, these generalist modelsare still restricted by pre-defined tasks and cannot support flexible open-ended task customizationbased on language instructions like LLMs.2.3 Instruction TuningLanguage instructions are a powerful way to express various NLP tasks and examples for LLMs,as introduced by GPT-3 [ 6]. Following this idea, subsequent works, such as InstructGPT [ 42],FLAN [ 14,67], and OPT-IML [ 25], explore the instruction-tuning method [ 66,65] and demonstratethat this simple approach effectively enhances the zero-shot and few-shot capabilities of LLMs.The language instruction paradigm has also been adopted by the computer vision community todefine image-to-text tasks. Flamingo [ 1] is a milestone work that uses vision and language inputs asprompts and achieves remarkable few-shot results in various vision-language tasks, such as imagecaptioning\n",
      "--- [from pdf0]\n",
      " [ 10] and VQA [ 3]. BLIP-2 [ 29] further connects the visual encoder with LLMs through aquerying transformer and a linear projection layer to build strong multimodal models. MiniGPT-4 [ 80]and LLaV A [ 33] finetune the BLIP-2-style models on synthetic multimodal instruction-followingdata to unleash the potential of LLMs. However, these models mainly focus on image-to-texttasks and fail to address visual perception, such as object detection, instance segmentation, poseestimation, etc. To tackle image inpainting tasks, Bar et al. [4] introduces the first visual promptingframework that utilizes inpainting with discrete tokens on images. Painter [ 63] and SegGPT [ 64]employ masked image modeling on raw pixels for in-context learning with paired images. Whilethese visual prompt models demonstrate good results in segmentation tasks, their applicability\n",
      "--- [from pdf0]\n",
      " tonumerous real-world vision tasks is challenging. Moreover, defining the visual prompts as imageinpainting is inconsistent with the language instructions in LLMs, hard to leverage the reasoning,parsing ability, and world knowledge of LLMs. In this work, we aim to align vision-centric taskswith language tasks, use language instructions to unifiedly and flexibly define all tasks, and solvethem with a shared LLM-based task decoder.3 VisionLLM3.1 Overall ArchitectureThis work targets to provide a unified generalist framework that can seamlessly integrate the strengthsof large language models (LLMs) with the specific requirements of vision-centric tasks. As shown in4RandomQueryLanguage-GuidedImage Token...<text><text>Desired Output:<c1> <p1> <p3> ...Open-Ended Task Decoder with LLMLanguage-GuidedImageTokenizerBackbone...Vision-languageexample:\"Describetheimage<\n",
      "--- [from pdf0]\n",
      "image>indetails.\"Vision-onlyexample:\"Foreachobjectinimage<image>thatisamemberofclassset<class>,outputatuplewiththeclasslabelandthecoordinatesofapolygonwith16pointsthatenclosestheobject.Thecoordinatesshouldbewithinrange<range>.Theoutputformatshouldbe(c,x1,y1,...).\"LanguageInstructions\t<text>𝐹!𝐹\"𝑇Figure 3: Overall architecture of the proposed VisionLLM. It consists of three parts: a unifiedlanguage instruction designed to accommodate both vision and vision-language tasks, an imagetokenizer that encodes visual information guided by language instructions, and an LLM-based open-ended task decoder that executes diverse tasks defined by language instructions.Figure 3, the overall architecture of VisionLLM consists of three key designs: (1) a unified languageinstruction that\n",
      "--- [from pdf0]\n",
      " provides a consistent interface for vision-centric task definition and customization;(2) a language-guided image tokenizer, which encodes visual information in alignment with the givenlanguage prompt, enabling the model to comprehend and parse the visual content effectively; and(3) an LLM-based open-task decoder, which utilizes the encoded visual information and languageinstructions to generate satisfactory predictions or outputs. The three designs work together to achievea flexible and open-ended framework that can handle various vision-centric tasks at different levels oftask customization through language instructions.Different from previous interactive systems [ 68,73,50,35,30] that rely on APIs, our VisionLLMpresents a more flexible and end-to-end pipeline. Given language instructions that describe the currenttasks and an input image, the model first uses a language-guided image tokenizer to encode the imagetokens based on the given prompt. Then, the image tokens and language instructions are fed to\n",
      "--- [from pdf0]\n",
      " anLLM-based open-ended task decoder. Finally, it evaluates the generated outputs against the taskdefinition given by the unified language instructions, enabling the model to produce task-specificresults. This seamless, end-to-end pipeline enables VisionLLM to effectively combine vision andlanguage, achieving remarkable performance in open-ended and customizable vision-centric tasks.3.2 Unified Language InstructionWe first introduce unified language instructions to describe vision-centric tasks. This design enablesthe unification of various vision-only and vision-language task descriptions and allows for flexibletask customization.Vision-Language Tasks. The instructions for vision-language tasks such as image captioning andvisual question answering (VQA) are straightforward and similar to NLP tasks. Following previousmethods [ 29,83,33], we describe the image captioning task like “ The image is <image> .Pleasegenerate a caption for the image: ”, and the VQA task\n",
      "--- [from pdf0]\n",
      " like “ The image is <image> .Please generatean answer for the image according to the question: <question> ”. Here, <image> and<question>are the placeholdersok of the image tokens and the question, respectively.Vision-Only Tasks. Designing effective language instructions for vision tasks is a challengingendeavor due to the differences in modality and task format between vision and language. Here, wedescribe vision tasks by providing a task description and specifying the desired output format vialanguage instructions.(1) The task description conveys the intended task to the language model. Following self-instruct [ 65],we design a set of seed instructions with placeholders and employ LLMs to generate a large numberof related task descriptions and randomly select one of them during training.5(2) For conventional visual perception tasks like object detection and instance segmentation, wepropose a unified output format represented as a tuple (C, P), where Cden\n",
      "--- [from pdf0]\n",
      "otes the class indexin the category set <class> , and P={xi, yi}Ni=1represents Npoints that locate the object. Toalign with the format of word tokens, both the class index Cand the coordinates of points xi, yiaretransformed into discretized tokens. Specifically, the class index is an integer starting from 0, andthe continuous coordinates of the points are uniformly discretized into an integer within the range[-<range> ,<range> ]. For object detection and visual grounding tasks, the point number Nis equalto 2, representing the the top-left and bottom-right points of object’s bounding box. In the case ofinstance segmentation, we employ multiple ( N >8) points along the object boundary to represent aninstance mask [ 69]. Other perception tasks such as pose estimation (keypoint detection) can also beformulated as language instructions in this way.An example of language instruction for\n",
      "--- [from pdf0]\n",
      " the instance segmentation task is as follows: “ Segment all theobjects of category set <class> within the <range> of the image and generate a list of the format(c, x1, y1, x2, y2, ..., x8, y8). Here, c represents the index of the class label starting from 0, and (x1,y1, x2, y2, ..., x8, y8) correspond to the offsets of boundary points of the object relative to the centerpoint. The image is: <image> ”.3.3 Language-Guided Image TokenizerVisionLLM considers images as a kind of foreign language and converts them into token represen-tations. Unlike previous works [ 17,60,34] that utilize fixed-size patch embeddings to representimages, we introduce the language-guided image tokenizer to flexibly encode visual information thataligns with task-specific\n",
      "--- [from pdf0]\n",
      " language prompts or instructions.Specifically, give an image X∈RH×W×3with height Hand width W, we first feed it to the imagebackbones ( e.g., ResNet [ 23]) and extract visual features Fvof four different scales. Addition-ally, we leverage a text encoder ( e.g., BERT [ 16]) to extract the language features Flfrom givenprompts. The language features are then injected into each scale of visual features through cross-attention [ 55], yielding multi-scale language-aware visual features, enabling the alignment of featuresacross modalities.Afterward, we propose to adopt a transformer-based network ( e.g., Deformable DETR [ 82]) with Mrandom-initialized queries Q={qi}Mi=1to capture the high-level information of images. We buildthe transformer-based network on top of the multi-scale language-aware visual features to extract Mimage tokens T\n",
      "--- [from pdf0]\n",
      "={(ei, li)}Mi=1, each of which is represented by an embedding eiand a location li,denoting the semantic and positional information of the token. This design not only represents theimages independent of input resolution but also extracts the visual representation that is informativewith respect to the language prompts.3.4 LLM-based Open-Ended Task DecoderWe build our decoder on Alpaca [ 53], an LLM that is adapted from LLaMA [ 54], to handle variousvision-related tasks with language guidance. However, Alpaca has some inherent drawbacks forvision-centric tasks, such as (1) It only has a few digit tokens ( e.g., 0∼9) in its vocabulary, whichrestricts its ability to locate objects by numbers; (2) It uses multiple tokens to represent the categoryname, resulting in an inefficient scheme in object classification; and (3) It is a causal model that isine\n",
      "--- [from pdf0]\n",
      "fficient for visual perception tasks.To tackle these issues, we expand the vocabulary of LLM with additional tokens specially designedfor vision-centric tasks. First, we add a set of location tokens, denoted as { <p-512> , ..., <p0> ,...,<p512> }, where <p i> represents the discretized offset of i∈[−512,512] to the location liof the image token, and the relative value to image height or width is equal to i/512. Thesetokens successfully transform the object localization task from continuous variable prediction tomore unified discrete bin classification. Second, we introduce semantics-agnostic classificationtokens { <c0> ,<c1> , ...,<c511> } to replace category name tokens, which overcomes the inefficiencyof using multiple tokens to represent categories. The mapping between category names and theclassification tokens is flexibly provided in the category set <\n",
      "--- [from pdf0]\n",
      "class> of language instructions, such as{\"person\":<c0> ,\"car\":<c1> ,\"black cat\":<c2>, ...}. This design allows our model to selectthe appropriate category name from the provided category set, facilitating efficient and accurateobject classification.6LLM-based Open-Ended Task DecoderTask definedby instructionsparsingformat 1: \"<cls> <x1> <y1> ...\"format 2: \"<bos>\"...format n: ...Figure 4: Illustration of the “output-format-as-query” decoding process. “ <cls> <x1> <y1> ...”denote the queries of the object’s class index andboundary points, and “ <bos> ” denotes the begin-ning of string.Moreover, to address the inefficiency causedby the causal framework, we introduce output-format-as-query decoding. We first use LL\n",
      "--- [from pdf0]\n",
      "Ms toparse the structural output format from the taskinstructions ( e.g., “<cls> <x1> <y1> <x2><y2> ” for object detection, “ <bos> ” for imagecaptioning), and then feed the tokens of struc-tural output format as queries to the decoderto generate the desired output according to thequeries. This simple method enables our modelto not only avoid inefficient token-by-token de-coding in visual perception tasks, but also keepa unified framework for vision-language tasks.In this way, the output of object location andclassification is formulated as a foreign lan-guage, thus unifying these vision-centric tasksinto the format of token classification. Therefore, both vision-language and vision-only tasks can besupervised with the cross entropy loss like language tasks. In addition, for efficient training, we adoptthe Low-Rank\n",
      "--- [from pdf0]\n",
      " Adaptation (LoRA) approach [ 24], which allows us to train and fine-tune the modelswithout excessive computational costs. It also acts as a bridge between the language and visual tokens,facilitating effective alignment between the two modalities, ensuring better task customization, andimproving the convergence of the overall system.4 Experiment4.1 Experimental SettingsDatasets. VisionLLM unifies the output formats of vision and language tasks as vocabulary gen-eration, which enables models to be jointly trained on a wide range of tasks. In the experiments,we investigate the general modeling capacities of VisionLLM on five vision-centric tasks, includingobject detection, instance segmentation, visual grounding, image captioning, and visual questionanswering. For object detection and instance segmentation, COCO2017 [ 32] is used for training andevaluation. For visual grounding, we combine the annotations of RefCOCO [ 75], RefCOCO+ [ 75]\n",
      "--- [from pdf0]\n",
      "and RefCOCOg [ 39] for training, resulting in over 120k referred objects in total. And our models areevaluated on the validation set of RefCOCO. For image captioning and visual question answering, weadopt COCO Caption [ 10] and LLaV A-Instruct-150K [ 33] as the training source. We evaluate the im-age captioning performance on the COCO Karpathy test split following common practice [ 28,58,71].We mainly use qualitative results (see Figure 2d) to demonstrate the VQA capability of our model,as LLaV A-Instruct-150K is not compatible with the standard VQA benchmark. These tasks differin their granularity, ranging from coarse-grained image level to fine-grained pixel level, enabling acomprehensive evaluation of the model’s ability to adapt to different levels of customization throughlanguage instructions.Implementation Details.\n",
      "--- [from pdf0]\n",
      " We implement two variants of VisionLLM with two image backbones, i.e.,ResNet [ 23] and InternImage-H [ 59]. For the language-guided image tokenizer, we adopt BERT-Base [ 5] as the text encoder and Deformable DETR (D-DETR) [ 82] to capture high-level information.We set the number of queries Mto 100, and the number of encoder/decoder layers to 6 for D-DETR.For the LLM, we employ Alpaca-7B [ 53], a LLaMA [ 54] model fine-tuned with instructions, andequip it with LoRA [24] for parameter-efficient fine-tuning.The model is trained in two stages. In the first stage, we initialize the model with the pre-trainedweights of D-DETR, BERT, and Alpaca-7B, and train the\n",
      "--- [from pdf0]\n",
      " visual backbone and the language-guidedimage tokenizer, while freezing most parameters of the LLM except a few LoRA parameters. Tosimplify the training complexity, in this stage, we mainly focus on object detection tasks with randomobject categories and task descriptions. In the second stage, we freeze the visual backbone andintroduce the unified supervision of multiple tasks. Unless otherwise specified, the training runs for50epochs on 4×8NVIDIA A100 GPUs. AdamW [ 36] is used as the optimizer, with one sample perGPU. We employ the cosine annealing schedule [ 37] as the learning policy, with an initial learning7Table 1: Results on standard vision-centric tasks. ‘Intern-H” denotes InternImage-H [ 59]. “sep”indicates that the model is separately trained on each task.Method BackboneOpen-EndedDetection Instance Seg. Grounding CaptioningAP AP 50AP75AP\n",
      "--- [from pdf0]\n",
      " AP 50AP75 P@0.5 BLEU-4 CIDErSpecialist ModelsFaster R-CNN-FPN [48] ResNet-50 - 40.3 61.0 44.0 - - - - - -DETR-DC5 [7] ResNet-50 - 43.3 63.1 45.9 - - - - - -Deformable-DETR [82] ResNet-50 - 45.7 65.0 49.1 - - - - - -Mask R-CNN [22] ResNet-50 - 41.0 61.7 44.9 37.1 58.4 40.1 - - -Polar Mask [69] ResNet-50 - - - - 30.5 52.0 31.1 - - -Pix2Seq [8] ResNet-50 - 43.2 61.0 46.1 - - - - - -UNITER\n",
      "--- [from pdf0]\n",
      " [11] ResNet-101 - - - - - - - 81.4 - -VILLA [19] ResNet-101 - - - - - - - 82.4 - -MDETR [27] ResNet-101 - - - - - - - 86.8 - -VL-T5 [13] T5-B - - - - - - - - - 116.5Generalist ModelsUniTab [72] ResNet-101 - - - - - - - 88.6 - 115.8Uni-Perceiver [83] ViT-B - - - - - - - - 32.0 -Uni-Perceiver-MoE [81] ViT-B - - - - - - - - 33.2 -Uni-Perceiver-V2 [28] ViT-B - 58.6 - - 50.6 - - - 35.4 116.9Pix2\n",
      "--- [from pdf0]\n",
      "Seq v2 [9] ViT-B - 46.5 - - 38.2 - - - 34.9 -VisionLLM-R50 sep ResNet-50 -44.8 64.1 48.5 25.2 50.6 22.4 84.4 30.8 112.4VisionLLM-R50 ResNet-50 ✓ 44.6 64.0 48.1 25.1 50.0 22.4 80.6 31.0 112.5VisionLLM-H Intern-H ✓ 60.2 79.3 65.8 30.6 61.2 27.6 86.7 32.1 114.2rate of 2×10−4. In addition to the experiments in the main paper, more experimental settings andablation studies are provided in the supplementary material due to space limitations.4.2 Task-Level CustomizationWe first evaluate the task-level customization capability of VisionLLM. VisionLL\n",
      "--- [from pdf0]\n",
      "M supports coarse-grained task customization, including visual perception tasks and visual-language tasks. Table 1presents the evaluation results on four standard vision-centric tasks, including object detection,instance segmentation, visual grounding, and image captioning. We compare our model with task-specific methods as well as recently-proposed vision generalist models. Note that, unless specificallymentioned, the results of our model come from a shared-parameter generalist model and switchdifferent tasks by changing the language instructions only. Detailed instructions could be found inthe supplementary material.Object Detection. Object detection is a fundamental computer vision task that involves identifyingand localizing objects of interest within an image. Our method achieves comparable or higher resultsto others, 44.6mAP, with a ResNet-50 [ 23] backbone. With the same backbone i.e.ResNet-50,our method outperforms Pix2Seq [ 8] by1.4mAP, which also\n",
      "--- [from pdf0]\n",
      " discretizes the output coordinates tointegers. Furthermore, benefiting from the output-format-as-query framework (see Sec. 3.4), we candecode multiple predictions in parallel during inference, making our approach more efficient. UsingInternImage-H [ 59] as the visual backbone, we obtained 60.2% mAP, which is close to the currentstate-of-the-art detection-specific model [59], demonstrating the scalability of our generalist model.Visual Grounding. Visual grounding associates textual descriptions with corresponding regionsor objects within an image. Training visual grounding and object detection can potentially conflictwith each other, as object detection aims to detect all the objects, while visual grounding should onlylocalize the referred object and suppress other objects. Benefiting from our unified task instructionsand the strong instruction comprehension capabilities of LLMs, our model performs both taskseffectively and achieves a result of 80.6P@0.5 for visual grounding. With\n",
      "--- [from pdf0]\n",
      " InternImage-H as thebackbone, we achieve 86.7P@0.5 on the validation set of RefCOCO.Instance Segmentation. Instance segmentation involves identifying and segmenting individualobjects within an image. We employ a flexible number of points ( i.e., 8∼24) along the objectboundary to represent an instance mask. Compared to mainstream models specific to instancesegmentation, our model has a comparable mask AP 50(61.2% with InternImage-H [ 59]) but relativelylow mask AP 75. This gap could potentially arise from factors as follows: (1) We discretize the output8Table 2: Experiments of object-level and output format customization. We conduct theseexperiments based on VisionLLM-R50, and report the performance of box AP and mask AP onCOCO minival for (a) and (b), respectively. “#Classes” and “#\n",
      "--- [from pdf0]\n",
      "Points” indicate the number of classesand boundary points, respectively. “*” indicates that we report the mean AP of the given classes, e.g.,10 classes.(a) Object-level customization.#Classes AP AP 50AP75APSAPMAPL10∗48.9 72.6 51.2 31.7 47.5 67.320∗52.7 73.6 56.8 31.8 53.2 70.540∗49.3 70.7 53.2 33.1 53.6 63.880∗44.6 64.0 48.1 26.7 47.9 60.5(b) Output format customization.#Points AP AP 50AP75APSAPMAPL8 18.5 45.7 11.6 9.9 19.7 28.714 22.9 48.3 19.4 11.0 25.1 36.016 24.2 49.9 20.9\n",
      "--- [from pdf0]\n",
      " 11.5 26.3 36.824 25.1 50.0 22.4 12.5 27.4 38.2Table 3: Ablation studies on language-guided image tokenizer and hyper-parameters.(a) Effect of text encoder in thelanguage-guided image tokenizer.w/ BERT Freeze COCO RefCOCO- - 44.7 48.1✓ - 44.8 84.1✓ ✓ 1.3 34.3(b) Effect of image tokenizationmethod.Tokenization APAverage Pooling 23.1Ours 44.8(c) Effect of the num-ber of bins (#Bins).#Bins AP257 34.9513 40.81025 44.82049 44.8coordinates to integers for unifying tasks, which introduces information loss; (2) Due to the memoryand computational constraint, the number of points in our model is limited, which also results in a\n",
      "--- [from pdf0]\n",
      "performance drop; and (3) Point-based methods typically yield lower results compared to direct maskprediction methods, such as Mask R-CNN [22].Image Captioning. We also evaluate our model in a representative vision-language task, i.e.imagecaptioning task, and report the BLEU-4 [ 43] and CIDEr [ 56] metrics. Note that we do not adopt thebeam search [2] or CIDEr optimization [49]. We can observe that VisionLLM achieves competitiveperformance to previous methods. With ResNet-50, we obtain a BLEU-4 score of 31.0and a CIDErscore of 112.5. When using InternImage-H as the backbone, our model achieves a comparableBLEU-4 score of 32.1and a CIDEr score of 114.2. These results demonstrate the effectiveness ofVisionLLM in generating descriptive and contextually relevant captions for images.4\n",
      "--- [from pdf0]\n",
      ".3 Object-Level & Output Format CustomizationOur VisionLLM not only allows for customizing the task description, but also for adjusting thetarget object and the output format using language instructions. Here, we evaluate our model’sfine-grained customization ability on COCO. In particular, to customize the target object, we modifythe<class> in language instructions to change the model’s recognition target from 10classes to80classes. Likewise, to customize the output format, we modify the number of points in languageinstructions to change the task output format. Table 2 shows that our method can perform well forboth object-level and output format changes.4.4 Ablation StudyIn this section, we analyze the effect of key components and hyper-parameters on VisionLLM. Unlessotherwise specified, we use ResNet-50 [ 23] backbone and perform the ablation experiments for objectdetection tasks with random classes and task descriptions on COC\n",
      "--- [from pdf0]\n",
      "O2017 [32].Single Task vs.Multiple Tasks. We perform an ablation study to assess the impact of multi-tasklearning with language instructions on VisionLLM. As shown in Table 1, the single-task trainedmodel VisionLLM-R50 sepis slightly better than the jointly trained model VisionLLM-R50 exceptimage captioning. This is due to the multitasking conflicts that also affect previous generalistmodels [83, 81], and it reflects a trade-off between accuracy and generalization.Text Encoder in Language-Guided Image Tokenizer. We examine the role of text encoder ( i.e.,BERT) in our language-guided image tokenizer in Table 3a, where we report the results for object9detection and visual grounding. The first two rows show that BERT is not essential for objectdetection but it is crucial for visual grounding. We also investigate the effect of freezing the textencoder during training. The\n",
      "--- [from pdf0]\n",
      " last row indicates that freezing BERT hinders the alignment of visionand language modalities and thus degrades the performance for both tasks.Image Tokenization Method. As a comparison to our query-based tokenization, we employ averagepooling on the feature maps from the D-DETR encoder to obtain Mpatch embeddings, which serveas token representations for the image. Results in Table 3b indicate a clear advantage of our method.This is due to its ability to capture information from objects of various sizes in a more flexible way.Number of Localization Tokens. We vary the number of localization tokens from 257(i.e., -128∼128) to 2049 (i.e., -1024 ∼1024), to investigate its impact on visual perception performance. Aspresented in Table 3c, the model consistently exhibits improvement as the number of localizationtokens increases until it reaches a saturation point. Remarkably, a substantial performance boostis observed when the\n",
      "--- [from pdf0]\n",
      " number is raised from 257to1025 (+9.9AP). These results indicate that ahigher number of localization tokens enables the models to achieve finer localization abilities, therebyimproving localization accuracy.5 ConclusionIn this paper, we have presented VisionLLM, a novel framework that leverages the power of largelanguage models (LLMs) to address vision-centric tasks in an open-ended and customizable manner.We have designed unified language instruction that matches the format of language models and coversvarious vision-centric tasks including visual perception. We have also developed a language-guidedimage tokenizer and an LLM-based task decoder that can handle open-ended tasks according to thegiven language instructions. We have verified the effectiveness of our models on a series of taskswith different granularities, demonstrating their remarkable generality and flexibility.Broader Impact. We envision that this work will promote the fusion of visual and language tasks.In addition, since our work is built on open-\n",
      "--- [from pdf0]\n",
      "source pre-trained vision foundation models and largelanguage models, requiring low training resources, thus reducing the carbon footprint. We do notforesee obvious undesirable ethical/social impacts at this moment.10AppendixA Example InstructionsAs described in Sec. 3.2 of the main paper, we follow self-instruct [ 65] to design a set of seedinstructions with placeholders and employ LLMs to create diverse related task descriptions forcoarse-grained task-level customization. Here, we show some examples of instructions for task-levelcustomization, including object detection, instance segmentation, visual grounding, image captioning,and visual question answering (VQA). Following various instructions, our model can elegantly switchamong different vision-centric tasks and accomplish them in a unified manner like LLMs.A.1 Object DetectionExample 1. “Please examine the image and identify all objects in the category set <class> .For eachobject, specify its location within\n",
      "--- [from pdf0]\n",
      " the range <range> by determining the top-left and bottom-rightcorners of its bounding box. To indicate the object’s class and location, provide the output in theformat (c, x1, y1, x2, y2), where ‘c’ represents the class index starting from 0, and (x1, y1, x2, y2)correspond to the offsets of the bounding box corners relative to the center point. The image is:<image> ”Example 2. “Identify all the objects in the image that belong to the category set <class> and predicta bounding box around each one. The output should be a list in the format (c, x1, y1, x2, y2), where crepresents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-leftand\n",
      "--- [from pdf0]\n",
      " bottom-right corners of the box relative to the center point. The coordinates should be within<range> .The image is: <image> ”Example 3. “For each object in the image that is a member of the category set <class> ,output atuple with the index of class label starting from 0 and the offsets of corners relative to the center pointthat encloses the object. The offsets should be in the order of top-left and bottom-right corners of therectangle and should be within <range> .The output format should be (c, x1, y1, x2, y2). The imageis:<image> ”A.2 Instance SegmentationExample 1. “Segment the objects from the image with class labels from <class> and output theircoordinates within range <range> .The coordinates should be given as the boundary points relativeto the center point, and the output format should be (c\n",
      "--- [from pdf0]\n",
      ", x1, y1, x2, y2, ..., x20, y20), where c is theindex of the class label that starts from 0. The image is: <image> ”Example 2. “Segment all the objects from the category set <class> in the provided image and outputa tuple (c, x1, y1, x2, y2, ..., x14, y14) for each, where c is the index of the class label in the categoryset that starts from 0, and (x1, y1, x2, y2, ..., x14, y14) correspond to the offsets of boundary pointson the instance mask relative to the center point which should be within <range> .The image is:<image> ”Example 3. “In the provided image, please segment all the objects in category set <class> withinthe range <range> by providing their coordinates\n",
      "--- [from pdf0]\n",
      " in the (c, x1, y1, x2, y2, ..., x24, y24) format, where‘c’ denotes the index of the class label starting from 0, and (x1, y1, x2, y2, ..., x24, y24) stand for theoffsets of boundary points relative to the center point. The image is: <image> ”A.3 Visual GroundingExample 1. “Please find the object in the category set {<expression>:<cls0>} within the range<range> .Please provide the output in the format (c, x1, y1, x2, y2), where c is the class index startingfrom 0, and (x1, y1, x2, y2) are the offsets of the top-left and bottom-right corners of the boundingbox relative to the center point. The image is: <\n",
      "--- [from pdf0]\n",
      "image> ”Example 2. “Given the input image, category set {<expression>:<cls0>} ,and the range<range> ,please locate the object in the image and output the corresponding coordinates in the tuple(c, x1, y1, x2, y2), where c is the index of the class label starting from 0, and (x1, y1, x2, y2) are the11offsets of the top-left and bottom-right corners of the rectangle relative to the center point. The imageis:<image> ”Example 3. “For each object in the image that belongs to the {<expression>:<cls0>} categoryset, please provide the class label (starting from 0) and the offsets from the center of a boundingbox that encloses the object. The corner offsets should be in the order of top-left and bottom-right,and within the range\n",
      "--- [from pdf0]\n",
      " <range> .The output should be in the format (c, x1, y1, x2, y2). The image is:<image> ”A.4 Image CaptioningExample 1. “The image is <image> .Write a caption: ”Example 2. “The image is <image> .Please describe this image: ”Example 3. “With the objects in the <image> ,please generate a caption for the image: ”A.5 Visual Question AnsweringExample 1. “The image is <image> .Please generate an answer according to the question:<question> . ”Example 2. “The image is <image> .Please answer the question <question> according to theimage . ”Example 3. “With the objects in the <image> ,<question> . ”B Loss FunctionVisionLLM consists of two model components: language-guided image token\n",
      "--- [from pdf0]\n",
      "izer and LLM-basedopen-task decoder. So the total loss Lof our model can be written as:L=Ltok+Ldec, (1)whereLtokandLdecdenote the loss of language-guided image tokenizer and LLM-based open-taskdecoder, respectively. We introduce the two loss functions as follows:Language-Guided Image Tokenizer. Different from the Q-Former [ 29], we use a supervisionmethod similar to that of Deformable DETR [ 82], but with a different loss Ltok: category-agnosticclassification (focal loss [ 31]) and center point regression ( L1loss). As explained in Sec. 3.3, ourimage tokenizer extracts Mimage tokens T={(ei, li)}Mi=1, each of which is represented by anembedding eiand a location li(i.e., absolute coordinates of the center point).LLM-Based Open-\n",
      "--- [from pdf0]\n",
      "Ended Task Decoder. We handle two cases in decoding processing differently.(1) For regular word prediction, we train with standard next-token supervision [ 54,45,6,46]; (2) Forunordered set prediction ( e.g., bounding boxes), we first output a sequence of tokens according to theoutput format (see the output-format-as-query paradigm in Sec. 3.4), then use bipartite matching toalign the LLM-predicted outputs with the ground truths. Despite the differences, we use cross-entropyto compute the loss Ldecin a unified way for both cases.C Training ScheduleAs shown in Figure 5, to speed up the convergence of VisionLLM, we split the training schedule ofVisionLLM into two stages:Stage 1. In this stage, we initialize the language-guided image tokenizer by loading the pre-trainedweights of Deformable DETR [ 82] and BERT [ 16]. Additionally\n",
      "--- [from pdf0]\n",
      ", Alpaca [ 53] is employed as theLLM-based open-ended task decoder. To align visual tokens with text tokens, we make the language-guided image tokenizer trainable while freezing most parameters of the pre-trained Alpaca, with onlya few LoRA [ 24] parameters left tunable. We only focus on object detection in this stage to simplifythe training difficulty, with random task descriptions and object categories.Stage 2. The second stage builds upon the model weights obtained from the first stage. For efficiency,we freeze the visual backbone ( e.g., ResNet [ 23]) in the language-guided image tokenizer. Notably,120102030405001020304050Detection mAP (%)EpochSingle-StageTwo Stages (ours)Stage-1 (Object Detection)Stage-2 (Multiple Tasks)Figure 5: Comparison of two training schedules for VisionLLM. We found that a two-stag\n",
      "--- [from pdf0]\n",
      "etraining from easy to hard converges faster than a single-stage training.Table 4: More ablation studies for VisionLLM.(a) Effect of randomness.Randomness APNone 45.2+ Random Task Description 45.1++ Random Object Category 44.8+++ Random Output Format 44.6(Multi-task Joint Training)(b) Effect of LoRA [24].LoRA Randomness AP✗ ✗ 45.2✗ ✓ 1.2✓ ✓ 44.8(c) Effect of the numberof image tokens.#Tokens AP50 44.5100 44.8200 45.1300 45.2(d) Effect of Seq2Seq.Seq2Seq AP✓ -✗ 44.8(e) Large vocabulary object detection.Dataset #Classes APCOCO 80 44.8LVIS 1203 18.9this stage introduces the unified supervision of multiple tasks, including object detection,\n",
      "--- [from pdf0]\n",
      " instancesegmentation, visual grounding, image captioning, and VQA, facilitating the model to leverage thepower of LLMs to understand and manipulate visual information holistically.D More Ablation StudiesIn this section, we provide more ablation studies and analysis of VisionLLM. Unless otherwisespecified, we use ResNet-50 [ 23] backbone and perform the ablation experiments for object detectiontasks with random task descriptions and object categories on COCO 2017 [32].Randomness. In Table 4a, we examine the effect of introducing randomness during training forVisionLLM, including randomness in task descriptions, object categories, and output formats ( i.e.,multi-task joint training). Initially, without any randomness, the model achieves a box AP of 45.2.However, as randomness is gradually applied, interesting phenomena emerge: while there is a slightdecrease ( 45.2→44.6) in the AP of standard\n",
      "--- [from pdf0]\n",
      " detection with the introduction of randomness, theoverall benefits of enhanced task customization and open-ended capabilities outweigh this minortrade-off. Overall, introducing randomness during training in VisionLLM positively impacts itscapacity for open-ended tasks and customization.Low-Rank Adaptation (LoRA). As shown in Table 4b, when randomness is not applied, themodel achieves 45.2box AP without using LoRA [ 24]. However, when randomness is employed,it is observed that the model fails to converge without using LoRA. Conversely, when LoRA andrandomness are used together, the model is able to converge. This indicates that LoRA plays a crucialrole as a bridge between the language and visual tokens, enabling effective alignment between thetwo modalities and improving the convergence of the overall system.Number of Image Tokens. We vary the number of image tokens from 50to300to investigatetheir impact on the performance. Results are presented in Table 4\n",
      "--- [from pdf0]\n",
      "c. As the number of image tokensincreases, the performance continues to improve. This makes sense because a larger number of131 2 3 4 5 6 7 844.8 44.7 44.8 44.7 44.8 44.7 44.7 44.8PromptAPFigure 6: Evaluation results using eight different prompts. The first six prompts use different taskdescriptions of object detection, while the last two prompts employ random category orders. Theseresults show that the performance of different prompts is similar, only a 0.1AP gap is observed.bicyclecarmotorcycleairplanebustraintruckboattraffic lightfire hydrantstop signparking meterbenchbirdcatdoghorsesheepcowelephantbearzebragiraffebackpackumbrellahandbagtiesuitcasefrisbeeskissnowboardsports ballkitebaseball batbaseball gloveskateboardsurfboardtennis racketbottlewine glasscupfork\n",
      "--- [from pdf0]\n",
      "knifespoonbowlbananaapplesandwichorangebroccolicarrothot dogpizzadonutcakechaircouchpotted plantbeddining tabletoilettvlaptopmouseremotekeyboardcell phonemicrowaveoventoastersinkrefrigeratorbookclockvasescissorsteddy bearhair driertoothbrush010203040506070mAP (%)standard 80 classesrandom 40 classes(a) We randomly select 40 classes in random order to form the category set.bicyclecarmotorcycleairplanebustraintruckboattraffic lightfire hydrantstop signparking meterbenchbirdcatdoghorsesheepcowelephantbearzebragiraffebackpackumbrellahandbagtiesuitcasefrisbeeskissnowboardsports ballkitebaseball batbaseball gloveskateboardsurfboardtennis racketbottlewine glasscupforkknifespoonbowlbananaapplesandwichorangebrocc\n",
      "--- [from pdf0]\n",
      "olicarrothot dogpizzadonutcakechaircouchpotted plantbeddining tabletoilettvlaptopmouseremotekeyboardcell phonemicrowaveoventoastersinkrefrigeratorbookclockvasescissorsteddy bearhair driertoothbrush010203040506070mAP (%)standard 80 classesrandom 80 classes(b) We randomly change the order of 80 classes to form the category set.Figure 7: Per-category AP on COCO dataset. We randomly select some categories to form thecategory set <class> in language instructions.image tokens provides a more detailed description of the image content. Considering computationalcomplexity, we adopted 100image tokens in our experiments.Robustness to Prompt Changes. Since VisionLLM is trained with random prompts, includingrandom task descriptions and random categories, one may ask whether there is a large performancevariance across different prompts. To validate the stability of VisionLLM, we conduct experimentsusing eight different prompts\n",
      "--- [from pdf0]\n",
      ". The first six prompts employ different task descriptions, while the lasttwo prompts involve random category orders. In the case of random category orders, we map thecategories back to the COCO standard category order for evaluation. As shown in Figure 6, mostevaluation results are distributed closely to 44.8AP. The performance differences among prompts aremarginal, demonstrating that VisionLLM is robust to different prompts.Instruction Following Capability. As shown in Figure 7, when the prompt only contains 40classes,the performance for these categories remains normal, while the performance for the remainingcategories is close to zero. This indicates that VisionLLM can dynamically detect objects based onthe given class set <class> in instructions while disregarding the other classes that are not mentioned.This result highlights the flexibility of VisionLLM in adhering to instructions.Output-Format-As-Query vs.Seq2Seq. In VisionLLM, we introduce the output-format-\n",
      "--- [from pdf0]\n",
      "as-queryframework for LLM decoder. Alternatively, we also experiment with the sequence generation methodlike Pix2Seq [ 8] for object detection with random task descriptions and object categories. However,we find that the loss is hard to converge in this paradigm, which indicates that the seq2seq decodingmay need a more detailed design or a longer training schedule for the open-ended visual tasks, whilethe proposed output-format-as-query framework is more effective for open-ended tasks.Large-Vocabulary Object Recognization. To validate the capacity of VisionLLM in the large-vocabulary scenario, we further conduct the experiments on the challenging dataset LVIS [ 20] with1203 categories. Due to the limited number of language tokens, we randomly select 80 classesfor training in each iteration. During inference, we divide the 1203 categories into 16 groups and14#Points=8#Points=14#Points=16#Points=24Figure 8: Customization\n",
      "--- [from pdf0]\n",
      " of instance masks using the different number of points. Notably, weonly modify the output format mentioned in the prompt, i.e.the number of segmentation points. Formore details, please see the example prompts provided in Sec. A.2.predict the results in a sliding-window manner. As shown in Table 4e, without tricks like federal loss,VisionLLM-R50 can achieve 18.9mAP on LVIS.E Qualitative AnalysisCustomization of Segmentation Points. In this experiment, we focus on modifying the outputformat mentioned in the prompt , specifically the number of points for instance segmentation (seeSec. A.2). The results are visualized in Figure 8. Remarkably, by increasing the number of pointsfor segmentation, we observe that the model successfully predicts more refined object masks. Thisvalidates the capability of our method to precisely customize the output format, showcasing fine-grained control over the segmentation process.Custom\n",
      "--- [from pdf0]\n",
      "ization of Category Set. We change the content of the category set <class> in languageinstructions and visualize the predictions in Figure 9. It is observed that the model can correctlypredict the object category according to the provided category set. This demonstrates that VisionLLMhas a strong capacity to understand and reason over the semantic information of language instructions,which allows it for flexible category customization in open-vocabulary scenarios.Image Description & VQA. Benefiting from the power of LLMs, VisionLLM exhibits a strongability in generating long descriptions for images and answering visual questions with complexreasoning. We show the examples in Figure 10.15Instruction:“Identify the objects in the image that belong to {‘person’: <c0>, ..., ‘frisbee': <c29>, ...}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1,\n",
      "--- [from pdf0]\n",
      " y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”Instruction:“Identify the objects in the image that belong to {‘person’: <c0>, ‘bicycle’: <c1>, ..., ‘backpack': <c24>, ..., ‘toothbrush’: <c79>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1,\n",
      "--- [from pdf0]\n",
      " y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”Instruction :“Identify the objects in the image that belong to {‘frisbee': <c0>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”Instruction:“Identify the objects in the image that belong to {‘bicycle\n",
      "--- [from pdf0]\n",
      "’: <c0>, ‘surfboard’: <c1>, ‘sports ball': <c2>, ‘backpack’: <c3>, ‘the man wearing blue T-shirt’: <c4>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”Figure 9: Customization of category set. These results demonstrate that VisionLLM can correctlydetect the specified objects according to the category set <class> provided in language instructions.16Inst\n",
      "--- [from pdf0]\n",
      "ruction:“The image is: <image>.Analyze the image in a comprehensiveand detailedmanner.”VisionLLM:“The image features a busy city street with a large red double-decker bus driving down the road. There are several people walking on the sidewalk, some of whom are carrying backpacks. The bus is positioned towards the left side of the image. There are several people walking along the sidewalk, with some closer to the bus and others further away. The overall atmosphere suggests a busy urban environment with people going about their daily routines.”VisionLLM:“The image features a large, open-air market filled with a variety of fruits and vegetables. There are numerous mangoes, watermelons and apples displayed in the market, with some fruitsplaced closer to the foreground and others further back. The market is bustling with activity as people can be seen browsing and purchasing the fresh produce. The market is likely a popular destination for those\n",
      "--- [from pdf0]\n",
      " seeking a fresh and healthy diet.”Instruction:“The image is: <image>.Write a detaileddescription of the given image.”Instruction:“The image is: <image>.Write a shortdescription of the given image.”VisionLLM:“The image features a lively open-air market with a variety of fruits and vegetables. People are busy browsing and buying fresh produce.”(a) Image description with controllable text lengthInstruction:“The image is: <image>. Please generate an answer according to the question: what reasons could there be for the cat squatting on the shoes?”VisionLLM:“There could be several reasons. One possibility is that the cat is seeking warmth or comfort, as shoes can provide a cozy and warm surface for the cat to rest on. Another reason could be that the cat is seeking attention or affection from its owner, who is standing nearby. Cats\n",
      "--- [from pdf0]\n",
      " are known to be curious creatures, and they may be seeking attention or attention from their owners.”VisionLLM:“The working environment in this office space appears to be open, spacious, and well-equipped. There are several desks set up for employees, providing ample workspace for each individual. The office features a modern and contemporary design, which suggests a creative and collaborative work environment. The presence of several chairs and the organization of the office, including the laptops and monitors, indicate that the space is well-maintained and conducive to productivity. Overall, the office appears to be a pleasant and efficient work environment for its employees.”Instruction:“The image is: <image>.What do you think of the working environment here?”(b) Visual question answering with reasoningFigure 10: Visualization of the image description and VQA capabilities of VisionLLM.17References[1]Jean-Baptiste Alayrac, Jeff Donah\n",
      "--- [from pdf0]\n",
      "ue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shotlearning. arXiv preprint arXiv:2204.14198 , 2022. 1, 4[2]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and LeiZhang. Bottom-up and top-down attention for image captioning and visual question answering. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2018. 9[3]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conferenceon Computer\n",
      "--- [from pdf0]\n",
      " Vision , 2015. 4[4]Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting viaimage inpainting. Advances in Neural Information Processing Systems , 2022. 4[5]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for videounderstanding? In International Conference on Machine Learning , 2021. 7[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in Neural Information Processing Systems , 2020. 3, 4, 12[7]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\n",
      "--- [from pdf0]\n",
      " and SergeyZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision ,2020. 3, 4, 8[8]Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modelingframework for object detection. arXiv preprint arXiv:2109.10852 , 2021. 8, 14[9]Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequenceinterface for vision tasks. arXiv preprint arXiv:2206.07669 , 2022. 4, 8[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, andC Lawrence Zitnick. Microsoft coco\n",
      "--- [from pdf0]\n",
      " captions: Data collection and evaluation server. arXiv preprintarXiv:1504.00325 , 2015. 4, 7[11] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and JingjingLiu. Uniter: Universal image-text representation learning. In Computer Vision–ECCV 2020: 16th EuropeanConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX . Springer, 2020. 8[12] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformeradapter for dense predictions. In International Conference on Learning Representations , 2023. 1[13] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\n",
      "--- [from pdf0]\n",
      " generation.InInternational Conference on Machine Learning , 2021. 8[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXivpreprint arXiv:2210.11416 , 2022. 4[15] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS , 2023. 3[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-tional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 6, 12[17] Alexey Dosovitskiy, Lucas Beyer, Alexander K\n",
      "--- [from pdf0]\n",
      "olesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image isworth 16x16 words: Transformers for image recognition at scale. In International Conference on LearningRepresentations , 2021. 6[18] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprintarXiv:2211.07636 , 2022. 1[19] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarialtraining for vision-and-language representation learning. Advances in Neural Information ProcessingSystems , 2020.\n",
      "--- [from pdf0]\n",
      " 818[20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.InProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5356–5364,2019. 14[21] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose visionsystems. arXiv preprint arXiv:2104.00743 , 2021. 4[22] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of theIEEE/CVF International Conference on Computer Vision , 2017. 3, 8, 9[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.InProceedings of\n",
      "--- [from pdf0]\n",
      " the IEEE Conference on Computer Vision and Pattern Recognition , 2016. 6, 7, 8, 9, 12,13[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,2021. 7, 12, 13[25] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, KurtShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instructionmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. 4[26] Menglin J\n",
      "--- [from pdf0]\n",
      "ia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, andSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision , 2022. 2[27] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision , 2021. 8[28] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision andvision-language tasks. arXiv preprint arX\n",
      "--- [from pdf0]\n",
      "iv:2211.09808 , 2022. 4, 7, 8[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 4, 5, 12[30] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, andYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 2, 4, 5[31] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense objectdet\n",
      "--- [from pdf0]\n",
      "ection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2017. 12[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference onComputer Vision . Springer, 2014. 7, 9, 13[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprintarXiv:2304.08485 , 2023. 4, 5, 7[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFInternational Conference on Computer\n",
      "--- [from pdf0]\n",
      " Vision , 2021. 6[35] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, YangYang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatbotsbeyond language. arXiv preprint arXiv:2305.05662 , 2023. 2, 4, 5[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference onLearning Representations . 7[37] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprintarXiv:1608.03983 , 2016. 7[38] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh M\n",
      "--- [from pdf0]\n",
      "ottaghi, and Aniruddha Kembhavi. Unified-io: Aunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916 , 2022. 4[39] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , 2016. 719[40] OpenAI. Gpt-4 technical report. arXiv , 2023. 3[41] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI , 2022. 1, 3[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama\n",
      "--- [from pdf0]\n",
      ", Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems , 2022. 3, 4[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association for ComputationalLinguistics , 2002. 9[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training. 2018. 1, 4[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training. OpenAI , 2018. 12[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, D\n",
      "--- [from pdf0]\n",
      "ario Amodei, Ilya Sutskever, et al. Languagemodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. 12[47] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, GabrielBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.arXiv preprint arXiv:2205.06175 , 2022. 4[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detectionwith region proposal networks. In Advances in Neural Information Processing Systems , 2015. 8[49] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross\n",
      "--- [from pdf0]\n",
      ", and Vaibhava Goel. Self-criticalsequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition , 2017. 9[50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 , 2023. 2, 4,5[51] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou,and Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. InProceedings of the IEEE/CVF International Conference on Computer Vision , 2023. 1[52]\n",
      "--- [from pdf0]\n",
      " Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modelingfor self-supervised vision representation learning. arXiv preprint arXiv:2206.01204 , 2022. 1[53] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center forResearch on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 2023. 3, 6, 7, 12[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix\n",
      "--- [from pdf0]\n",
      ",Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971 , 2023. 3, 6, 7, 12[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ŁukaszKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems ,30, 2017. 6[56] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image descriptionevaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2015. 9[57] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu\n",
      "--- [from pdf0]\n",
      ", Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprintarXiv:2205.14100 , 2022. 1, 4[58] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052 , 2022. 1, 4, 7[59] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, LeweiLu, Hongsheng Li, et al\n",
      "--- [from pdf0]\n",
      ". Internimage: Exploring large-scale vision foundation models with deformableconvolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023. 1, 2,7, 820[60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, andLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media ,8(3):415–424, 2022. 6[61] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beitpretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208\n",
      "--- [from pdf0]\n",
      ".10442 , 2022. 1, 2[62] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: Ageneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499 , 2022. 2[63] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: Ageneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499 , 2022. 4[64] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:Segmenting everything in context. arXiv preprint arXiv:2304.03284 , 2023. 2, 4[65] Yizhong Wang, Yeganeh Kordi, Swaroop Mish\n",
      "--- [from pdf0]\n",
      "ra, Alisa Liu, Noah A Smith, Daniel Khashabi, andHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXivpreprint arXiv:2212.10560 , 2022. 4, 5, 11[66] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, AnjanaArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarkinggeneralization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705 ,2022. 4[67] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\n",
      "--- [from pdf0]\n",
      " Du, Andrew MDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 ,2021. 4[68] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 , 2023. 2, 4,5[69] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo.Polarmask: Single shot instance segmentation with polar representation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition , 2020. 6, 8[70] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang,\n",
      "--- [from pdf0]\n",
      " Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instanceperception as object discovery and retrieval. arXiv preprint arXiv:2303.06674 , 2023. 4[71] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, andLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In EuropeanConference on Computer Vision , pages 521–539. Springer, 2022. 4, 7[72] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, andLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In EuropeanConference on Computer Vision , 2022. 8[73] Zhengy\n",
      "--- [from pdf0]\n",
      "uan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning andaction. arXiv preprint arXiv:2303.11381 , 2023. 2, 4, 5[74] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorfulprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797 , 2021. 2[75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context inreferring expressions. In European Conference on Computer Vision . Springer, 2016\n",
      "--- [from pdf0]\n",
      ". 7[76] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and languageprompt learning. arXiv preprint arXiv:2210.07225 , 2022. 2[77] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, WendiZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 ,2022. 3[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, ChristopherDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.arXiv\n",
      "--- [from pdf0]\n",
      " preprint arXiv:2205.01068 , 2022. 321[79] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673 ,2022. 2[80] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.4[81] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and JifengDai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. arXiv preprintarXiv:2206.04674 , 2022. 1, 4, 8, 9[\n",
      "--- [from pdf0]\n",
      "82] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformabletransformers for end-to-end object detection. In International Conference on Learning Representations ,2021. 3, 6, 7, 8, 12[83] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022. 2, 4, 5, 8, 922\n",
      "--- [from docx0]\n",
      "Housing shortage in big cities can cause severe social consequences. Some people think only government action can solve this problem. To what extent do you agree or disagree?Nowadays, rural-urban migration has become an irreversible trend globally, burdening big cities with great population density and housing shortage. Many believe that only government can tackle this issue and I agree with the statement.The root cause for inadequate housing is the severe tension between the intense demand and low supply. Divergence in job growth, incomes and future prospects between strong-performing cities and those lagging behind has been starkly prominent, which leads to the massive and continuous flow of people into mega-cities and economic centers. It is just this mobility of labor that contributes to an increasing housing market. Meanwhile, the scarcity of land in these cities means a weak supply of houses. This incompatibility has decided that it is not a situation individuals can alter.However, if the government intervenes, this conundrum can be at least eased to\n",
      "--- [from docx0]\n",
      " some extent. For example, from the perspective of demand, the reallocation of high value-added industries to satellite or second tier cities will definitely alleviate the population overload and at same time facilitate the rise of such medium size cities. Meanwhile, from the angle of supply, the industries transfer will make room for more houses and the government can simultaneously implement regulations or taxes to control the real estate speculation so as to release the storage to people in real needs. Moreover, if the shortage still continues, urban expansion is another possible option. By transforming the farmlands into houses and apartments, the situation will improve.To conclude, I believe that housing crisis is never an issue that individual can tackle and only with government involvement will this be addressed or at least mitigated.\n"
     ]
    }
   ],
   "source": [
    "print(type(text_chunks), type(text_chunks[0]), len(text_chunks))\n",
    "for chunk in text_chunks:\n",
    "    print('---', chunk.page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embdedding\n",
    "- I/O\n",
    "    - Input: text_chunks \n",
    "    - Output: Search Space\n",
    "\n",
    "- Performance\n",
    "    - Using openai_embeddings -> ~4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.259869575500488\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "search_space = Chroma.from_documents(text_chunks, openai_embeddings)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = \"align the vision task to language model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3242034912109375\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "search_result = search_space.similarity_search(query)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [from pdf0]\n",
      " provides a consistent interface for vision-centric task definition and customization;(2) a language-guided image tokenizer, which encodes visual information in alignment with the givenlanguage prompt, enabling the model to comprehend and parse the visual content effectively; and(3) an LLM-based open-task decoder, which utilizes the encoded visual information and languageinstructions to generate satisfactory predictions or outputs. The three designs work together to achievea flexible and open-ended framework that can handle various vision-centric tasks at different levels oftask customization through language instructions.Different from previous interactive systems [ 68,73,50,35,30] that rely on APIs, our VisionLLMpresents a more flexible and end-to-end pipeline. Given language instructions that describe the currenttasks and an input image, the model first uses a language-guided image tokenizer to encode the imagetokens based on the given prompt. Then, the image tokens and language instructions are fed to \n",
      "\n",
      "--- [from pdf0]\n",
      " tonumerous real-world vision tasks is challenging. Moreover, defining the visual prompts as imageinpainting is inconsistent with the language instructions in LLMs, hard to leverage the reasoning,parsing ability, and world knowledge of LLMs. In this work, we aim to align vision-centric taskswith language tasks, use language instructions to unifiedly and flexibly define all tasks, and solvethem with a shared LLM-based task decoder.3 VisionLLM3.1 Overall ArchitectureThis work targets to provide a unified generalist framework that can seamlessly integrate the strengthsof large language models (LLMs) with the specific requirements of vision-centric tasks. As shown in4RandomQueryLanguage-GuidedImage Token...<text><text>Desired Output:<c1> <p1> <p3> ...Open-Ended Task Decoder with LLMLanguage-GuidedImageTokenizerBackbone...Vision-languageexample:\"Describetheimage< \n",
      "\n",
      "--- [from pdf0]\n",
      " throughlanguage instructions, embodying a flexible and open-ended approach that is not constrained bypre-set tasks. This versatility makes VisionLLM a robust and powerful generalist model for visionand vision-language tasks, opening up new possibilities for the development of unified generalistmodels that bridge the domains of vision and language.In summary, our main contributions are as follows:(1) We propose VisionLLM, the first framework that leverages the power of LLMs to address vision-centric tasks in an open-ended and customizable manner. By aligning the definitions of vision-centrictasks with LLM methodologies, VisionLLM breaks new ground in enabling the unified modeling ofvision and language, opening up possibilities for advancing the field.(2) We overcome many difficulties when porting LLMs to vision-centric tasks, by designing unifiedlanguage instruction that matches the format of language models and covers various vision-centrictasks including visual perception. Correspondingly, we develop a language-guided \n",
      "\n",
      "--- [from pdf0]\n",
      " like “ The image is <image> .Please generatean answer for the image according to the question: <question> ”. Here, <image> and<question>are the placeholdersok of the image tokens and the question, respectively.Vision-Only Tasks. Designing effective language instructions for vision tasks is a challengingendeavor due to the differences in modality and task format between vision and language. Here, wedescribe vision tasks by providing a task description and specifying the desired output format vialanguage instructions.(1) The task description conveys the intended task to the language model. Following self-instruct [ 65],we design a set of seed instructions with placeholders and employ LLMs to generate a large numberof related task descriptions and randomly select one of them during training.5(2) For conventional visual perception tasks like object detection and instance segmentation, wepropose a unified output format represented as a tuple (C, P), where Cden \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in search_result:\n",
    "    print('---', result.page_content, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
