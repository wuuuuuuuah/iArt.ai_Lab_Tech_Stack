{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders\n",
    "- I/O\n",
    "    - Input: files\n",
    "    - Output: text string\n",
    "- Test Files\n",
    "    - VisionLLM.pdf\n",
    "    - Random .txt file\n",
    "    - one .docx with one IELTS Writing Example inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files in a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './test_files_directory'\n",
    "\n",
    "directory_loader = DirectoryLoader(\n",
    "    path = directory_path,\n",
    "    glob = \"**/[!.]*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
      "Falling back to partitioning with ocr_only.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.846291303634644\n"
     ]
    }
   ],
   "source": [
    "# Test on one .txt one VisionLLM Paper -> 35.1s\n",
    "start = time.time()\n",
    "docs_from_directory = directory_loader.load()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78041\n"
     ]
    }
   ],
   "source": [
    "print(len(str(docs_from_directory[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'aligns the definitions of vision-centric tasks with the ' in str(docs_from_directory[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_from_directory:\n",
    "    texts.append(str(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.txt', 'w') as f:\n",
    "    f.write(str(docs_from_directory[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Content from Web Link\n",
    "- Using Unstructured Warpped by Langchain\n",
    "- Using requests & beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = ['https://en.wikipedia.org/wiki/Pok%C3%A9mon']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Langchain Unstructured\n",
    "- One Wikipedia Link above, takes 0.57s\n",
    "- A lot of \\n\\n in here (might because of \\n\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_loader = UnstructuredURLLoader(\n",
    "    urls = web_url,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5237431526184082\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "url_data = url_loader.load()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.txt', 'w') as f:\n",
    "    f.write(str(url_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(url_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The franchise began as Pocket Monsters: Red and Green' in str(url_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using requests + beautifulsoup4\n",
    "- takes 0.61 but with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_text_content_from_link(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the desired elements and extract their text content\n",
    "        text_content = ''\n",
    "        for element in soup.find_all('p'):  # Example: Extract text from all <p> tags\n",
    "            text_content += element.get_text() + '\\n'\n",
    "\n",
    "        return text_content\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from {url}. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.5877127647399902\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "text_web = ''\n",
    "for i in range(len(web_url)):\n",
    "    try:\n",
    "        web_content = get_text_content_from_link(web_url[i])\n",
    "        lines = web_content.split('\\n')\n",
    "        for line in lines:\n",
    "            if len(line) > 100:\n",
    "                web_content += line + '\\n'\n",
    "        text_web += ''.join(['',f'[this text is from url{i}]', '\\n', web_content])\n",
    "        print(i)\n",
    "    except:\n",
    "        pass\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The franchise began as Pocket Monsters: Red and Green' in text_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.append(text_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.txt', 'w') as f:\n",
    "    f.write(text_web)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .pdf File\n",
    "- Using PyPDF2 -> much faster than the Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It opens the file in binary mode, extract the text from each page\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for i in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[i]\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3291749954223633\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pdf_path = './test_files_directory/VisionLLM - Large Language Model is also an open-Ended Decoder for Vision-Centric Tasks.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.append(''.join(['[from pdf0]\\n', pdf_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'aligns the definitions of vision-centric' in pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.txt', 'w') as f:\n",
    "    f.write(pdf_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .doc Files\n",
    "- Takes 0.005s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_doc(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028154850006103516\n"
     ]
    }
   ],
   "source": [
    "doc_file_path = './test_files_directory/test.docx'\n",
    "\n",
    "start = time.time()\n",
    "text_docx = extract_text_from_doc(doc_file_path)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Nowadays, rural-urban migration has become an irreversible trend globally' in text_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.append(''.join(['[from docx0]\\n', text_docx]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save those text to .txt for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[this text is from url0]\\n\\n\\nPokémon[a][1][2][3] (an abbreviation for Pocket Monsters[b] in Japanese) is a Japanese media franchise managed by The Pokémon Company, founded by Nintendo, Game Freak, and Creatures. The franchise was created by Satoshi Tajiri in 1996,[4] and is centered around fictional creatures called \"Pokémon\". In Pokémon, Pokémon Trainers are people who catch, train, care for, and battle with Pokémon. The English slogan for the franchise is \"Gotta Catch \\'Em All!\".[5][6] There are currently 1015 Pokémon species.[7]\\n\\nThe franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996. Pokémon soon became a media mix franchise adapted into various different media.[8] Pokémon is one of the highest-grossing media franchises of all time. The Pokémon video game series is the third best-selling video game franchise of all time with more than 480 million copies sold[9] and one\\xa0billion mobile downloads.[10] The Pokémon video game series spawned an anime television series that has become the most successful video game adaptation of all time[11] with over 20 seasons and 1,000 episodes in 192 countries.[9] The Pokémon Trading Card Game is the highest-selling trading card game of all time[12] with over 52.9\\xa0billion cards sold. In addition, the Pokémon franchise includes the world\\'s top-selling toy brand,[13] an anime film series, a live-action film (Detective Pikachu), books, manga comics, music, merchandise, and a temporary theme park. The franchise is also represented in other Nintendo media, such as the Super Smash Bros. series, where various Pokémon characters appear as both playable and non-playable characters.\\n\\nThe Pokémon franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996.[14] Pokémon soon became a media mix franchise adapted into various different media, with the Pokémon Trading Card Game released in October 1996, the Pokémon Adventures manga first released in Japan in March 1997, and the Pocket Monsters: Original Series released in April 1997. Pocket Pikachu was released in Japan in March 1998, with the first ever Pokémon film, Pokémon: The First Movie, first released in Japan in July 1998.[15]\\n\\nIn 1998, Nintendo spent $25 million promoting Pokémon in the United States in partnership with Hasbro, KFC, and others.[16] Nintendo initially feared that Pokémon was too Japanese for Western tastes but Alfred Kahn, then CEO of 4Kids Entertainment, convinced the company otherwise.[17] The one who spotted Pokémon\\'s potential in the United States was Kahn\\'s colleague Thomas Kenney.[18]\\n\\nIn November 2005, 4Kids Entertainment, which had managed the non-game related licensing of Pokémon, announced that it had agreed not to renew the Pokémon representation agreement. The Pokémon Company International oversees all Pokémon licensing outside Asia.[19] In 2006, the franchise celebrated its tenth anniversary with the release of Pokémon Diamond and Pearl.[20] In 2016, the Pokémon Company celebrated Pokémon\\'s 20th anniversary by airing an ad during Super Bowl 50 in January and re-releasing the first Pokémon video games 1996 Game Boy games Pokémon Red, Green (only in Japan), and Blue, and the 1998 Game Boy Color game Pokémon Yellow for the Nintendo 3DS on February 26, 2016.[21][22] The mobile augmented reality game Pokémon Go was released in July 2016.[23] Pokémon Sun and Moon also released in the same year. The first live-action film in the franchise, Pokémon Detective Pikachu, based on the 2018 Nintendo 3DS spin-off game Detective Pikachu, was released in 2019.[24] The eighth generation of core series games began with Pokémon Sword and Shield, released worldwide on the Nintendo Switch on November 15, 2019.\\n\\nTo celebrate its 25th anniversary, the company released two additional titles for the Nintendo Switch: Pokémon Brilliant Diamond and Shining Pearl, remakes of the Nintendo DS Pokémon Diamond and Pearl games, on November 19, 2021, and its \"premake\" Pokémon Legends: Arceus, which was subsequently released on January 28, 2022.[25][26]\\n\\nPokémon Scarlet and Violet began the ninth generation of the game series when they released worldwide for the Nintendo Switch on November 18, 2022.[27]\\n\\nThe name \"Pokémon\" is a syllabic abbreviation of the Japanese brand Pocket Monsters.[28] The term \"Pokémon\", in addition to referring to the Pokémon franchise itself, also collectively refers to the many fictional species that have made appearances in Pokémon media. \"Pokémon\" is identical in the singular and plural, as is each individual species name; it is and would be grammatically correct to say \"one Pokémon\" and \"many Pokémon\", as well as \"one Pikachu\" and \"many Pikachu\".[29]\\n\\nPokémon executive director Satoshi Tajiri first thought of Pokémon, albeit with a different concept and name, around 1989, when the Game Boy was released. The concept of the Pokémon universe, in both the video games and the general fictional world of Pokémon, stems from the hobby of insect collecting, a popular pastime which Tajiri enjoyed as a child.[30] Players are designated as Pokémon Trainers and have three general goals: to complete the regional Pokédex by collecting all of the available Pokémon species found in the fictional region where a game takes place, to complete the national Pokédex by transferring Pokémon from other regions, and to train a team of powerful Pokémon from those they have caught to compete against teams owned by other Trainers so they may eventually win the Pokémon League and become the regional Champion. These themes of collecting, training, and battling are present in almost every version of the Pokémon franchise, including the video games, the anime and manga series, and the Pokémon Trading Card Game (also known as TCG).\\n\\nIn most incarnations of the Pokémon universe, a Trainer who encounters a wild Pokémon has the ability to capture that Pokémon by throwing a specially designed, mass-producible spherical tool called a Poké Ball at it. If the Pokémon is unable to escape the confines of the Poké Ball, it is considered to be under the ownership of that Trainer. Afterwards, it will obey whatever commands it receives from its new Trainer, unless the Trainer demonstrates such a lack of experience that the Pokémon would rather act on its own accord. Trainers can send out any of their Pokémon to wage non-lethal battles against other Pokémon; if the opposing Pokémon is wild, the Trainer can capture that Pokémon with a Poké Ball, increasing their collection of creatures. In Pokémon Go, and in Pokémon: Let\\'s Go, Pikachu! and Let\\'s Go, Eevee!, wild Pokémon encountered by players can be caught in Poké Balls, but most cannot be battled. Pokémon already owned by other Trainers cannot be captured, except under special circumstances in certain side games. If a Pokémon fully defeats an opponent in battle so that the opponent is knocked out (\"faints\"), the winning Pokémon gains experience points and may level up. Beginning with Pokémon X and Y, experience points are also gained from catching Pokémon in Poké Balls. When leveling up, the Pokémon\\'s battling aptitude statistics (\"stats\", such as \"Attack\" and \"Speed\") increase. At certain levels, the Pokémon may also learn new moves, which are techniques used in battle. In addition, many species of Pokémon can undergo a form of metamorphosis and transform into a similar but stronger species of Pokémon, a process called evolution; this process occurs spontaneously under differing circumstances, and is itself a central theme of the series. Some species of Pokémon may undergo a maximum of two evolutionary transformations, while others may undergo only one, and others may not evolve at all. For example, the Pokémon Pichu may evolve into Pikachu, which in turn may evolve into Raichu, following which no further evolutions may occur. Pokémon X and Y introduced the concept of \"Mega Evolution,\" by which certain fully evolved Pokémon may temporarily undergo an additional evolution into a stronger form for the purpose of battling; this evolution is considered a special case, and unlike other evolutionary stages, is reversible.\\n\\nIn the main series, each game\\'s single-player mode requires the Trainer to raise a team of Pokémon to defeat many non-player character (NPC) Trainers and their Pokémon. Each game lays out a somewhat linear path through a specific region of the Pokémon world for the Trainer to journey through, completing events and battling opponents along the way (including foiling the plans of an evil team of Pokémon Trainers who serve as antagonists to the player). Excluding Pokémon Sun and Moon and Pokémon Ultra Sun and Ultra Moon, the games feature eight powerful Trainers, referred to as Gym Leaders, that the Trainer must defeat in order to progress. As a reward, the Trainer receives a Gym Badge, and once all eight badges are collected, the Trainer is eligible to challenge the region\\'s Pokémon League, where four talented trainers (referred to collectively as the \"Elite Four\") challenge the Trainer to four Pokémon battles in succession. If the trainer can overcome this gauntlet, they must challenge the Regional Champion, the master Trainer who had previously defeated the Elite Four. Any Trainer who wins this last battle becomes the new champion.\\n\\nPokémon is set in the fictional Pokémon universe. There are numerous regions that have appeared in the various media of the Pokémon franchise. There are 9 main series regions set in the main series games: Kanto, Johto, Hoenn, Sinnoh/Hisui, Unova, Kalos, Alola, Galar, and Paldea. Each of the nine generations of the main series releases focuses on a new region. Every region consists of several cities and towns that the player must explore in order to overcome many waiting challenges, such as Gyms, Contests and villainous teams. At different locations within each region, the player can find different types of Pokémon, as well as helpful items and characters. Different regions are not accessible from one another at all within a single game, only with the exception of Kanto and Johto being linked together in Pokémon Gold, Silver, Crystal, HeartGold and SoulSilver versions. There are also regions set in spinoff games and two islands in the Pokémon anime (Orange Islands and Decolore Islands), all still set within the same fictional universe.\\n\\nEach main series region in the Pokémon universe is based on a real world location. The first four regions introduced are based on locations in Japan, being Kantō, Kansai, Kyushu, and Hokkaidō, with later regions being based on parts of New York City, France, Hawaii, the United Kingdom, and the Iberian Peninsula.[31][32]\\n\\nAll of the licensed Pokémon properties overseen by the Pokémon Company International are divided roughly by generation. These generations are roughly chronological divisions by release; every several years, when a sequel to the 1996 role-playing video games Pokémon Red and Green is released that features new Pokémon, characters, and gameplay concepts, that sequel is considered the start of a new generation of the franchise. The main Pokémon video games and their spin-offs, the anime, manga, and trading card game are all updated with the new Pokémon properties each time a new generation begins.[33] Some Pokémon from the newer games appear in anime episodes or films months, or even years, before the game they were programmed for came out. The first generation began in Japan with Pokémon Red and Green on the Game Boy. As of 2022, there are nine generations of main series video games. The most recent games in the main series, Pokémon Scarlet and Violet began the ninth and latest generation when they released worldwide for the Nintendo Switch on November 18, 2022.[34][35][36]\\n\\nKanto region\\n\\nJohto region\\n\\nKanto region \\n\\nHoenn region\\n\\nKanto region\\n\\nSinnoh region\\n\\nJohto region\\n\\nKanto region\\n\\nUnova region\\n\\nKalos region\\n\\nHoenn region\\n\\nAlola region\\n\\nKanto region\\n\\nGalar region\\n\\nSinnoh/Hisui region\\n\\nPaldea region\\n\\nPokémon, also known as Pokémon the Series to Western audiences since the year 2013, is an anime television series based on the Pokémon video game series. It was originally broadcast on TV Tokyo in 1997. More than 1,200 episodes of the anime has been produced and aired,[42] divided into 8 series in Japan and 26 seasons internationally. It is one of the longest currently running anime series.[42]\\n\\nThe anime follows the quest of the main character, Ash Ketchum, a Pokémon Trainer, as he and a small group of friends travel around the world of Pokémon along with their Pokémon partners.[43]\\n\\nVarious children\\'s books, collectively known as Pokémon Junior, are also based on the anime.[44]\\n\\nAn eight-part anime series called Pokémon: Twilight Wings aired on YouTube in 2020.[45] The series was animated by Studio Colorido.[46]\\n\\nAn eight part anime series in celebration of the Pokémon 25th anniversary called Pokémon Evolutions aired on YouTube in 2021.[47]\\n\\nIn July 2021, it was announced that a live action Pokémon series is in early development at Netflix with Joe Henderson attached to write and executive produce.[48]\\n\\nThere have been 23 animated theatrical Pokémon films, which have been directed by Kunihiko Yuyama and Tetsuo Yajima, and distributed in Japan by Toho since 1998. The pair of films, Pokémon the Movie: Black—Victini and Reshiram and White—Victini and Zekrom are considered together as one film. Collectibles, such as promotional trading cards, have been available with some of the films. Since the 20th film, the films have been set in an alternate continuity separate from the anime series.\\n\\nPokémon CDs have been released in North America, some of them in conjunction with the theatrical releases of the first three and the 20th Pokémon films. These releases were commonplace until late 2001. On March 27, 2007, a tenth anniversary CD was released containing 18 tracks from the English dub; this was the first English-language release in over five years. Soundtracks of the Pokémon feature films have been released in Japan each year in conjunction with the theatrical releases. In 2017, a soundtrack album featuring music from the North American versions of the 17th through 20th movies was released.\\n\\n^  The exact date of release is unknown.\\n\\n^  Featuring music from Pokémon the Movie: Diancie and the Cocoon of Destruction, Pokémon the Movie: Hoopa and the Clash of Ages, Pokémon the Movie: Volcanion and the Mechanical Marvel, and Pokémon the Movie: I Choose You!\\n\\n The Pokémon Trading Card Game (TCG) is a collectible card game with a goal similar to a Pokémon battle in the video game series. Players use Pokémon cards, with individual strengths and weaknesses, in an attempt to defeat their opponent by \"knocking out\" their Pokémon cards.[51] The game was published in North America by Wizards of the Coast in 1999.[52] With the release of the Game Boy Advance video games Pokémon Ruby and Sapphire, the Pokémon Company took back the card game from Wizards of the Coast and started publishing the cards themselves.[52] The Expedition expansion introduced the Pokémon-e Trading Card Game, where the cards (for the most part) were compatible with the Nintendo e-Reader. Nintendo discontinued its production of e-Reader compatible cards with the release of FireRed and LeafGreen. In 1998, Nintendo released a Game Boy Color version of the trading card game in Japan; Pokémon Trading Card Game was subsequently released to the US and Europe in 2000. The game included digital versions of cards from the original set of cards and the first two expansions (Jungle and Fossil), as well as several cards exclusive to the game. A sequel was released in Japan in 2001.[53]\\n\\nThere are various Pokémon manga series, four of which were released in English by Viz Media, and seven of them released in English by Chuang Yi. The manga series vary from game-based series to being based on the anime and the Trading Card Game. Original stories have also been published. As there are several series created by different authors, most Pokémon manga series differ greatly from each other and other media, such as the anime.[example  needed] Pokémon Pocket Monsters and Pokémon Adventures are the two manga in production since the first generation.\\n\\nIn July 2021, it was announced that a live-action Pokémon series is reportedly in development at Netflix. Joe Henderson, showrunner of Lucifer, is signed on as writer and executive producer.[62]\\n\\nPokémon has been criticized by some fundamentalist Christians over perceived occult and violent themes and the concept of \"Pokémon evolution\", which they feel goes against the Biblical creation account in Genesis.[63] Sat2000, a satellite television station based in Vatican City, has countered that the Pokémon Trading Card Game and video games are \"full of inventive imagination\" and have no \"harmful moral side effects\".[64][65] In the United Kingdom, the \"Christian Power Cards\" game was introduced in 1999 by David Tate who stated, \"Some people aren\\'t happy with Pokémon and want an alternative, others just want Christian games.\" The game was similar to the Pokémon Trading Card Game but used Biblical figures.[66]\\n\\nIn 1999, Nintendo stopped manufacturing the Japanese version of the \"Koga\\'s Ninja Trick\" trading card because it depicted a manji, a traditionally Buddhist symbol with no negative connotations. The Jewish civil rights group Anti-Defamation League complained because the symbol is the reverse of a swastika, a Nazi symbol. The cards were intended for sale in Japan only, but the popularity of Pokémon led to import into the United States with approval from Nintendo. The Anti-Defamation League understood that the portrayed symbol was not intended to offend and acknowledged the sensitivity that Nintendo showed by removing the product.[67][68]\\n\\nIn 1999, two nine-year-old boys from Merrick, New York, sued Nintendo because they claimed the Pokémon Trading Card Game caused their problematic gambling.[69]\\n\\nIn 2001, Saudi Arabia banned Pokémon games and the trading cards, alleging that the franchise promoted Zionism by displaying the Star of David in the trading cards (the Colorless energy from the Pokémon Trading Card Game resembles a six-pointed star) as well as other religious symbols such as crosses they associated with Christianity and triangles they associated with Freemasonry; the games also involved gambling, which is in violation of Muslim doctrine.[70][71]\\n\\nPokémon has also been accused of promoting materialism.[72]\\n\\nIn 2012, PETA criticized the concept of Pokémon as supporting cruelty to animals. PETA compared the game\\'s concept, of capturing animals and forcing them to fight, to cockfights, dog fighting rings and circuses, events frequently criticized for cruelty to animals. PETA released a game spoofing Pokémon where the Pokémon battle their trainers to win their freedom.[73] PETA reaffirmed their objections in 2016 with the release of Pokémon Go, promoting the hashtag #GottaFreeThemAll.[74]\\n\\nOn December 16, 1997, more than 635 Japanese children were admitted to hospitals with epileptic seizures.[75] It was determined the seizures were caused by watching an episode of Pokémon \"Dennō Senshi Porygon\", (most commonly translated \"Electric Soldier Porygon\", season 1, episode 38); as a result, this episode has not been aired since. In this particular episode, there were bright explosions with rapidly alternating blue and red color patterns.[76] It was determined in subsequent research that these strobing light effects cause some individuals to have epileptic seizures, even if the person had no previous history of epilepsy.[77] This incident is a common focus of Pokémon-related parodies in other media, and was lampooned by The Simpsons episode \"Thirty Minutes over Tokyo\" in a short cameo[78] and the South Park episode \"Chinpokomon\",[79] among others.\\n\\nWithin its first two days of release, Pokémon Go raised safety concerns among players. Multiple people also suffered minor injuries from falling while playing the game due to being distracted.[80]\\n\\nMultiple police departments in various countries have issued warnings, some tongue-in-cheek, regarding inattentive driving, trespassing, and being targeted by criminals due to being unaware of one\\'s surroundings.[81][82] People have suffered various injuries from accidents related to the game,[83][84][85][86] and Bosnian players have been warned to stay out of minefields left over from the 1990s Bosnian War.[87] On July 20, 2016, it was reported that an 18-year-old boy in Chiquimula, Guatemala, was shot and killed while playing the game in the late evening hours. This was the first reported death in connection with the app. The boy\\'s 17-year-old cousin, who was accompanying the victim, was shot in the foot. Police speculated that the shooters used the game\\'s GPS capability to find the two.[88]\\n\\nPokémon, being a globally popular franchise, has left a significant mark on today\\'s popular culture. The various species of Pokémon have become pop culture icons; examples include two different Pikachu balloons in the Macy\\'s Thanksgiving Day Parade, Pokémon-themed airplanes operated by All Nippon Airways, merchandise items, and a traveling theme park that was in Nagoya, Japan in 2005 and in Taipei in 2006. Pokémon also appeared on the cover of the U.S. magazine Time in 1999.[89] The Comedy Central show Drawn Together has a character named Ling-Ling who is a parody of Pikachu.[90] Several other shows such as The Simpsons[91] South Park[92] and Robot Chicken[93] and The Flash have made references and spoofs of Pokémon, among other series. Pokémon was featured on VH1\\'s I Love the \\'90s: Part Deux. A live action show based on the anime called Pokémon Live! toured the United States in late 2000.[94] Jim Butcher cites Pokémon as one of the inspirations for the Codex Alera series of novels.[95]\\n\\nPokémon has even made its mark in the realm of science. This includes animals named after Pokémon, such as Stentorceps weedlei (named after the Pokémon Weedle for its resemblance) and Chilicola charizard (named after the Pokémon Charizard) as well as Binburrum articuno, Binburrum zapdos, and Binburrum moltres (named after the Pokémon Articuno, Zapdos, and Moltres, respectively).[96][97] There is also a protein named after Pikachu, called Pikachurin.\\n\\nIn November 2001, Nintendo opened a store called the Pokémon Center in New York, in Rockefeller Center,[98] modeled after the two other Pokémon Center stores in Tokyo and Osaka and named after a staple of the video game series. Pokémon Centers are fictional buildings where Trainers take their injured Pokémon to be healed after combat.[99] The store sold Pokémon merchandise on a total of two floors, with items ranging from collectible shirts to stuffed Pokémon plushies.[100] The store also featured a Pokémon Distributing Machine in which players would place their game to receive an egg of a Pokémon that was being given out at that time. The store also had tables that were open for players of the Pokémon Trading Card Game to duel each other or an employee. The store was closed and replaced by the Nintendo World Store on May 14, 2005.[101] Four Pokémon Center kiosks were put in malls in the Seattle area.[102] The Pokémon Center online store was relaunched on August 6, 2014.[103]\\n\\nProfessor of education Joseph Tobin theorizes that the success of the franchise was due to the long list of names that could be learned by children and repeated in their peer groups. Its rich fictional universe provides opportunities for discussion and demonstration of knowledge in front of their peers. The names of the creatures were linked to its characteristics, which converged with the children\\'s belief that names have symbolic power. Children can pick their favourite Pokémon and affirm their individuality while at the same time affirming their conformance to the values of the group, and they can distinguish themselves from others by asserting what they liked and what they did not like from every chapter. Pokémon gained popularity because it provides a sense of identity to a wide variety of children, and lost it quickly when many of those children found that the identity groups were too big and searched for identities that would distinguish them into smaller groups.[104][page\\xa0needed]\\n\\nPokémon\\'s history has been marked at times by rivalry with the Digimon media franchise that debuted at a similar time. Described as \"the other \\'mon\\'\" by IGN\\'s Juan Castro, Digimon has not enjoyed Pokémon\\'s level of international popularity or success, but has maintained a dedicated fanbase.[105] IGN\\'s Lucas M. Thomas stated that Pokémon is Digimon\\'s \"constant competition and comparison\", attributing the former\\'s relative success to the simplicity of its evolution mechanic as opposed to Digivolution.[106] The two have been noted for conceptual and stylistic similarities by sources such as GameZone.[107] A debate among fans exists over which of the two franchises came first.[108] In actuality, the first Pokémon media, Pokémon Red and Green, were released initially on February 27, 1996;[109] whereas the Digimon virtual pet was released on June 26, 1997.\\n\\nWhile Pokémon\\'s target demographic is children, early purchasers of Pokémon Omega Ruby and Alpha Sapphire were in their 20s.[110] Many fans are adults who originally played the games as children and had later returned to the series.[110]\\n\\nNumerous fan sites exist for the Pokémon franchise, including Bulbagarden, a site hosting the wiki-based encyclopedia Bulbapedia,[111][112][113] and Serebii,[114] a news and reference website.[115] Large fan communities exist on other platforms, such as the subreddit r/pokemon, which has over 4 million subscribers.[116]\\n\\nA significant community around the Pokémon video games\\' metagame has existed for a long time, analyzing the best ways to use each Pokémon to their full potential in competitive battles. The most prolific competitive community is Smogon University, which has created a widely accepted tier-based battle system.[117]\\nSmogon is affiliated with an online Pokémon game called Pokémon Showdown, in which players create a team and battle against other players around the world using the competitive tiers created by Smogon.[118]\\n\\nIn early 2014, an anonymous video streamer on Twitch launched Twitch Plays Pokémon, a small experiment trying to crowdsource playing subsequent Pokémon games, that started with the game Pokémon Red and has since included subsequent games in the series.[119][120]\\n\\nA study at Stanford Neurosciences published in Nature performed magnetic resonance imaging scans of 11 Pokémon experts and 11 controls, finding that seeing Pokémon stimulated activity in the visual cortex, in a different place than is triggered by recognizing faces, places, or words, demonstrating the brain\\'s ability to create such specialized areas.[121][122]\\n\\nPokémon[a][1][2][3] (an abbreviation for Pocket Monsters[b] in Japanese) is a Japanese media franchise managed by The Pokémon Company, founded by Nintendo, Game Freak, and Creatures. The franchise was created by Satoshi Tajiri in 1996,[4] and is centered around fictional creatures called \"Pokémon\". In Pokémon, Pokémon Trainers are people who catch, train, care for, and battle with Pokémon. The English slogan for the franchise is \"Gotta Catch \\'Em All!\".[5][6] There are currently 1015 Pokémon species.[7]\\nThe franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996. Pokémon soon became a media mix franchise adapted into various different media.[8] Pokémon is one of the highest-grossing media franchises of all time. The Pokémon video game series is the third best-selling video game franchise of all time with more than 480 million copies sold[9] and one\\xa0billion mobile downloads.[10] The Pokémon video game series spawned an anime television series that has become the most successful video game adaptation of all time[11] with over 20 seasons and 1,000 episodes in 192 countries.[9] The Pokémon Trading Card Game is the highest-selling trading card game of all time[12] with over 52.9\\xa0billion cards sold. In addition, the Pokémon franchise includes the world\\'s top-selling toy brand,[13] an anime film series, a live-action film (Detective Pikachu), books, manga comics, music, merchandise, and a temporary theme park. The franchise is also represented in other Nintendo media, such as the Super Smash Bros. series, where various Pokémon characters appear as both playable and non-playable characters.\\nThe Pokémon franchise began as Pocket Monsters: Red and Green (later released outside of Japan as Pokémon Red and Blue), a pair of video games for the original Game Boy handheld system that were developed by Game Freak and published by Nintendo in February 1996.[14] Pokémon soon became a media mix franchise adapted into various different media, with the Pokémon Trading Card Game released in October 1996, the Pokémon Adventures manga first released in Japan in March 1997, and the Pocket Monsters: Original Series released in April 1997. Pocket Pikachu was released in Japan in March 1998, with the first ever Pokémon film, Pokémon: The First Movie, first released in Japan in July 1998.[15]\\nIn 1998, Nintendo spent $25 million promoting Pokémon in the United States in partnership with Hasbro, KFC, and others.[16] Nintendo initially feared that Pokémon was too Japanese for Western tastes but Alfred Kahn, then CEO of 4Kids Entertainment, convinced the company otherwise.[17] The one who spotted Pokémon\\'s potential in the United States was Kahn\\'s colleague Thomas Kenney.[18]\\nIn November 2005, 4Kids Entertainment, which had managed the non-game related licensing of Pokémon, announced that it had agreed not to renew the Pokémon representation agreement. The Pokémon Company International oversees all Pokémon licensing outside Asia.[19] In 2006, the franchise celebrated its tenth anniversary with the release of Pokémon Diamond and Pearl.[20] In 2016, the Pokémon Company celebrated Pokémon\\'s 20th anniversary by airing an ad during Super Bowl 50 in January and re-releasing the first Pokémon video games 1996 Game Boy games Pokémon Red, Green (only in Japan), and Blue, and the 1998 Game Boy Color game Pokémon Yellow for the Nintendo 3DS on February 26, 2016.[21][22] The mobile augmented reality game Pokémon Go was released in July 2016.[23] Pokémon Sun and Moon also released in the same year. The first live-action film in the franchise, Pokémon Detective Pikachu, based on the 2018 Nintendo 3DS spin-off game Detective Pikachu, was released in 2019.[24] The eighth generation of core series games began with Pokémon Sword and Shield, released worldwide on the Nintendo Switch on November 15, 2019.\\nTo celebrate its 25th anniversary, the company released two additional titles for the Nintendo Switch: Pokémon Brilliant Diamond and Shining Pearl, remakes of the Nintendo DS Pokémon Diamond and Pearl games, on November 19, 2021, and its \"premake\" Pokémon Legends: Arceus, which was subsequently released on January 28, 2022.[25][26]\\nPokémon Scarlet and Violet began the ninth generation of the game series when they released worldwide for the Nintendo Switch on November 18, 2022.[27]\\nThe name \"Pokémon\" is a syllabic abbreviation of the Japanese brand Pocket Monsters.[28] The term \"Pokémon\", in addition to referring to the Pokémon franchise itself, also collectively refers to the many fictional species that have made appearances in Pokémon media. \"Pokémon\" is identical in the singular and plural, as is each individual species name; it is and would be grammatically correct to say \"one Pokémon\" and \"many Pokémon\", as well as \"one Pikachu\" and \"many Pikachu\".[29]\\nPokémon executive director Satoshi Tajiri first thought of Pokémon, albeit with a different concept and name, around 1989, when the Game Boy was released. The concept of the Pokémon universe, in both the video games and the general fictional world of Pokémon, stems from the hobby of insect collecting, a popular pastime which Tajiri enjoyed as a child.[30] Players are designated as Pokémon Trainers and have three general goals: to complete the regional Pokédex by collecting all of the available Pokémon species found in the fictional region where a game takes place, to complete the national Pokédex by transferring Pokémon from other regions, and to train a team of powerful Pokémon from those they have caught to compete against teams owned by other Trainers so they may eventually win the Pokémon League and become the regional Champion. These themes of collecting, training, and battling are present in almost every version of the Pokémon franchise, including the video games, the anime and manga series, and the Pokémon Trading Card Game (also known as TCG).\\nIn most incarnations of the Pokémon universe, a Trainer who encounters a wild Pokémon has the ability to capture that Pokémon by throwing a specially designed, mass-producible spherical tool called a Poké Ball at it. If the Pokémon is unable to escape the confines of the Poké Ball, it is considered to be under the ownership of that Trainer. Afterwards, it will obey whatever commands it receives from its new Trainer, unless the Trainer demonstrates such a lack of experience that the Pokémon would rather act on its own accord. Trainers can send out any of their Pokémon to wage non-lethal battles against other Pokémon; if the opposing Pokémon is wild, the Trainer can capture that Pokémon with a Poké Ball, increasing their collection of creatures. In Pokémon Go, and in Pokémon: Let\\'s Go, Pikachu! and Let\\'s Go, Eevee!, wild Pokémon encountered by players can be caught in Poké Balls, but most cannot be battled. Pokémon already owned by other Trainers cannot be captured, except under special circumstances in certain side games. If a Pokémon fully defeats an opponent in battle so that the opponent is knocked out (\"faints\"), the winning Pokémon gains experience points and may level up. Beginning with Pokémon X and Y, experience points are also gained from catching Pokémon in Poké Balls. When leveling up, the Pokémon\\'s battling aptitude statistics (\"stats\", such as \"Attack\" and \"Speed\") increase. At certain levels, the Pokémon may also learn new moves, which are techniques used in battle. In addition, many species of Pokémon can undergo a form of metamorphosis and transform into a similar but stronger species of Pokémon, a process called evolution; this process occurs spontaneously under differing circumstances, and is itself a central theme of the series. Some species of Pokémon may undergo a maximum of two evolutionary transformations, while others may undergo only one, and others may not evolve at all. For example, the Pokémon Pichu may evolve into Pikachu, which in turn may evolve into Raichu, following which no further evolutions may occur. Pokémon X and Y introduced the concept of \"Mega Evolution,\" by which certain fully evolved Pokémon may temporarily undergo an additional evolution into a stronger form for the purpose of battling; this evolution is considered a special case, and unlike other evolutionary stages, is reversible.\\nIn the main series, each game\\'s single-player mode requires the Trainer to raise a team of Pokémon to defeat many non-player character (NPC) Trainers and their Pokémon. Each game lays out a somewhat linear path through a specific region of the Pokémon world for the Trainer to journey through, completing events and battling opponents along the way (including foiling the plans of an evil team of Pokémon Trainers who serve as antagonists to the player). Excluding Pokémon Sun and Moon and Pokémon Ultra Sun and Ultra Moon, the games feature eight powerful Trainers, referred to as Gym Leaders, that the Trainer must defeat in order to progress. As a reward, the Trainer receives a Gym Badge, and once all eight badges are collected, the Trainer is eligible to challenge the region\\'s Pokémon League, where four talented trainers (referred to collectively as the \"Elite Four\") challenge the Trainer to four Pokémon battles in succession. If the trainer can overcome this gauntlet, they must challenge the Regional Champion, the master Trainer who had previously defeated the Elite Four. Any Trainer who wins this last battle becomes the new champion.\\nPokémon is set in the fictional Pokémon universe. There are numerous regions that have appeared in the various media of the Pokémon franchise. There are 9 main series regions set in the main series games: Kanto, Johto, Hoenn, Sinnoh/Hisui, Unova, Kalos, Alola, Galar, and Paldea. Each of the nine generations of the main series releases focuses on a new region. Every region consists of several cities and towns that the player must explore in order to overcome many waiting challenges, such as Gyms, Contests and villainous teams. At different locations within each region, the player can find different types of Pokémon, as well as helpful items and characters. Different regions are not accessible from one another at all within a single game, only with the exception of Kanto and Johto being linked together in Pokémon Gold, Silver, Crystal, HeartGold and SoulSilver versions. There are also regions set in spinoff games and two islands in the Pokémon anime (Orange Islands and Decolore Islands), all still set within the same fictional universe.\\nEach main series region in the Pokémon universe is based on a real world location. The first four regions introduced are based on locations in Japan, being Kantō, Kansai, Kyushu, and Hokkaidō, with later regions being based on parts of New York City, France, Hawaii, the United Kingdom, and the Iberian Peninsula.[31][32]\\nAll of the licensed Pokémon properties overseen by the Pokémon Company International are divided roughly by generation. These generations are roughly chronological divisions by release; every several years, when a sequel to the 1996 role-playing video games Pokémon Red and Green is released that features new Pokémon, characters, and gameplay concepts, that sequel is considered the start of a new generation of the franchise. The main Pokémon video games and their spin-offs, the anime, manga, and trading card game are all updated with the new Pokémon properties each time a new generation begins.[33] Some Pokémon from the newer games appear in anime episodes or films months, or even years, before the game they were programmed for came out. The first generation began in Japan with Pokémon Red and Green on the Game Boy. As of 2022, there are nine generations of main series video games. The most recent games in the main series, Pokémon Scarlet and Violet began the ninth and latest generation when they released worldwide for the Nintendo Switch on November 18, 2022.[34][35][36]\\nPokémon, also known as Pokémon the Series to Western audiences since the year 2013, is an anime television series based on the Pokémon video game series. It was originally broadcast on TV Tokyo in 1997. More than 1,200 episodes of the anime has been produced and aired,[42] divided into 8 series in Japan and 26 seasons internationally. It is one of the longest currently running anime series.[42]\\nThe anime follows the quest of the main character, Ash Ketchum, a Pokémon Trainer, as he and a small group of friends travel around the world of Pokémon along with their Pokémon partners.[43]\\nAn eight-part anime series called Pokémon: Twilight Wings aired on YouTube in 2020.[45] The series was animated by Studio Colorido.[46]\\nAn eight part anime series in celebration of the Pokémon 25th anniversary called Pokémon Evolutions aired on YouTube in 2021.[47]\\nIn July 2021, it was announced that a live action Pokémon series is in early development at Netflix with Joe Henderson attached to write and executive produce.[48]\\nThere have been 23 animated theatrical Pokémon films, which have been directed by Kunihiko Yuyama and Tetsuo Yajima, and distributed in Japan by Toho since 1998. The pair of films, Pokémon the Movie: Black—Victini and Reshiram and White—Victini and Zekrom are considered together as one film. Collectibles, such as promotional trading cards, have been available with some of the films. Since the 20th film, the films have been set in an alternate continuity separate from the anime series.\\nPokémon CDs have been released in North America, some of them in conjunction with the theatrical releases of the first three and the 20th Pokémon films. These releases were commonplace until late 2001. On March 27, 2007, a tenth anniversary CD was released containing 18 tracks from the English dub; this was the first English-language release in over five years. Soundtracks of the Pokémon feature films have been released in Japan each year in conjunction with the theatrical releases. In 2017, a soundtrack album featuring music from the North American versions of the 17th through 20th movies was released.\\n^  Featuring music from Pokémon the Movie: Diancie and the Cocoon of Destruction, Pokémon the Movie: Hoopa and the Clash of Ages, Pokémon the Movie: Volcanion and the Mechanical Marvel, and Pokémon the Movie: I Choose You!\\n The Pokémon Trading Card Game (TCG) is a collectible card game with a goal similar to a Pokémon battle in the video game series. Players use Pokémon cards, with individual strengths and weaknesses, in an attempt to defeat their opponent by \"knocking out\" their Pokémon cards.[51] The game was published in North America by Wizards of the Coast in 1999.[52] With the release of the Game Boy Advance video games Pokémon Ruby and Sapphire, the Pokémon Company took back the card game from Wizards of the Coast and started publishing the cards themselves.[52] The Expedition expansion introduced the Pokémon-e Trading Card Game, where the cards (for the most part) were compatible with the Nintendo e-Reader. Nintendo discontinued its production of e-Reader compatible cards with the release of FireRed and LeafGreen. In 1998, Nintendo released a Game Boy Color version of the trading card game in Japan; Pokémon Trading Card Game was subsequently released to the US and Europe in 2000. The game included digital versions of cards from the original set of cards and the first two expansions (Jungle and Fossil), as well as several cards exclusive to the game. A sequel was released in Japan in 2001.[53]\\nThere are various Pokémon manga series, four of which were released in English by Viz Media, and seven of them released in English by Chuang Yi. The manga series vary from game-based series to being based on the anime and the Trading Card Game. Original stories have also been published. As there are several series created by different authors, most Pokémon manga series differ greatly from each other and other media, such as the anime.[example  needed] Pokémon Pocket Monsters and Pokémon Adventures are the two manga in production since the first generation.\\nIn July 2021, it was announced that a live-action Pokémon series is reportedly in development at Netflix. Joe Henderson, showrunner of Lucifer, is signed on as writer and executive producer.[62]\\nPokémon has been criticized by some fundamentalist Christians over perceived occult and violent themes and the concept of \"Pokémon evolution\", which they feel goes against the Biblical creation account in Genesis.[63] Sat2000, a satellite television station based in Vatican City, has countered that the Pokémon Trading Card Game and video games are \"full of inventive imagination\" and have no \"harmful moral side effects\".[64][65] In the United Kingdom, the \"Christian Power Cards\" game was introduced in 1999 by David Tate who stated, \"Some people aren\\'t happy with Pokémon and want an alternative, others just want Christian games.\" The game was similar to the Pokémon Trading Card Game but used Biblical figures.[66]\\nIn 1999, Nintendo stopped manufacturing the Japanese version of the \"Koga\\'s Ninja Trick\" trading card because it depicted a manji, a traditionally Buddhist symbol with no negative connotations. The Jewish civil rights group Anti-Defamation League complained because the symbol is the reverse of a swastika, a Nazi symbol. The cards were intended for sale in Japan only, but the popularity of Pokémon led to import into the United States with approval from Nintendo. The Anti-Defamation League understood that the portrayed symbol was not intended to offend and acknowledged the sensitivity that Nintendo showed by removing the product.[67][68]\\nIn 1999, two nine-year-old boys from Merrick, New York, sued Nintendo because they claimed the Pokémon Trading Card Game caused their problematic gambling.[69]\\nIn 2001, Saudi Arabia banned Pokémon games and the trading cards, alleging that the franchise promoted Zionism by displaying the Star of David in the trading cards (the Colorless energy from the Pokémon Trading Card Game resembles a six-pointed star) as well as other religious symbols such as crosses they associated with Christianity and triangles they associated with Freemasonry; the games also involved gambling, which is in violation of Muslim doctrine.[70][71]\\nIn 2012, PETA criticized the concept of Pokémon as supporting cruelty to animals. PETA compared the game\\'s concept, of capturing animals and forcing them to fight, to cockfights, dog fighting rings and circuses, events frequently criticized for cruelty to animals. PETA released a game spoofing Pokémon where the Pokémon battle their trainers to win their freedom.[73] PETA reaffirmed their objections in 2016 with the release of Pokémon Go, promoting the hashtag #GottaFreeThemAll.[74]\\nOn December 16, 1997, more than 635 Japanese children were admitted to hospitals with epileptic seizures.[75] It was determined the seizures were caused by watching an episode of Pokémon \"Dennō Senshi Porygon\", (most commonly translated \"Electric Soldier Porygon\", season 1, episode 38); as a result, this episode has not been aired since. In this particular episode, there were bright explosions with rapidly alternating blue and red color patterns.[76] It was determined in subsequent research that these strobing light effects cause some individuals to have epileptic seizures, even if the person had no previous history of epilepsy.[77] This incident is a common focus of Pokémon-related parodies in other media, and was lampooned by The Simpsons episode \"Thirty Minutes over Tokyo\" in a short cameo[78] and the South Park episode \"Chinpokomon\",[79] among others.\\nWithin its first two days of release, Pokémon Go raised safety concerns among players. Multiple people also suffered minor injuries from falling while playing the game due to being distracted.[80]\\nMultiple police departments in various countries have issued warnings, some tongue-in-cheek, regarding inattentive driving, trespassing, and being targeted by criminals due to being unaware of one\\'s surroundings.[81][82] People have suffered various injuries from accidents related to the game,[83][84][85][86] and Bosnian players have been warned to stay out of minefields left over from the 1990s Bosnian War.[87] On July 20, 2016, it was reported that an 18-year-old boy in Chiquimula, Guatemala, was shot and killed while playing the game in the late evening hours. This was the first reported death in connection with the app. The boy\\'s 17-year-old cousin, who was accompanying the victim, was shot in the foot. Police speculated that the shooters used the game\\'s GPS capability to find the two.[88]\\nPokémon, being a globally popular franchise, has left a significant mark on today\\'s popular culture. The various species of Pokémon have become pop culture icons; examples include two different Pikachu balloons in the Macy\\'s Thanksgiving Day Parade, Pokémon-themed airplanes operated by All Nippon Airways, merchandise items, and a traveling theme park that was in Nagoya, Japan in 2005 and in Taipei in 2006. Pokémon also appeared on the cover of the U.S. magazine Time in 1999.[89] The Comedy Central show Drawn Together has a character named Ling-Ling who is a parody of Pikachu.[90] Several other shows such as The Simpsons[91] South Park[92] and Robot Chicken[93] and The Flash have made references and spoofs of Pokémon, among other series. Pokémon was featured on VH1\\'s I Love the \\'90s: Part Deux. A live action show based on the anime called Pokémon Live! toured the United States in late 2000.[94] Jim Butcher cites Pokémon as one of the inspirations for the Codex Alera series of novels.[95]\\nPokémon has even made its mark in the realm of science. This includes animals named after Pokémon, such as Stentorceps weedlei (named after the Pokémon Weedle for its resemblance) and Chilicola charizard (named after the Pokémon Charizard) as well as Binburrum articuno, Binburrum zapdos, and Binburrum moltres (named after the Pokémon Articuno, Zapdos, and Moltres, respectively).[96][97] There is also a protein named after Pikachu, called Pikachurin.\\nIn November 2001, Nintendo opened a store called the Pokémon Center in New York, in Rockefeller Center,[98] modeled after the two other Pokémon Center stores in Tokyo and Osaka and named after a staple of the video game series. Pokémon Centers are fictional buildings where Trainers take their injured Pokémon to be healed after combat.[99] The store sold Pokémon merchandise on a total of two floors, with items ranging from collectible shirts to stuffed Pokémon plushies.[100] The store also featured a Pokémon Distributing Machine in which players would place their game to receive an egg of a Pokémon that was being given out at that time. The store also had tables that were open for players of the Pokémon Trading Card Game to duel each other or an employee. The store was closed and replaced by the Nintendo World Store on May 14, 2005.[101] Four Pokémon Center kiosks were put in malls in the Seattle area.[102] The Pokémon Center online store was relaunched on August 6, 2014.[103]\\nProfessor of education Joseph Tobin theorizes that the success of the franchise was due to the long list of names that could be learned by children and repeated in their peer groups. Its rich fictional universe provides opportunities for discussion and demonstration of knowledge in front of their peers. The names of the creatures were linked to its characteristics, which converged with the children\\'s belief that names have symbolic power. Children can pick their favourite Pokémon and affirm their individuality while at the same time affirming their conformance to the values of the group, and they can distinguish themselves from others by asserting what they liked and what they did not like from every chapter. Pokémon gained popularity because it provides a sense of identity to a wide variety of children, and lost it quickly when many of those children found that the identity groups were too big and searched for identities that would distinguish them into smaller groups.[104][page\\xa0needed]\\nPokémon\\'s history has been marked at times by rivalry with the Digimon media franchise that debuted at a similar time. Described as \"the other \\'mon\\'\" by IGN\\'s Juan Castro, Digimon has not enjoyed Pokémon\\'s level of international popularity or success, but has maintained a dedicated fanbase.[105] IGN\\'s Lucas M. Thomas stated that Pokémon is Digimon\\'s \"constant competition and comparison\", attributing the former\\'s relative success to the simplicity of its evolution mechanic as opposed to Digivolution.[106] The two have been noted for conceptual and stylistic similarities by sources such as GameZone.[107] A debate among fans exists over which of the two franchises came first.[108] In actuality, the first Pokémon media, Pokémon Red and Green, were released initially on February 27, 1996;[109] whereas the Digimon virtual pet was released on June 26, 1997.\\nWhile Pokémon\\'s target demographic is children, early purchasers of Pokémon Omega Ruby and Alpha Sapphire were in their 20s.[110] Many fans are adults who originally played the games as children and had later returned to the series.[110]\\nNumerous fan sites exist for the Pokémon franchise, including Bulbagarden, a site hosting the wiki-based encyclopedia Bulbapedia,[111][112][113] and Serebii,[114] a news and reference website.[115] Large fan communities exist on other platforms, such as the subreddit r/pokemon, which has over 4 million subscribers.[116]\\nA significant community around the Pokémon video games\\' metagame has existed for a long time, analyzing the best ways to use each Pokémon to their full potential in competitive battles. The most prolific competitive community is Smogon University, which has created a widely accepted tier-based battle system.[117]\\nSmogon is affiliated with an online Pokémon game called Pokémon Showdown, in which players create a team and battle against other players around the world using the competitive tiers created by Smogon.[118]\\nIn early 2014, an anonymous video streamer on Twitch launched Twitch Plays Pokémon, a small experiment trying to crowdsource playing subsequent Pokémon games, that started with the game Pokémon Red and has since included subsequent games in the series.[119][120]\\nA study at Stanford Neurosciences published in Nature performed magnetic resonance imaging scans of 11 Pokémon experts and 11 controls, finding that seeing Pokémon stimulated activity in the visual cortex, in a different place than is triggered by recognizing faces, places, or words, demonstrating the brain\\'s ability to create such specialized areas.[121][122]\\n',\n",
       " '[from pdf0]\\nVisionLLM: Large Language Model is also\\nan Open-Ended Decoder for Vision-Centric Tasks\\nWenhai Wang∗1, Zhe Chen∗2,1, Xiaokang Chen∗3,1, Jiannan Wu∗4,1, Xizhou Zhu5,1\\nGang Zeng3, Ping Luo4,1, Tong Lu2, Jie Zhou6, Yu Qiao1, Jifeng Dai†6,1\\n1OpenGVLab, Shanghai AI Laboratory2Nanjing University3Peking University\\n4The University of HongKong5SenseTime Research6Tsinghua University\\nCode: https://github.com/OpenGVLab/VisionLLM\\nDemo: https://github.com/OpenGVLab/InternGPT\\nAbstract\\nLarge language models (LLMs) have notably accelerated progress towards artificial\\ngeneral intelligence (AGI), with their impressive zero-shot capacity for user-tailored\\ntasks, endowing them with immense potential across a range of applications.\\nHowever, in the field of computer vision, despite the availability of numerous\\npowerful vision foundation models (VFMs), they are still restricted to tasks in a\\npre-defined form, struggling to match the open-ended task capabilities of LLMs.\\nIn this work, we present an LLM-based framework for vision-centric tasks, termed\\nVisionLLM. This framework provides a unified perspective for vision and language\\ntasks by treating images as a foreign language and aligning vision-centric tasks\\nwith language tasks that can be flexibly defined and managed using language\\ninstructions. An LLM-based decoder can then make appropriate predictions based\\non these instructions for open-ended tasks. Extensive experiments show that the\\nproposed VisionLLM can achieve different levels of task customization through\\nlanguage instructions, from fine-grained object-level to coarse-grained task-level\\ncustomization, all with good results. It’s noteworthy that, with a generalist LLM-\\nbased framework, our model can achieve over 60% mAP on COCO, on par with\\ndetection-specific models. We hope this model can set a new baseline for generalist\\nvision and language models. The code and demo shall be released.\\n1 Introduction\\nThe emergence of large language models (LLMs) like ChatGPT [ 41] has revolutionized the landscape\\nof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-\\ndressing various natural language processing (NLP) tasks through user-tailored prompts or language\\ninstructions. Despite these advancements, it’s essential to note that the triumph of LLMs does not\\neffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between\\nmodalities and task formats.\\nThe field of computer vision presents a unique set of challenges and paradigms that differ from\\nthose of NLP. The traditional paradigm of vision foundation models is pre-training followed by\\nfine-tuning [ 59,12,51,61,18,52], which is effective but comes with significant marginal costs\\nwhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as\\nmulti-task unification [ 44,58,1,57,81] have been used to achieve generalist capability, they often\\nstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\\n∗Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns at\\nShanghai AI Laboratory.†Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.arXiv:2305.11175v2  [cs.CV]  25 May 2023VisionGeneralistModelPre-defined tasks:detection, captioning, VQA, grounding, ...(a) Vision generalist models [ 59,\\n61,83] are constrained by the for-\\nmat of pre-defined tasks.\\nVisualPromptTuning\\n(b) Visual prompt tuning [ 26,64,\\n62] are inconsistent with the for-\\nmat of LLMs.\\nVision + LLM\\nTask deﬁned by  instruc6onsDesired output:<c1> <p1> <p3> ...(c) VisionLLM (ours) can flexibly\\nmanage vision-centric tasks using\\nlanguage instructions like LLMs .\\nFigure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist\\nmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistent\\nwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision\\ntasks by using language instructions.\\ntask capabilities compared to LLMs. Recently, visual prompt tuning [ 26,74,79,76,62] has emerged\\nas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance\\nsegmentation, and pose estimation, using visual masking. However, the format of visual prompts\\nconsiderably deviates from that of language instructions, making it challenging to directly apply the\\nreasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent\\nneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the\\nspecific requirements of vision-centric tasks.\\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric\\ntasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,\\nVisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,\\nit comprises three core components: (1) a unified language instruction designed for vision and\\nvision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended\\ntask decoder that orchestrates various tasks using language instructions. With this framework, a\\nwide range of vision-centric tasks can be seamlessly integrated, including object detection, instance\\nsegmentation, image captioning, and visual grounding. In addition, the framework also facilitates\\ntask customization at different levels of granularity, allowing for the customization of target objects,\\noutput formats, task descriptions, etc.\\nCompared to current popular API-based applications [ 68,73,50,35,30], our model takes a unified,\\nend-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency\\nof the overall process, and leveraging the strengths and data of both VFMs and LLMs within a\\nsingle, cohesive system. Furthermore, our model surpasses the limitations of generalist vision models\\npre-trained on pre-defined tasks. VisionLLM can effectively manage vision-centric tasks through\\nlanguage instructions, embodying a flexible and open-ended approach that is not constrained by\\npre-set tasks. This versatility makes VisionLLM a robust and powerful generalist model for vision\\nand vision-language tasks, opening up new possibilities for the development of unified generalist\\nmodels that bridge the domains of vision and language.\\nIn summary, our main contributions are as follows:\\n(1) We propose VisionLLM, the first framework that leverages the power of LLMs to address vision-\\ncentric tasks in an open-ended and customizable manner. By aligning the definitions of vision-centric\\ntasks with LLM methodologies, VisionLLM breaks new ground in enabling the unified modeling of\\nvision and language, opening up possibilities for advancing the field.\\n(2) We overcome many difficulties when porting LLMs to vision-centric tasks, by designing unified\\nlanguage instruction that matches the format of language models and covers various vision-centric\\ntasks including visual perception. Correspondingly, we develop a language-guided image tokenizer\\nand an LLM-based task decoder that can handle open-ended tasks according to the given language\\ninstructions based on the LLMs’ reasoning and parsing capabilities.\\n2Human: “Identify the objects in <image>that belong to {‘What is the child eating?’: <c0>, ‘red gamepad\\': <c1>}and draw a bounding box around eachone. The output should be a list oftuples in  the format (c, x1, y1, x2, y2), where c is the class label and x1, y1, x2, y2 are the coordinates of the top-left and bottom-right corners of the bounding box. The coordinates should be within the range 1024.\"VisionLLM: \"Thebounding boxesare[(<c0>, 226.4, 229.8, 363.1, 347.4), (<c1>, 441.1, 183.5, 538.6, 269.9)].\"<c1><c0>(a) Object Detection with Customized Class Set (e.g.,\\nquestion, reasoning text, open-vocabulary descrip-\\ntion)\\nHuman: \"For each object in <image>that is a member of {\\'motorbike\\': <c0>}, output a tuple with the class label and the coordinates of a polygon with 16 pointsthatencloses the object.  The coordinates should be within the range 1024. The output format should be (c, x1, y1, x2, y2, ..., x16, y16).\"VisionLLM: \"The detectedpolygonsare[(<c0>, 135.3, 95.7, 123.4, 53.4, 84.9, 57.6, 66.8, 60.5, 60.1, 72.3, 34.2, 71.4, ..., 124.9, 119.3)].\"\\n<c0> (b) Object Location with Editable Output Format\\nwhich is placed on top of a wooden table. The dog has its head in the middle of the cooling pad, and a fan is positioned behind it, blowing air onto the pad. The table is outdoors, in a grassy area, and there are two chairs placed around it, one on each side.The scene appears to be set in a sunny weather, with a bright and sunny sky, which contributes to the overall outdoor atmosphere.\"\\nHuman: \"Provide a long descriptionof the <image>.\"VisionLLM:  \"The image features a white doglaying down on a foldable outdoor cooling pad, \\n(c) Image Description with Controllable Text Length\\nHuman: \"With the objects in the <image>, if you feel hot, what will you do?\"VisionLLM: \"First, You can also try turning off the computer and other electrical appliancesin the\\nroom to reduce the heat generated by their operation.As an alternative, you can use the desk fanprovided in the image to create a cooling effect in the room. You can plug in the fan and adjust its settings to ensure it blows air in a direction that reaches the room’s occupants effectively.\" (d) Visual Question Answer with Complex Reasoning\\nFigure 2: Results and visualizations of our VisionLLM . Guided by language instructions, our\\nunified generalist framework showcases its effectiveness on diverse open-ended vision-centric tasks.\\nThe text marked with a gray background indicates the customized instructions and the desired outputs.\\n(3) We construct a series of tasks with different granularities to verify the effectiveness of our\\nmodels, ranging from easy to hard, and from pre-defined to flexible. Through these validations, we\\ndemonstrate the remarkable generality of our models, showcasing their ability to handle diverse\\nscenarios, including random object categories, random output formats, and random task descriptions,\\nas shown in Figure 2. The successful outcomes of these validations underscore the tremendous\\npotential of our model in harnessing the capabilities of LLMs to control and guide vision-centric\\ntasks. In addition, with a generalist LLM-based framework, our model also yields promising results\\non various vision-centric tasks. Notably, our generalist model achieves an impressive mAP score of\\n60+% on the COCO dataset, surpassing many detection-specific models [ 82,7,22] and approaching\\nthe state-of-the-art record.\\n2 Related Work\\n2.1 Large Language Model\\nLarge language models (LLMs) have gained significant attention in the field of natural language\\nprocessing (NLP) and artificial general intelligence (AGI), due to their impressive capabilities\\nin language generation, in-context learning, world knowledge, and reasoning. The GPT family,\\nincluding GPT-3 [ 6], ChatGPT [ 41], GPT-4 [ 40], and InstructGPT [ 42] are most representative\\nworks of LLMs. Other LLMs like OPT [ 78], LLaMA [ 54], MOSS [ 15], and GLM [ 77] have\\nalso made substantial contributions to the field. These models achieve high performance and are\\nopen-sourced, serving as valuable resources for training large models and as foundations for further\\nfine-tuning for specific purposes. For instance, Alpaca [ 53] introduces a self-instruct framework\\nthat facilitates instruction tuning of the LLaMA model, reducing the reliance on human-written\\n3instruction data. Recently, the emergence of these LLMs has also opened up API-based applications\\nfor solving vision-centric tasks. These applications have integrated visual APIs with language models\\nto enable decision-making or planning based on visual information, such as Visual ChatGPT [ 68],\\nMM-REACT [ 73], HuggingGPT [ 50], InternGPT [ 35], and VideoChat [ 30]. However, despite the\\nconvenience of using language-based instructions to define tasks and describe visual elements, these\\ninteractive systems [ 68,73,50,35,30] still face limitations in capturing fine-grained visual details\\nand understanding complex visual contexts, which hinder their ability to effectively connecting vision\\nand language models. In summary, while LLMs have shown tremendous potential in various NLP\\napplications, their applicability to vision-centric tasks has been limited by the challenges posed by\\nmodalities and task formats.\\n2.2 Vision Generalist Model\\nThe pursuit of generalist models [ 83,38,70], which aim to handle a wide range of tasks using a\\nshared architecture and parameters, has been a long-standing goal in the machine learning community.\\nInspired by the success of sequence-to-sequence (seq2seq) models in the field of NLP [ 44], recent\\nadvancements such as OFA [ 58], Flamingo [ 1], and GIT [ 57] propose modeling diverse tasks as\\nsequence generation tasks. Unified-IO [ 38], Pix2Seq v2 [ 9], and UniTab [ 71] extend this idea by using\\ndiscrete coordinate tokens to encode and decode spatial information for more tasks. Gato [ 47] also\\nincorporates reinforcement learning tasks into the seq2seq framework, while GPV [ 21] develops a\\ngeneral-purpose vision system by combining a seq2seq module with a DETR-based visual encoder [ 7].\\nHowever, these methods suffer from some limitations, such as slow inference speed and performance\\ndegradation due to the non-parallel auto-regressive decoding process. Uni-Perceivers [ 83,81,28]\\nsolve these issues by unifying different tasks using the maximum likelihood target for each input\\nbased on representation similarity, regardless of their modality, making it possible to support both\\ngeneration and non-generation tasks in a unified framework. Nevertheless, these generalist models\\nare still restricted by pre-defined tasks and cannot support flexible open-ended task customization\\nbased on language instructions like LLMs.\\n2.3 Instruction Tuning\\nLanguage instructions are a powerful way to express various NLP tasks and examples for LLMs,\\nas introduced by GPT-3 [ 6]. Following this idea, subsequent works, such as InstructGPT [ 42],\\nFLAN [ 14,67], and OPT-IML [ 25], explore the instruction-tuning method [ 66,65] and demonstrate\\nthat this simple approach effectively enhances the zero-shot and few-shot capabilities of LLMs.\\nThe language instruction paradigm has also been adopted by the computer vision community to\\ndefine image-to-text tasks. Flamingo [ 1] is a milestone work that uses vision and language inputs as\\nprompts and achieves remarkable few-shot results in various vision-language tasks, such as image\\ncaptioning [ 10] and VQA [ 3]. BLIP-2 [ 29] further connects the visual encoder with LLMs through a\\nquerying transformer and a linear projection layer to build strong multimodal models. MiniGPT-4 [ 80]\\nand LLaV A [ 33] finetune the BLIP-2-style models on synthetic multimodal instruction-following\\ndata to unleash the potential of LLMs. However, these models mainly focus on image-to-text\\ntasks and fail to address visual perception, such as object detection, instance segmentation, pose\\nestimation, etc. To tackle image inpainting tasks, Bar et al. [4] introduces the first visual prompting\\nframework that utilizes inpainting with discrete tokens on images. Painter [ 63] and SegGPT [ 64]\\nemploy masked image modeling on raw pixels for in-context learning with paired images. While\\nthese visual prompt models demonstrate good results in segmentation tasks, their applicability to\\nnumerous real-world vision tasks is challenging. Moreover, defining the visual prompts as image\\ninpainting is inconsistent with the language instructions in LLMs, hard to leverage the reasoning,\\nparsing ability, and world knowledge of LLMs. In this work, we aim to align vision-centric tasks\\nwith language tasks, use language instructions to unifiedly and flexibly define all tasks, and solve\\nthem with a shared LLM-based task decoder.\\n3 VisionLLM\\n3.1 Overall Architecture\\nThis work targets to provide a unified generalist framework that can seamlessly integrate the strengths\\nof large language models (LLMs) with the specific requirements of vision-centric tasks. As shown in\\n4RandomQueryLanguage-GuidedImage Token...<text><text>Desired Output:<c1> <p1> <p3> ...\\nOpen-Ended Task Decoder with LLM\\nLanguage-GuidedImageTokenizer\\nBackbone...\\nVision-languageexample:\"Describetheimage<image>indetails.\"Vision-onlyexample:\"Foreachobjectinimage<image>thatisamemberofclassset<class>,outputatuplewiththeclasslabelandthecoordinatesofapolygonwith16pointsthatenclosestheobject.Thecoordinatesshouldbewithinrange<range>.Theoutputformatshouldbe(c,x1,y1,...).\"LanguageInstructions\\t<text>𝐹!𝐹\"𝑇Figure 3: Overall architecture of the proposed VisionLLM. It consists of three parts: a unified\\nlanguage instruction designed to accommodate both vision and vision-language tasks, an image\\ntokenizer that encodes visual information guided by language instructions, and an LLM-based open-\\nended task decoder that executes diverse tasks defined by language instructions.\\nFigure 3, the overall architecture of VisionLLM consists of three key designs: (1) a unified language\\ninstruction that provides a consistent interface for vision-centric task definition and customization;\\n(2) a language-guided image tokenizer, which encodes visual information in alignment with the given\\nlanguage prompt, enabling the model to comprehend and parse the visual content effectively; and\\n(3) an LLM-based open-task decoder, which utilizes the encoded visual information and language\\ninstructions to generate satisfactory predictions or outputs. The three designs work together to achieve\\na flexible and open-ended framework that can handle various vision-centric tasks at different levels of\\ntask customization through language instructions.\\nDifferent from previous interactive systems [ 68,73,50,35,30] that rely on APIs, our VisionLLM\\npresents a more flexible and end-to-end pipeline. Given language instructions that describe the current\\ntasks and an input image, the model first uses a language-guided image tokenizer to encode the image\\ntokens based on the given prompt. Then, the image tokens and language instructions are fed to an\\nLLM-based open-ended task decoder. Finally, it evaluates the generated outputs against the task\\ndefinition given by the unified language instructions, enabling the model to produce task-specific\\nresults. This seamless, end-to-end pipeline enables VisionLLM to effectively combine vision and\\nlanguage, achieving remarkable performance in open-ended and customizable vision-centric tasks.\\n3.2 Unified Language Instruction\\nWe first introduce unified language instructions to describe vision-centric tasks. This design enables\\nthe unification of various vision-only and vision-language task descriptions and allows for flexible\\ntask customization.\\nVision-Language Tasks. The instructions for vision-language tasks such as image captioning and\\nvisual question answering (VQA) are straightforward and similar to NLP tasks. Following previous\\nmethods [ 29,83,33], we describe the image captioning task like “ The image is <image> .Please\\ngenerate a caption for the image: ”, and the VQA task like “ The image is <image> .Please generate\\nan answer for the image according to the question: <question> ”. Here, <image> and<question>\\nare the placeholdersok of the image tokens and the question, respectively.\\nVision-Only Tasks. Designing effective language instructions for vision tasks is a challenging\\nendeavor due to the differences in modality and task format between vision and language. Here, we\\ndescribe vision tasks by providing a task description and specifying the desired output format via\\nlanguage instructions.\\n(1) The task description conveys the intended task to the language model. Following self-instruct [ 65],\\nwe design a set of seed instructions with placeholders and employ LLMs to generate a large number\\nof related task descriptions and randomly select one of them during training.\\n5(2) For conventional visual perception tasks like object detection and instance segmentation, we\\npropose a unified output format represented as a tuple (C, P), where Cdenotes the class index\\nin the category set <class> , and P={xi, yi}N\\ni=1represents Npoints that locate the object. To\\nalign with the format of word tokens, both the class index Cand the coordinates of points xi, yiare\\ntransformed into discretized tokens. Specifically, the class index is an integer starting from 0, and\\nthe continuous coordinates of the points are uniformly discretized into an integer within the range\\n[-<range> ,<range> ]. For object detection and visual grounding tasks, the point number Nis equal\\nto 2, representing the the top-left and bottom-right points of object’s bounding box. In the case of\\ninstance segmentation, we employ multiple ( N >8) points along the object boundary to represent an\\ninstance mask [ 69]. Other perception tasks such as pose estimation (keypoint detection) can also be\\nformulated as language instructions in this way.\\nAn example of language instruction for the instance segmentation task is as follows: “ Segment all the\\nobjects of category set <class> within the <range> of the image and generate a list of the format\\n(c, x1, y1, x2, y2, ..., x8, y8). Here, c represents the index of the class label starting from 0, and (x1,\\ny1, x2, y2, ..., x8, y8) correspond to the offsets of boundary points of the object relative to the center\\npoint. The image is: <image> ”.\\n3.3 Language-Guided Image Tokenizer\\nVisionLLM considers images as a kind of foreign language and converts them into token represen-\\ntations. Unlike previous works [ 17,60,34] that utilize fixed-size patch embeddings to represent\\nimages, we introduce the language-guided image tokenizer to flexibly encode visual information that\\naligns with task-specific language prompts or instructions.\\nSpecifically, give an image X∈RH×W×3with height Hand width W, we first feed it to the image\\nbackbones ( e.g., ResNet [ 23]) and extract visual features Fvof four different scales. Addition-\\nally, we leverage a text encoder ( e.g., BERT [ 16]) to extract the language features Flfrom given\\nprompts. The language features are then injected into each scale of visual features through cross-\\nattention [ 55], yielding multi-scale language-aware visual features, enabling the alignment of features\\nacross modalities.\\nAfterward, we propose to adopt a transformer-based network ( e.g., Deformable DETR [ 82]) with M\\nrandom-initialized queries Q={qi}M\\ni=1to capture the high-level information of images. We build\\nthe transformer-based network on top of the multi-scale language-aware visual features to extract M\\nimage tokens T={(ei, li)}M\\ni=1, each of which is represented by an embedding eiand a location li,\\ndenoting the semantic and positional information of the token. This design not only represents the\\nimages independent of input resolution but also extracts the visual representation that is informative\\nwith respect to the language prompts.\\n3.4 LLM-based Open-Ended Task Decoder\\nWe build our decoder on Alpaca [ 53], an LLM that is adapted from LLaMA [ 54], to handle various\\nvision-related tasks with language guidance. However, Alpaca has some inherent drawbacks for\\nvision-centric tasks, such as (1) It only has a few digit tokens ( e.g., 0∼9) in its vocabulary, which\\nrestricts its ability to locate objects by numbers; (2) It uses multiple tokens to represent the category\\nname, resulting in an inefficient scheme in object classification; and (3) It is a causal model that is\\ninefficient for visual perception tasks.\\nTo tackle these issues, we expand the vocabulary of LLM with additional tokens specially designed\\nfor vision-centric tasks. First, we add a set of location tokens, denoted as { <p-512> , ..., <p0> ,\\n...,<p512> }, where <p i> represents the discretized offset of i∈[−512,512] to the location li\\nof the image token, and the relative value to image height or width is equal to i/512. These\\ntokens successfully transform the object localization task from continuous variable prediction to\\nmore unified discrete bin classification. Second, we introduce semantics-agnostic classification\\ntokens { <c0> ,<c1> , ...,<c511> } to replace category name tokens, which overcomes the inefficiency\\nof using multiple tokens to represent categories. The mapping between category names and the\\nclassification tokens is flexibly provided in the category set <class> of language instructions, such as\\n{\"person\":<c0> ,\"car\":<c1> ,\"black cat\":<c2>, ...}. This design allows our model to select\\nthe appropriate category name from the provided category set, facilitating efficient and accurate\\nobject classification.\\n6LLM-based Open-Ended Task DecoderTask definedby instructionsparsingformat 1: \"<cls> <x1> <y1> ...\"format 2: \"<bos>\"...format n: ...Figure 4: Illustration of the “output-format-as-\\nquery” decoding process. “ <cls> <x1> <y1> ...”\\ndenote the queries of the object’s class index and\\nboundary points, and “ <bos> ” denotes the begin-\\nning of string.Moreover, to address the inefficiency caused\\nby the causal framework, we introduce output-\\nformat-as-query decoding. We first use LLMs to\\nparse the structural output format from the task\\ninstructions ( e.g., “<cls> <x1> <y1> <x2>\\n<y2> ” for object detection, “ <bos> ” for image\\ncaptioning), and then feed the tokens of struc-\\ntural output format as queries to the decoder\\nto generate the desired output according to the\\nqueries. This simple method enables our model\\nto not only avoid inefficient token-by-token de-\\ncoding in visual perception tasks, but also keep\\na unified framework for vision-language tasks.\\nIn this way, the output of object location and\\nclassification is formulated as a foreign lan-\\nguage, thus unifying these vision-centric tasks\\ninto the format of token classification. Therefore, both vision-language and vision-only tasks can be\\nsupervised with the cross entropy loss like language tasks. In addition, for efficient training, we adopt\\nthe Low-Rank Adaptation (LoRA) approach [ 24], which allows us to train and fine-tune the models\\nwithout excessive computational costs. It also acts as a bridge between the language and visual tokens,\\nfacilitating effective alignment between the two modalities, ensuring better task customization, and\\nimproving the convergence of the overall system.\\n4 Experiment\\n4.1 Experimental Settings\\nDatasets. VisionLLM unifies the output formats of vision and language tasks as vocabulary gen-\\neration, which enables models to be jointly trained on a wide range of tasks. In the experiments,\\nwe investigate the general modeling capacities of VisionLLM on five vision-centric tasks, including\\nobject detection, instance segmentation, visual grounding, image captioning, and visual question\\nanswering. For object detection and instance segmentation, COCO2017 [ 32] is used for training and\\nevaluation. For visual grounding, we combine the annotations of RefCOCO [ 75], RefCOCO+ [ 75]\\nand RefCOCOg [ 39] for training, resulting in over 120k referred objects in total. And our models are\\nevaluated on the validation set of RefCOCO. For image captioning and visual question answering, we\\nadopt COCO Caption [ 10] and LLaV A-Instruct-150K [ 33] as the training source. We evaluate the im-\\nage captioning performance on the COCO Karpathy test split following common practice [ 28,58,71].\\nWe mainly use qualitative results (see Figure 2d) to demonstrate the VQA capability of our model,\\nas LLaV A-Instruct-150K is not compatible with the standard VQA benchmark. These tasks differ\\nin their granularity, ranging from coarse-grained image level to fine-grained pixel level, enabling a\\ncomprehensive evaluation of the model’s ability to adapt to different levels of customization through\\nlanguage instructions.\\nImplementation Details. We implement two variants of VisionLLM with two image backbones, i.e.,\\nResNet [ 23] and InternImage-H [ 59]. For the language-guided image tokenizer, we adopt BERT-\\nBase [ 5] as the text encoder and Deformable DETR (D-DETR) [ 82] to capture high-level information.\\nWe set the number of queries Mto 100, and the number of encoder/decoder layers to 6 for D-DETR.\\nFor the LLM, we employ Alpaca-7B [ 53], a LLaMA [ 54] model fine-tuned with instructions, and\\nequip it with LoRA [24] for parameter-efficient fine-tuning.\\nThe model is trained in two stages. In the first stage, we initialize the model with the pre-trained\\nweights of D-DETR, BERT, and Alpaca-7B, and train the visual backbone and the language-guided\\nimage tokenizer, while freezing most parameters of the LLM except a few LoRA parameters. To\\nsimplify the training complexity, in this stage, we mainly focus on object detection tasks with random\\nobject categories and task descriptions. In the second stage, we freeze the visual backbone and\\nintroduce the unified supervision of multiple tasks. Unless otherwise specified, the training runs for\\n50epochs on 4×8NVIDIA A100 GPUs. AdamW [ 36] is used as the optimizer, with one sample per\\nGPU. We employ the cosine annealing schedule [ 37] as the learning policy, with an initial learning\\n7Table 1: Results on standard vision-centric tasks. ‘Intern-H” denotes InternImage-H [ 59]. “sep”\\nindicates that the model is separately trained on each task.\\nMethod BackboneOpen-\\nEndedDetection Instance Seg. Grounding Captioning\\nAP AP 50AP75AP AP 50AP75 P@0.5 BLEU-4 CIDEr\\nSpecialist Models\\nFaster R-CNN-FPN [48] ResNet-50 - 40.3 61.0 44.0 - - - - - -\\nDETR-DC5 [7] ResNet-50 - 43.3 63.1 45.9 - - - - - -\\nDeformable-DETR [82] ResNet-50 - 45.7 65.0 49.1 - - - - - -\\nMask R-CNN [22] ResNet-50 - 41.0 61.7 44.9 37.1 58.4 40.1 - - -\\nPolar Mask [69] ResNet-50 - - - - 30.5 52.0 31.1 - - -\\nPix2Seq [8] ResNet-50 - 43.2 61.0 46.1 - - - - - -\\nUNITER [11] ResNet-101 - - - - - - - 81.4 - -\\nVILLA [19] ResNet-101 - - - - - - - 82.4 - -\\nMDETR [27] ResNet-101 - - - - - - - 86.8 - -\\nVL-T5 [13] T5-B - - - - - - - - - 116.5\\nGeneralist Models\\nUniTab [72] ResNet-101 - - - - - - - 88.6 - 115.8\\nUni-Perceiver [83] ViT-B - - - - - - - - 32.0 -\\nUni-Perceiver-MoE [81] ViT-B - - - - - - - - 33.2 -\\nUni-Perceiver-V2 [28] ViT-B - 58.6 - - 50.6 - - - 35.4 116.9\\nPix2Seq v2 [9] ViT-B - 46.5 - - 38.2 - - - 34.9 -\\nVisionLLM-R50 sep ResNet-50 -44.8 64.1 48.5 25.2 50.6 22.4 84.4 30.8 112.4\\nVisionLLM-R50 ResNet-50 ✓ 44.6 64.0 48.1 25.1 50.0 22.4 80.6 31.0 112.5\\nVisionLLM-H Intern-H ✓ 60.2 79.3 65.8 30.6 61.2 27.6 86.7 32.1 114.2\\nrate of 2×10−4. In addition to the experiments in the main paper, more experimental settings and\\nablation studies are provided in the supplementary material due to space limitations.\\n4.2 Task-Level Customization\\nWe first evaluate the task-level customization capability of VisionLLM. VisionLLM supports coarse-\\ngrained task customization, including visual perception tasks and visual-language tasks. Table 1\\npresents the evaluation results on four standard vision-centric tasks, including object detection,\\ninstance segmentation, visual grounding, and image captioning. We compare our model with task-\\nspecific methods as well as recently-proposed vision generalist models. Note that, unless specifically\\nmentioned, the results of our model come from a shared-parameter generalist model and switch\\ndifferent tasks by changing the language instructions only. Detailed instructions could be found in\\nthe supplementary material.\\nObject Detection. Object detection is a fundamental computer vision task that involves identifying\\nand localizing objects of interest within an image. Our method achieves comparable or higher results\\nto others, 44.6mAP, with a ResNet-50 [ 23] backbone. With the same backbone i.e.ResNet-50,\\nour method outperforms Pix2Seq [ 8] by1.4mAP, which also discretizes the output coordinates to\\nintegers. Furthermore, benefiting from the output-format-as-query framework (see Sec. 3.4), we can\\ndecode multiple predictions in parallel during inference, making our approach more efficient. Using\\nInternImage-H [ 59] as the visual backbone, we obtained 60.2% mAP, which is close to the current\\nstate-of-the-art detection-specific model [59], demonstrating the scalability of our generalist model.\\nVisual Grounding. Visual grounding associates textual descriptions with corresponding regions\\nor objects within an image. Training visual grounding and object detection can potentially conflict\\nwith each other, as object detection aims to detect all the objects, while visual grounding should only\\nlocalize the referred object and suppress other objects. Benefiting from our unified task instructions\\nand the strong instruction comprehension capabilities of LLMs, our model performs both tasks\\neffectively and achieves a result of 80.6P@0.5 for visual grounding. With InternImage-H as the\\nbackbone, we achieve 86.7P@0.5 on the validation set of RefCOCO.\\nInstance Segmentation. Instance segmentation involves identifying and segmenting individual\\nobjects within an image. We employ a flexible number of points ( i.e., 8∼24) along the object\\nboundary to represent an instance mask. Compared to mainstream models specific to instance\\nsegmentation, our model has a comparable mask AP 50(61.2% with InternImage-H [ 59]) but relatively\\nlow mask AP 75. This gap could potentially arise from factors as follows: (1) We discretize the output\\n8Table 2: Experiments of object-level and output format customization. We conduct these\\nexperiments based on VisionLLM-R50, and report the performance of box AP and mask AP on\\nCOCO minival for (a) and (b), respectively. “#Classes” and “#Points” indicate the number of classes\\nand boundary points, respectively. “*” indicates that we report the mean AP of the given classes, e.g.,\\n10 classes.\\n(a) Object-level customization.\\n#Classes AP AP 50AP75APSAPMAPL\\n10∗48.9 72.6 51.2 31.7 47.5 67.3\\n20∗52.7 73.6 56.8 31.8 53.2 70.5\\n40∗49.3 70.7 53.2 33.1 53.6 63.8\\n80∗44.6 64.0 48.1 26.7 47.9 60.5(b) Output format customization.\\n#Points AP AP 50AP75APSAPMAPL\\n8 18.5 45.7 11.6 9.9 19.7 28.7\\n14 22.9 48.3 19.4 11.0 25.1 36.0\\n16 24.2 49.9 20.9 11.5 26.3 36.8\\n24 25.1 50.0 22.4 12.5 27.4 38.2\\nTable 3: Ablation studies on language-guided image tokenizer and hyper-parameters.\\n(a) Effect of text encoder in the\\nlanguage-guided image tokenizer.\\nw/ BERT Freeze COCO RefCOCO\\n- - 44.7 48.1\\n✓ - 44.8 84.1\\n✓ ✓ 1.3 34.3(b) Effect of image tokenization\\nmethod.\\nTokenization AP\\nAverage Pooling 23.1\\nOurs 44.8(c) Effect of the num-\\nber of bins (#Bins).\\n#Bins AP\\n257 34.9\\n513 40.8\\n1025 44.8\\n2049 44.8\\ncoordinates to integers for unifying tasks, which introduces information loss; (2) Due to the memory\\nand computational constraint, the number of points in our model is limited, which also results in a\\nperformance drop; and (3) Point-based methods typically yield lower results compared to direct mask\\nprediction methods, such as Mask R-CNN [22].\\nImage Captioning. We also evaluate our model in a representative vision-language task, i.e.image\\ncaptioning task, and report the BLEU-4 [ 43] and CIDEr [ 56] metrics. Note that we do not adopt the\\nbeam search [2] or CIDEr optimization [49]. We can observe that VisionLLM achieves competitive\\nperformance to previous methods. With ResNet-50, we obtain a BLEU-4 score of 31.0and a CIDEr\\nscore of 112.5. When using InternImage-H as the backbone, our model achieves a comparable\\nBLEU-4 score of 32.1and a CIDEr score of 114.2. These results demonstrate the effectiveness of\\nVisionLLM in generating descriptive and contextually relevant captions for images.\\n4.3 Object-Level & Output Format Customization\\nOur VisionLLM not only allows for customizing the task description, but also for adjusting the\\ntarget object and the output format using language instructions. Here, we evaluate our model’s\\nfine-grained customization ability on COCO. In particular, to customize the target object, we modify\\nthe<class> in language instructions to change the model’s recognition target from 10classes to\\n80classes. Likewise, to customize the output format, we modify the number of points in language\\ninstructions to change the task output format. Table 2 shows that our method can perform well for\\nboth object-level and output format changes.\\n4.4 Ablation Study\\nIn this section, we analyze the effect of key components and hyper-parameters on VisionLLM. Unless\\notherwise specified, we use ResNet-50 [ 23] backbone and perform the ablation experiments for object\\ndetection tasks with random classes and task descriptions on COCO2017 [32].\\nSingle Task vs.Multiple Tasks. We perform an ablation study to assess the impact of multi-task\\nlearning with language instructions on VisionLLM. As shown in Table 1, the single-task trained\\nmodel VisionLLM-R50 sepis slightly better than the jointly trained model VisionLLM-R50 except\\nimage captioning. This is due to the multitasking conflicts that also affect previous generalist\\nmodels [83, 81], and it reflects a trade-off between accuracy and generalization.\\nText Encoder in Language-Guided Image Tokenizer. We examine the role of text encoder ( i.e.,\\nBERT) in our language-guided image tokenizer in Table 3a, where we report the results for object\\n9detection and visual grounding. The first two rows show that BERT is not essential for object\\ndetection but it is crucial for visual grounding. We also investigate the effect of freezing the text\\nencoder during training. The last row indicates that freezing BERT hinders the alignment of vision\\nand language modalities and thus degrades the performance for both tasks.\\nImage Tokenization Method. As a comparison to our query-based tokenization, we employ average\\npooling on the feature maps from the D-DETR encoder to obtain Mpatch embeddings, which serve\\nas token representations for the image. Results in Table 3b indicate a clear advantage of our method.\\nThis is due to its ability to capture information from objects of various sizes in a more flexible way.\\nNumber of Localization Tokens. We vary the number of localization tokens from 257(i.e., -\\n128∼128) to 2049 (i.e., -1024 ∼1024), to investigate its impact on visual perception performance. As\\npresented in Table 3c, the model consistently exhibits improvement as the number of localization\\ntokens increases until it reaches a saturation point. Remarkably, a substantial performance boost\\nis observed when the number is raised from 257to1025 (+9.9AP). These results indicate that a\\nhigher number of localization tokens enables the models to achieve finer localization abilities, thereby\\nimproving localization accuracy.\\n5 Conclusion\\nIn this paper, we have presented VisionLLM, a novel framework that leverages the power of large\\nlanguage models (LLMs) to address vision-centric tasks in an open-ended and customizable manner.\\nWe have designed unified language instruction that matches the format of language models and covers\\nvarious vision-centric tasks including visual perception. We have also developed a language-guided\\nimage tokenizer and an LLM-based task decoder that can handle open-ended tasks according to the\\ngiven language instructions. We have verified the effectiveness of our models on a series of tasks\\nwith different granularities, demonstrating their remarkable generality and flexibility.\\nBroader Impact. We envision that this work will promote the fusion of visual and language tasks.\\nIn addition, since our work is built on open-source pre-trained vision foundation models and large\\nlanguage models, requiring low training resources, thus reducing the carbon footprint. We do not\\nforesee obvious undesirable ethical/social impacts at this moment.\\n10Appendix\\nA Example Instructions\\nAs described in Sec. 3.2 of the main paper, we follow self-instruct [ 65] to design a set of seed\\ninstructions with placeholders and employ LLMs to create diverse related task descriptions for\\ncoarse-grained task-level customization. Here, we show some examples of instructions for task-level\\ncustomization, including object detection, instance segmentation, visual grounding, image captioning,\\nand visual question answering (VQA). Following various instructions, our model can elegantly switch\\namong different vision-centric tasks and accomplish them in a unified manner like LLMs.\\nA.1 Object Detection\\nExample 1. “Please examine the image and identify all objects in the category set <class> .For each\\nobject, specify its location within the range <range> by determining the top-left and bottom-right\\ncorners of its bounding box. To indicate the object’s class and location, provide the output in the\\nformat (c, x1, y1, x2, y2), where ‘c’ represents the class index starting from 0, and (x1, y1, x2, y2)\\ncorrespond to the offsets of the bounding box corners relative to the center point. The image is:\\n<image> ”\\nExample 2. “Identify all the objects in the image that belong to the category set <class> and predict\\na bounding box around each one. The output should be a list in the format (c, x1, y1, x2, y2), where c\\nrepresents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left\\nand bottom-right corners of the box relative to the center point. The coordinates should be within\\n<range> .The image is: <image> ”\\nExample 3. “For each object in the image that is a member of the category set <class> ,output a\\ntuple with the index of class label starting from 0 and the offsets of corners relative to the center point\\nthat encloses the object. The offsets should be in the order of top-left and bottom-right corners of the\\nrectangle and should be within <range> .The output format should be (c, x1, y1, x2, y2). The image\\nis:<image> ”\\nA.2 Instance Segmentation\\nExample 1. “Segment the objects from the image with class labels from <class> and output their\\ncoordinates within range <range> .The coordinates should be given as the boundary points relative\\nto the center point, and the output format should be (c, x1, y1, x2, y2, ..., x20, y20), where c is the\\nindex of the class label that starts from 0. The image is: <image> ”\\nExample 2. “Segment all the objects from the category set <class> in the provided image and output\\na tuple (c, x1, y1, x2, y2, ..., x14, y14) for each, where c is the index of the class label in the category\\nset that starts from 0, and (x1, y1, x2, y2, ..., x14, y14) correspond to the offsets of boundary points\\non the instance mask relative to the center point which should be within <range> .The image is:\\n<image> ”\\nExample 3. “In the provided image, please segment all the objects in category set <class> within\\nthe range <range> by providing their coordinates in the (c, x1, y1, x2, y2, ..., x24, y24) format, where\\n‘c’ denotes the index of the class label starting from 0, and (x1, y1, x2, y2, ..., x24, y24) stand for the\\noffsets of boundary points relative to the center point. The image is: <image> ”\\nA.3 Visual Grounding\\nExample 1. “Please find the object in the category set {<expression>:<cls0>} within the range\\n<range> .Please provide the output in the format (c, x1, y1, x2, y2), where c is the class index starting\\nfrom 0, and (x1, y1, x2, y2) are the offsets of the top-left and bottom-right corners of the bounding\\nbox relative to the center point. The image is: <image> ”\\nExample 2. “Given the input image, category set {<expression>:<cls0>} ,and the range\\n<range> ,please locate the object in the image and output the corresponding coordinates in the tuple\\n(c, x1, y1, x2, y2), where c is the index of the class label starting from 0, and (x1, y1, x2, y2) are the\\n11offsets of the top-left and bottom-right corners of the rectangle relative to the center point. The image\\nis:<image> ”\\nExample 3. “For each object in the image that belongs to the {<expression>:<cls0>} category\\nset, please provide the class label (starting from 0) and the offsets from the center of a bounding\\nbox that encloses the object. The corner offsets should be in the order of top-left and bottom-right,\\nand within the range <range> .The output should be in the format (c, x1, y1, x2, y2). The image is:\\n<image> ”\\nA.4 Image Captioning\\nExample 1. “The image is <image> .Write a caption: ”\\nExample 2. “The image is <image> .Please describe this image: ”\\nExample 3. “With the objects in the <image> ,please generate a caption for the image: ”\\nA.5 Visual Question Answering\\nExample 1. “The image is <image> .Please generate an answer according to the question:\\n<question> . ”\\nExample 2. “The image is <image> .Please answer the question <question> according to the\\nimage . ”\\nExample 3. “With the objects in the <image> ,<question> . ”\\nB Loss Function\\nVisionLLM consists of two model components: language-guided image tokenizer and LLM-based\\nopen-task decoder. So the total loss Lof our model can be written as:\\nL=Ltok+Ldec, (1)\\nwhereLtokandLdecdenote the loss of language-guided image tokenizer and LLM-based open-task\\ndecoder, respectively. We introduce the two loss functions as follows:\\nLanguage-Guided Image Tokenizer. Different from the Q-Former [ 29], we use a supervision\\nmethod similar to that of Deformable DETR [ 82], but with a different loss Ltok: category-agnostic\\nclassification (focal loss [ 31]) and center point regression ( L1loss). As explained in Sec. 3.3, our\\nimage tokenizer extracts Mimage tokens T={(ei, li)}M\\ni=1, each of which is represented by an\\nembedding eiand a location li(i.e., absolute coordinates of the center point).\\nLLM-Based Open-Ended Task Decoder. We handle two cases in decoding processing differently.\\n(1) For regular word prediction, we train with standard next-token supervision [ 54,45,6,46]; (2) For\\nunordered set prediction ( e.g., bounding boxes), we first output a sequence of tokens according to the\\noutput format (see the output-format-as-query paradigm in Sec. 3.4), then use bipartite matching to\\nalign the LLM-predicted outputs with the ground truths. Despite the differences, we use cross-entropy\\nto compute the loss Ldecin a unified way for both cases.\\nC Training Schedule\\nAs shown in Figure 5, to speed up the convergence of VisionLLM, we split the training schedule of\\nVisionLLM into two stages:\\nStage 1. In this stage, we initialize the language-guided image tokenizer by loading the pre-trained\\nweights of Deformable DETR [ 82] and BERT [ 16]. Additionally, Alpaca [ 53] is employed as the\\nLLM-based open-ended task decoder. To align visual tokens with text tokens, we make the language-\\nguided image tokenizer trainable while freezing most parameters of the pre-trained Alpaca, with only\\na few LoRA [ 24] parameters left tunable. We only focus on object detection in this stage to simplify\\nthe training difficulty, with random task descriptions and object categories.\\nStage 2. The second stage builds upon the model weights obtained from the first stage. For efficiency,\\nwe freeze the visual backbone ( e.g., ResNet [ 23]) in the language-guided image tokenizer. Notably,\\n1201020304050\\n01020304050Detection mAP (%)EpochSingle-StageTwo Stages (ours)Stage-1 (Object Detection)Stage-2 (Multiple Tasks)Figure 5: Comparison of two training schedules for VisionLLM. We found that a two-stage\\ntraining from easy to hard converges faster than a single-stage training.\\nTable 4: More ablation studies for VisionLLM.\\n(a) Effect of randomness.\\nRandomness AP\\nNone 45.2\\n+ Random Task Description 45.1\\n++ Random Object Category 44.8\\n+++ Random Output Format 44.6\\n(Multi-task Joint Training)(b) Effect of LoRA [24].\\nLoRA Randomness AP\\n✗ ✗ 45.2\\n✗ ✓ 1.2\\n✓ ✓ 44.8(c) Effect of the number\\nof image tokens.\\n#Tokens AP\\n50 44.5\\n100 44.8\\n200 45.1\\n300 45.2\\n(d) Effect of Seq2Seq.\\nSeq2Seq AP\\n✓ -\\n✗ 44.8(e) Large vocabulary object detection.\\nDataset #Classes AP\\nCOCO 80 44.8\\nLVIS 1203 18.9\\nthis stage introduces the unified supervision of multiple tasks, including object detection, instance\\nsegmentation, visual grounding, image captioning, and VQA, facilitating the model to leverage the\\npower of LLMs to understand and manipulate visual information holistically.\\nD More Ablation Studies\\nIn this section, we provide more ablation studies and analysis of VisionLLM. Unless otherwise\\nspecified, we use ResNet-50 [ 23] backbone and perform the ablation experiments for object detection\\ntasks with random task descriptions and object categories on COCO 2017 [32].\\nRandomness. In Table 4a, we examine the effect of introducing randomness during training for\\nVisionLLM, including randomness in task descriptions, object categories, and output formats ( i.e.,\\nmulti-task joint training). Initially, without any randomness, the model achieves a box AP of 45.2.\\nHowever, as randomness is gradually applied, interesting phenomena emerge: while there is a slight\\ndecrease ( 45.2→44.6) in the AP of standard detection with the introduction of randomness, the\\noverall benefits of enhanced task customization and open-ended capabilities outweigh this minor\\ntrade-off. Overall, introducing randomness during training in VisionLLM positively impacts its\\ncapacity for open-ended tasks and customization.\\nLow-Rank Adaptation (LoRA). As shown in Table 4b, when randomness is not applied, the\\nmodel achieves 45.2box AP without using LoRA [ 24]. However, when randomness is employed,\\nit is observed that the model fails to converge without using LoRA. Conversely, when LoRA and\\nrandomness are used together, the model is able to converge. This indicates that LoRA plays a crucial\\nrole as a bridge between the language and visual tokens, enabling effective alignment between the\\ntwo modalities and improving the convergence of the overall system.\\nNumber of Image Tokens. We vary the number of image tokens from 50to300to investigate\\ntheir impact on the performance. Results are presented in Table 4c. As the number of image tokens\\nincreases, the performance continues to improve. This makes sense because a larger number of\\n131 2 3 4 5 6 7 844.8 44.7 44.8 44.7 44.8 44.7 44.7 44.8\\nPromptAP\\nFigure 6: Evaluation results using eight different prompts. The first six prompts use different task\\ndescriptions of object detection, while the last two prompts employ random category orders. These\\nresults show that the performance of different prompts is similar, only a 0.1AP gap is observed.\\nbicyclecar\\nmotorcycleairplanebustraintruckboat\\ntraffic lightfire hydrantstop sign\\nparking meterbenchbirdcatdoghorsesheepcow\\nelephantbearzebragiraffe\\nbackpackumbrellahandbagtie\\nsuitcasefrisbeeskis\\nsnowboardsports ballkite\\nbaseball batbaseball gloveskateboardsurfboard\\ntennis racketbottle\\nwine glasscupforkknifespoonbowl\\nbananaapple\\nsandwichorangebroccolicarrothot dogpizzadonutcakechaircouch\\npotted plantbed\\ndining tabletoilettv\\nlaptopmouseremotekeyboardcell phonemicrowaveoventoastersink\\nrefrigeratorbookclockvase\\nscissors\\nteddy bearhair driertoothbrush010203040506070mAP (%)standard 80 classes\\nrandom 40 classes\\n(a) We randomly select 40 classes in random order to form the category set.\\nbicyclecar\\nmotorcycleairplanebustraintruckboat\\ntraffic lightfire hydrantstop sign\\nparking meterbenchbirdcatdoghorsesheepcow\\nelephantbearzebragiraffe\\nbackpackumbrellahandbagtie\\nsuitcasefrisbeeskis\\nsnowboardsports ballkite\\nbaseball batbaseball gloveskateboardsurfboard\\ntennis racketbottle\\nwine glasscupforkknifespoonbowl\\nbananaapple\\nsandwichorangebroccolicarrothot dogpizzadonutcakechaircouch\\npotted plantbed\\ndining tabletoilettv\\nlaptopmouseremotekeyboardcell phonemicrowaveoventoastersink\\nrefrigeratorbookclockvase\\nscissors\\nteddy bearhair driertoothbrush010203040506070mAP (%)standard 80 classes\\nrandom 80 classes\\n(b) We randomly change the order of 80 classes to form the category set.\\nFigure 7: Per-category AP on COCO dataset. We randomly select some categories to form the\\ncategory set <class> in language instructions.\\nimage tokens provides a more detailed description of the image content. Considering computational\\ncomplexity, we adopted 100image tokens in our experiments.\\nRobustness to Prompt Changes. Since VisionLLM is trained with random prompts, including\\nrandom task descriptions and random categories, one may ask whether there is a large performance\\nvariance across different prompts. To validate the stability of VisionLLM, we conduct experiments\\nusing eight different prompts. The first six prompts employ different task descriptions, while the last\\ntwo prompts involve random category orders. In the case of random category orders, we map the\\ncategories back to the COCO standard category order for evaluation. As shown in Figure 6, most\\nevaluation results are distributed closely to 44.8AP. The performance differences among prompts are\\nmarginal, demonstrating that VisionLLM is robust to different prompts.\\nInstruction Following Capability. As shown in Figure 7, when the prompt only contains 40classes,\\nthe performance for these categories remains normal, while the performance for the remaining\\ncategories is close to zero. This indicates that VisionLLM can dynamically detect objects based on\\nthe given class set <class> in instructions while disregarding the other classes that are not mentioned.\\nThis result highlights the flexibility of VisionLLM in adhering to instructions.\\nOutput-Format-As-Query vs.Seq2Seq. In VisionLLM, we introduce the output-format-as-query\\nframework for LLM decoder. Alternatively, we also experiment with the sequence generation method\\nlike Pix2Seq [ 8] for object detection with random task descriptions and object categories. However,\\nwe find that the loss is hard to converge in this paradigm, which indicates that the seq2seq decoding\\nmay need a more detailed design or a longer training schedule for the open-ended visual tasks, while\\nthe proposed output-format-as-query framework is more effective for open-ended tasks.\\nLarge-Vocabulary Object Recognization. To validate the capacity of VisionLLM in the large-\\nvocabulary scenario, we further conduct the experiments on the challenging dataset LVIS [ 20] with\\n1203 categories. Due to the limited number of language tokens, we randomly select 80 classes\\nfor training in each iteration. During inference, we divide the 1203 categories into 16 groups and\\n14#Points=8\\n#Points=14\\n#Points=16\\n#Points=24\\nFigure 8: Customization of instance masks using the different number of points. Notably, we\\nonly modify the output format mentioned in the prompt, i.e.the number of segmentation points. For\\nmore details, please see the example prompts provided in Sec. A.2.\\npredict the results in a sliding-window manner. As shown in Table 4e, without tricks like federal loss,\\nVisionLLM-R50 can achieve 18.9mAP on LVIS.\\nE Qualitative Analysis\\nCustomization of Segmentation Points. In this experiment, we focus on modifying the output\\nformat mentioned in the prompt , specifically the number of points for instance segmentation (see\\nSec. A.2). The results are visualized in Figure 8. Remarkably, by increasing the number of points\\nfor segmentation, we observe that the model successfully predicts more refined object masks. This\\nvalidates the capability of our method to precisely customize the output format, showcasing fine-\\ngrained control over the segmentation process.\\nCustomization of Category Set. We change the content of the category set <class> in language\\ninstructions and visualize the predictions in Figure 9. It is observed that the model can correctly\\npredict the object category according to the provided category set. This demonstrates that VisionLLM\\nhas a strong capacity to understand and reason over the semantic information of language instructions,\\nwhich allows it for flexible category customization in open-vocabulary scenarios.\\nImage Description & VQA. Benefiting from the power of LLMs, VisionLLM exhibits a strong\\nability in generating long descriptions for images and answering visual questions with complex\\nreasoning. We show the examples in Figure 10.\\n15Instruction:“Identify the objects in the image that belong to {‘person’: <c0>, ..., ‘frisbee\\': <c29>, ...}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”\\nInstruction:“Identify the objects in the image that belong to {‘person’: <c0>, ‘bicycle’: <c1>, ..., ‘backpack\\': <c24>, ..., ‘toothbrush’: <c79>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”\\nInstruction :“Identify the objects in the image that belong to {‘frisbee\\': <c0>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”\\nInstruction:“Identify the objects in the image that belong to {‘bicycle’: <c0>, ‘surfboard’: <c1>, ‘sports ball\\': <c2>, ‘backpack’: <c3>, ‘the man wearing blue T-shirt’: <c4>}and draw a bounding box around eachone. The output should be a list oftuples in the format (c, x1, y1, x2, y2), where ‘c’represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is:<image>”\\nFigure 9: Customization of category set. These results demonstrate that VisionLLM can correctly\\ndetect the specified objects according to the category set <class> provided in language instructions.\\n16Instruction:“The image is: <image>.Analyze the image in a comprehensiveand detailedmanner.”VisionLLM:“The image features a busy city street with a large red double-decker bus driving down the road. There are several people walking on the sidewalk, some of whom are carrying backpacks. The bus is positioned towards the left side of the image. There are several people walking along the sidewalk, with some closer to the bus and others further away. The overall atmosphere suggests a busy urban environment with people going about their daily routines.”\\nVisionLLM:“The image features a large, open-air market filled with a variety of fruits and vegetables. There are numerous mangoes, watermelons and apples displayed in the market, with some fruitsplaced closer to the foreground and others further back. The market is bustling with activity as people can be seen browsing and purchasing the fresh produce. The market is likely a popular destination for those seeking a fresh and healthy diet.”Instruction:“The image is: <image>.Write a detaileddescription of the given image.”\\nInstruction:“The image is: <image>.Write a shortdescription of the given image.”VisionLLM:“The image features a lively open-air market with a variety of fruits and vegetables. People are busy browsing and buying fresh produce.”(a) Image description with controllable text length\\nInstruction:“The image is: <image>. Please generate an answer according to the question: what reasons could there be for the cat squatting on the shoes?”VisionLLM:“There could be several reasons. One possibility is that the cat is seeking warmth or comfort, as shoes can provide a cozy and warm surface for the cat to rest on. Another reason could be that the cat is seeking attention or affection from its owner, who is standing nearby. Cats are known to be curious creatures, and they may be seeking attention or attention from their owners.”VisionLLM:“The working environment in this office space appears to be open, spacious, and well-equipped. There are several desks set up for employees, providing ample workspace for each individual. The office features a modern and contemporary design, which suggests a creative and collaborative work environment. The presence of several chairs and the organization of the office, including the laptops and monitors, indicate that the space is well-maintained and conducive to productivity. Overall, the office appears to be a pleasant and efficient work environment for its employees.”Instruction:“The image is: <image>.What do you think of the working environment here?”\\n(b) Visual question answering with reasoning\\nFigure 10: Visualization of the image description and VQA capabilities of VisionLLM.\\n17References\\n[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nArthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot\\nlearning. arXiv preprint arXiv:2204.14198 , 2022. 1, 4\\n[2]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2018. 9\\n[3]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference\\non Computer Vision , 2015. 4\\n[4]Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via\\nimage inpainting. Advances in Neural Information Processing Systems , 2022. 4\\n[5]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\\nunderstanding? In International Conference on Machine Learning , 2021. 7\\n[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in Neural Information Processing Systems , 2020. 3, 4, 12\\n[7]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision ,\\n2020. 3, 4, 8\\n[8]Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\\nframework for object detection. arXiv preprint arXiv:2109.10852 , 2021. 8, 14\\n[9]Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence\\ninterface for vision tasks. arXiv preprint arXiv:2206.07669 , 2022. 4, 8\\n[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\\narXiv:1504.00325 , 2015. 4, 7\\n[11] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing\\nLiu. Uniter: Universal image-text representation learning. In Computer Vision–ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX . Springer, 2020. 8\\n[12] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\\nadapter for dense predictions. In International Conference on Learning Representations , 2023. 1\\n[13] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\\nInInternational Conference on Machine Learning , 2021. 8\\n[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\\npreprint arXiv:2210.11416 , 2022. 4\\n[15] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS , 2023. 3\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 6, 12\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning\\nRepresentations , 2021. 6\\n[18] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint\\narXiv:2211.07636 , 2022. 1\\n[19] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\\ntraining for vision-and-language representation learning. Advances in Neural Information Processing\\nSystems , 2020. 8\\n18[20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.\\nInProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5356–5364,\\n2019. 14\\n[21] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose vision\\nsystems. arXiv preprint arXiv:2104.00743 , 2021. 4\\n[22] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , 2017. 3, 8, 9\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\\nInProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016. 6, 7, 8, 9, 12,\\n13\\n[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,\\n2021. 7, 12, 13\\n[25] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt\\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction\\nmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. 4\\n[26] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\\nSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision , 2022. 2\\n[27] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\\nMdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , 2021. 8\\n[28] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\\nXiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and\\nvision-language tasks. arXiv preprint arXiv:2211.09808 , 2022. 4, 7, 8\\n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 4, 5, 12\\n[30] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 2, 4, 5\\n[31] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object\\ndetection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2017. 12\\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on\\nComputer Vision . Springer, 2014. 7, 9, 13\\n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\\narXiv:2304.08485 , 2023. 4, 5, 7\\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , 2021. 6\\n[35] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang\\nYang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatbots\\nbeyond language. arXiv preprint arXiv:2305.05662 , 2023. 2, 4, 5\\n[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\\nLearning Representations . 7\\n[37] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint\\narXiv:1608.03983 , 2016. 7\\n[38] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916 , 2022. 4\\n[39] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\\nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition , 2016. 7\\n19[40] OpenAI. Gpt-4 technical report. arXiv , 2023. 3\\n[41] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI , 2022. 1, 3\\n[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. Advances in Neural Information Processing Systems , 2022. 3, 4\\n[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\\nLinguistics , 2002. 9\\n[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\\nby generative pre-training. 2018. 1, 4\\n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\\nby generative pre-training. OpenAI , 2018. 12\\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. 12\\n[47] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.\\narXiv preprint arXiv:2205.06175 , 2022. 4\\n[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\\nwith region proposal networks. In Advances in Neural Information Processing Systems , 2015. 8\\n[49] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\\nsequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition , 2017. 9\\n[50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 , 2023. 2, 4,\\n5\\n[51] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou,\\nand Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision , 2023. 1\\n[52] Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modeling\\nfor self-supervised vision representation learning. arXiv preprint arXiv:2206.01204 , 2022. 1\\n[53] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for\\nResearch on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 2023. 3, 6, 7, 12\\n[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023. 3, 6, 7, 12\\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems ,\\n30, 2017. 6\\n[56] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\\nevaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2015. 9\\n[57] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint\\narXiv:2205.14100 , 2022. 1, 4\\n[58] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\\nZhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-\\nsequence learning framework. arXiv preprint arXiv:2202.03052 , 2022. 1, 4, 7\\n[59] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\\nconvolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023. 1, 2,\\n7, 8\\n20[60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\\nLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media ,\\n8(3):415–424, 2022. 6\\n[61] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 , 2022. 1, 2\\n[62] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499 , 2022. 2\\n[63] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499 , 2022. 4\\n[64] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\\nSegmenting everything in context. arXiv preprint arXiv:2304.03284 , 2023. 2, 4\\n[65] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\\npreprint arXiv:2212.10560 , 2022. 4, 5, 11\\n[66] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana\\nArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking\\ngeneralization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705 ,\\n2022. 4\\n[67] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 ,\\n2021. 4\\n[68] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 , 2023. 2, 4,\\n5\\n[69] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo.\\nPolarmask: Single shot instance segmentation with polar representation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2020. 6, 8\\n[70] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\\nperception as object discovery and retrieval. arXiv preprint arXiv:2303.06674 , 2023. 4\\n[71] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\\nLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European\\nConference on Computer Vision , pages 521–539. Springer, 2022. 4, 7\\n[72] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\\nLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European\\nConference on Computer Vision , 2022. 8\\n[73] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\\naction. arXiv preprint arXiv:2303.11381 , 2023. 2, 4, 5\\n[74] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful\\nprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797 , 2021. 2\\n[75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\\nreferring expressions. In European Conference on Computer Vision . Springer, 2016. 7\\n[76] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language\\nprompt learning. arXiv preprint arXiv:2210.07225 , 2022. 2\\n[77] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 ,\\n2022. 3\\n[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068 , 2022. 3\\n21[79] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673 ,\\n2022. 2\\n[80] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.\\n4\\n[81] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng\\nDai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. arXiv preprint\\narXiv:2206.04674 , 2022. 1, 4, 8, 9\\n[82] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\\ntransformers for end-to-end object detection. In International Conference on Learning Representations ,\\n2021. 3, 6, 7, 8, 12\\n[83] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-\\nperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022. 2, 4, 5, 8, 9\\n22',\n",
       " '[from docx0]\\nHousing shortage in big cities can cause severe social consequences. Some people think only government action can solve this problem. To what extent do you agree or disagree?Nowadays, rural-urban migration has become an irreversible trend globally, burdening big cities with great population density and housing shortage. Many believe that only government can tackle this issue and I agree with the statement.The root cause for inadequate housing is the severe tension between the intense demand and low supply. Divergence in job growth, incomes and future prospects between strong-performing cities and those lagging behind has been starkly prominent, which leads to the massive and continuous flow of people into mega-cities and economic centers. It is just this mobility of labor that contributes to an increasing housing market. Meanwhile, the scarcity of land in these cities means a weak supply of houses. This incompatibility has decided that it is not a situation individuals can alter.However, if the government intervenes, this conundrum can be at least eased to some extent. For example, from the perspective of demand, the reallocation of high value-added industries to satellite or second tier cities will definitely alleviate the population overload and at same time facilitate the rise of such medium size cities. Meanwhile, from the angle of supply, the industries transfer will make room for more houses and the government can simultaneously implement regulations or taxes to control the real estate speculation so as to release the storage to people in real needs. Moreover, if the shortage still continues, urban expansion is another possible option. By transforming the farmlands into houses and apartments, the situation will improve.To conclude, I believe that housing crisis is never an issue that individual can tackle and only with government involvement will this be addressed or at least mitigated.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w') as f:\n",
    "    f.write('++-------------------++\\n'.join(texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
