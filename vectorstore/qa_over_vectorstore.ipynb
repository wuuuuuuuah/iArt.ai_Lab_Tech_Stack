{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to load your OPENAI_API_KEY into your environment\n",
    "with open(\"my_key.txt\", \"r\") as f:\n",
    "    key_file = f.read()\n",
    "\n",
    "key_file = key_file.split(\"\\n\")\n",
    "keys = {}\n",
    "\n",
    "for line in key_file:\n",
    "    key, value = line.split(':')\n",
    "    keys[key.strip()] = value.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions & Answers over VectorStore\n",
    "- I/O Spec\n",
    "    - Input: \n",
    "        - Files Directory (Now Support .txt, .doc, .pdf)\n",
    "        - Web Link\n",
    "    - Output: None\n",
    "\n",
    "- Chain Spec\n",
    "    - Type: Index-Related Chains (Stuffing)\n",
    "    - Components:\n",
    "        - Document Loaders: \n",
    "            - Directory\n",
    "            - Web Link\n",
    "        - Text Splitters: RecursiveCharacterTextSplitter\n",
    "        - VectorStores: Chroma\n",
    "        - Embedding Model: OpenAI - text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = [\n",
    "    \"https://en.wikipedia.org/wiki/Attention\",\n",
    "]\n",
    "directory_path = \"Files_Directory\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage File\n",
    "Set up the Google Drive API:\n",
    "\n",
    "Follow the steps mentioned in the first response to set up the Google Drive API and obtain the credentials.json file.\n",
    "Set up a Python web server:\n",
    "\n",
    "Choose a web framework such as Flask or Django to handle the server-side functionality.\n",
    "Install the necessary dependencies for the chosen framework.\n",
    "Implement the authentication flow:\n",
    "\n",
    "Provide a user interface for users to log in with their Google accounts and authorize your application to access their Google Drive.\n",
    "Use the Google Sign-In API or OAuth 2.0 to handle the authentication process.\n",
    "Retrieve the access token after successful authentication.\n",
    "Handle file upload:\n",
    "\n",
    "Create a file upload endpoint in your server-side application.\n",
    "Receive the file upload request from the client-side, along with the access token obtained in the authentication step.\n",
    "Use the access token to authenticate and authorize access to the user's Google Drive.\n",
    "Use the Google Drive API to retrieve the file contents based on the provided file ID.\n",
    "Save the file contents to your server or perform any required processing on the file.\n",
    "Implement error handling and security measures:\n",
    "\n",
    "Handle any potential errors that may occur during the authentication or file upload process.\n",
    "Implement appropriate security measures to protect user data and prevent unauthorized access.\n",
    "Test and deploy your application:\n",
    "\n",
    "Test your application thoroughly to ensure it functions as expected.\n",
    "Deploy your application to a web server or cloud platform to make it accessible to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GCSFileLoader\n",
    "loader = GCSFileLoader(\n",
    "    project_name = \"aist\",\n",
    "    bucket = \"langchain\",\n",
    "    blob = \"sasa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/langchain/document_loaders/gcs_file.py:31\u001b[0m, in \u001b[0;36mGCSFileLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import google-cloud-storage python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install google-cloud-storage`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[39m# Initialise a client\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m storage_client \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mClient(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproject_name)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Create a bucket object for our bucket\u001b[39;00m\n\u001b[1;32m     33\u001b[0m bucket \u001b[39m=\u001b[39m storage_client\u001b[39m.\u001b[39mget_bucket(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbucket)\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/google/cloud/storage/client.py:173\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, project, credentials, _http, client_info, client_options, use_auth_w_custom_endpoint)\u001b[0m\n\u001b[1;32m    170\u001b[0m             no_project \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    171\u001b[0m             project \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<none>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[39msuper\u001b[39;49m(Client, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    174\u001b[0m     project\u001b[39m=\u001b[39;49mproject,\n\u001b[1;32m    175\u001b[0m     credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[1;32m    176\u001b[0m     client_options\u001b[39m=\u001b[39;49mclient_options,\n\u001b[1;32m    177\u001b[0m     _http\u001b[39m=\u001b[39;49m_http,\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m no_project:\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/google/cloud/client/__init__.py:321\u001b[0m, in \u001b[0;36mClientWithProject.__init__\u001b[0;34m(self, project, credentials, client_options, _http)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, project\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, credentials\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, client_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _http\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     _ClientProjectMixin\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, project\u001b[39m=\u001b[39mproject, credentials\u001b[39m=\u001b[39mcredentials)\n\u001b[0;32m--> 321\u001b[0m     Client\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    322\u001b[0m         \u001b[39mself\u001b[39;49m, credentials\u001b[39m=\u001b[39;49mcredentials, client_options\u001b[39m=\u001b[39;49mclient_options, _http\u001b[39m=\u001b[39;49m_http\n\u001b[1;32m    323\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/google/cloud/client/__init__.py:178\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, credentials, _http, client_options)\u001b[0m\n\u001b[1;32m    174\u001b[0m         credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mload_credentials_from_file(\n\u001b[1;32m    175\u001b[0m             client_options\u001b[39m.\u001b[39mcredentials_file, scopes\u001b[39m=\u001b[39mscopes\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39;49mauth\u001b[39m.\u001b[39;49mdefault(scopes\u001b[39m=\u001b[39;49mscopes)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_credentials \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mcredentials\u001b[39m.\u001b[39mwith_scopes_if_required(\n\u001b[1;32m    181\u001b[0m     credentials, scopes\u001b[39m=\u001b[39mscopes\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m client_options\u001b[39m.\u001b[39mquota_project_id:\n",
      "File \u001b[0;32m~/miniconda3/envs/SemanticKernel/lib/python3.11/site-packages/google/auth/_default.py:648\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    640\u001b[0m             _LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    641\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mNo project ID could be determined. Consider running \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`gcloud config set project` or setting the \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39menvironment variable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                 environment_vars\u001b[39m.\u001b[39mPROJECT,\n\u001b[1;32m    645\u001b[0m             )\n\u001b[1;32m    646\u001b[0m         \u001b[39mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[0;32m--> 648\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files in a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_loader = DirectoryLoader(\n",
    "    path = directory_path,\n",
    "    glob = \"**/[!.]*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
      "Falling back to partitioning with ocr_only.\n"
     ]
    }
   ],
   "source": [
    "docs_from_directory = directory_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='arXiv:2305.01625v1 [cs.CL] 2 May 2023\\n\\nUnlimiformer: Long-Range Transformers with Unlimited Length Input\\n\\nAmanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley Carnegie Mellon University, USA {abertsch,ualon,gneubig, mgormley}@cs.cmu.edu\\n\\nAbstract\\n\\nTransformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every to- ken in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention compu- tation across all layers to a single k-nearest- neighbor index; this index can be kept on ei- ther the GPU or CPU memory and queried in sub-linear time. This way, we can index ex- tremely long input sequences, while every at- tention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We demonstrate Unlimiformer’s efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART (Lewis et al., 2020a) and Longformer (Beltagy et al., 2020a) by extending them to unlimited inputs without additional learned weights and without modi- fying their code. We make our code and mod- els publicly available\\'.\\n\\n1 Introduction\\n\\nTransformers (Vaswani et al., 2017) are the domi- nant sequence-to-sequence architecture. Pretrained transformers generally have a context window of 512 (e.g. BERT (Devlin et al., 2019)) or 1024 to- kens (e.g. BART (Lewis et al., 2020b)), which are sufficient lengths for many current conditional generation datasets (XSum; Narayan et al., 2018) (CNN/DM; Nallapati et al., 2016).\\n\\nFor inputs between 1k and 16k tokens, special- ized long-context models have been developed. These models employ clever techniques to spar- sify or approximate attention (e.g. Longformer\\n\\n\\'https://github.com/abertsch72/unlimiformer\\n\\n--- 16384 tokens\\n\\n--- 4096 tokens\\n\\n--- 1024 tokens\\n\\n10°\\n\\n104\\n\\nInput tokens\\n\\n103§\\n\\n(BAy) wnsx\\n\\n(BAY) INQ/NND\\n\\n(BAY) AIXIY\\n\\n(6Ay) Hodaynon\\n\\n(BAY) Wnspim\\n\\n(BAY) YOSAIRLIEN\\n\\n(bAy) wns3oog\\n\\n(xe) vO3A3eIeN\\n\\n(xe) wns3oog\\n\\n(Xel) WNSH{IM\\n\\nFigure 1: Long-range transformers can avoid input truncation in some datasets; however, there are datasets with inputs many times longer than these models’ max- imum input length. The dotted lines represent three common maximum input lengths for models; the bars are the average or maximum input length in each dataset, as indicated. Averages for datasets from Koh et al. (2022).\\n\\n(Beltagy et al., 2020b), Performers (Choroman- ski et al., 2020)), allowing the maximum input length to quadruple while remaining computation- ally feasible. Datasets in this length include most long-document summarization or question answer- ing datasets, such as arXiv summarization (Cohan et al., 2018).\\n\\nBut 16,384 is not the upper limit for the length of context required for generation: tasks that involve long narratives, such as book summarization (Krys- cinski et al., 2021) or narrative question-answering (Kocisky et al., 2018), often have inputs exceeding 100k tokens. A challenge set for Wikipedia arti- cle generation (Liu* et al., 2018) contains inputs longer than 500k tokens. Open-domain tasks in generative question answering could conceivably synthesize information from even larger inputs, e.g.\\n\\nDatastore of one long input\\n\\nEncoder\\n\\nEncode chunks\\n\\nkNN Search\\n\\nRetrieved\\n\\nhidden states\\n\\nA\\n\\nDecoder Layer\\n\\nCross attention\\n\\n|ab c\\n\\nx A\\n\\nInput: | a b ® d @ f\\n\\nd | e | f\\n\\nFigure 2: In this example, the encoder’s maximum input length is 2 tokens. A 6-token input is encoded in chunks and stored in the datastore. We inject Unlimiformer into each decoder layer prior to cross-attention. In Unlim- iformer, we perform kNN search to select a 2-token context for each attention head from the datastore; then, cross-attention is computed using keys and values from the entire input sequence.\\n\\nanswering a question about the aggregate prop- erties of all living person articles on Wikipedia. Figure 1 shows the size of several popular sum- marization and question answering datasets, plot- ted against common context window lengths; the longest inputs are more than 34 times longer than Longformer’s context window.\\n\\nIn these extremely-long-input cases, vanilla transformers cannot be feasibly scaled, as naive attention has quadratic complexity. Long-input transformers, though more efficient than standard transformers, require significant computational re- sources, which increase with increased context win- dow size. Additionally, increasing the context win- dow necessitates re-training the model from scratch with a new context window size, which is compu- tationally and environmentally costly.\\n\\n‘We introduce Unlimiformer, a retrieval-based method which augments pretrained language mod- els to accept inputs of unbounded length at test time. Unlimiformer can be injected into any existing encoder-decoder transformer to permit unbounded inputs. Given a long input sequence, Unlimiformer constructs a datastore over the hidden states of all input tokens. Then, the decoder’s standard cross- attention queries the datastore, and attends to the top-k input tokens. The datastore can be stored in either GPU or CPU memory and admits sublinear queries.\\n\\nUnlimiformer can be applied directly over a trained model, and can improve an existing check- point without any further training. When finetun- ing Unlimiformer, performance is further improved. We demonstrate that Unlimiformer can be applied to multiple base models, such as BART (Lewis\\n\\net al., 2020a) or PRIMERA (Xiao et al., 2022), without adding weights and without re-training. Across a variety of long-range seq2seq datasets, Unlimiformer not only performs better on these datasets than strong long-range transformers such as Longformer (Beltagy et al., 2020b), SLED (Ivgi et al., 2022) and Memorizing Transformers (Wu et al., 2022), but we also find that Unlimiformer can be applied on top of a Longformer-encoder- decoder model for further improvement.\\n\\n2 Unlimiformer\\n\\nTransformers are limited in their maximum input length because of the fixed size of the encoder con- text window. However, at different points in de- coding, different information may be relevant; also, different attention heads may be attending to differ- ent types of information (Clark et al., 2019). Thus, a fixed context window may waste effort on tokens that an attention head does not attend strongly to.\\n\\nUnlimiformer allows each head to choose a sep- arate context window from the full-length input at each decoding step. This is formalized by injecting an Unlimiformer lookup into the decoder: prior to cross-attention, the model performs a k-nearest neighbor (kNN) search in an external datastore to choose a set of per-decoder-layer per-attention- head tokens to attend to.\\n\\n2.1 Encoding\\n\\nTo encode an input sequence that is longer than the model’s context window, we encode overlapping chunks of the input, following Ivgi et al. (2022), keeping only the middle half of the outputs from each chunk, to ensure that the encodings have suf-\\n\\nficient context on both sides. Finally, we index the encoded inputs in a datastore, using a library such as Faiss (Johnson et al., 2019).\\n\\n2.2 Retrieval\\n\\n\\n\\naugmented cross\\n\\n\\n\\nattention\\n\\nIn standard cross-attention, a transformer decoder attends to the encoder’s last hidden states, where the encoder usually truncates the input and encodes only the  first tokens in the input sequence.\\n\\nInstead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from a much longer input sequence for each cross- attention head, and attend only fo these top-k. This allows retrieving keys from the entire input se- quence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens, while usually preserv- ing more than 99% of the attention mass.\\n\\nFigure 2 displays our generic changes to any sequence-to-sequence transformer’s architecture. The full input is encoded using the encoder in chunks and stored in a datastore; then, the datastore of encoded hidden states is queried at each decod- ing step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency as detailed below.\\n\\n2.3 Attention reformulation\\n\\nThe use of a datastore for the encoded tokens, pio- neered by Wu et al. (2022), increases the maximum input length significantly. However, this naive ap- proach requires constructing separate datastores for the attention keys and values at each layer and each head, for a total of 2 x L x H datastores, where L is the number of decoder layers and H is the num- ber of attention heads.”> A separate datastore for each attention head in each decoder layer would be both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer.\\n\\nInstead, we present a different order of comput- ing the well-known transformer attention formula, which allows us to store a single datastore across all attention heads and all decoder layers.\\n\\nThe standard cross\\n\\n\\n\\nattention calculation for a\\n\\n2See Memorizing Transformers’ official implementa- tion at https:/github.com/google-research/meliad/blob/ main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L.334-L339\\n\\nsingle head in a transformer is:\\n\\nAttn(Q, K, V\\') = softmax (QKT> Vo where () is the product of the decoder state and the query weight matrix, and K, V\\' are the product of the last encoder hidden state with the key and value weight matrices respectively. Our goal is to retrieve a set of keys K that maximize QK T with the size of K fixed to the size of the model’s con- text window, and then perform normal attention computation over Kpeq.\\n\\nLet h, be the decoder state and h. be the last encoder layer’s hidden state. We can refactor the transformer’s attention computation as follows:>\\n\\nQK\" = (haWy) (heWy) \" @) = (haW,) W) b, \" - (dequT ) hT\\n\\nThus, the retrieval step can be formulated as choos- ing the encoder hidden states h. that maximize (haﬂ/’[/quT ) h.\\'. This rewriting has two major advantages: first, there is no need to store the keys for each head and layer separately: we can store a single datastore of the hidden states states h. only, and just project the queries to hyW, W, us- ing head-specific W, and W} second, the values can be calculated trivially given he, so there is no need to store the values in a separate datastore than the keys (or compute them at all) before decoding. Thus, rather than constructing 2 x L x H datas- tores and retrieving from every datastore during each decoding step, we construct a single datastore and retrieve from it by just projecting the decoder hidden states to per-head hgﬂ/’[/quT .\\n\\nThis reformulation has not, to our knowledge, been performed before in retrieval-augmented at- tention. This allows the application of retrieval- augmented attention at each head and layer with negligible increase in time and space required. In contrast to Memorizing Transformers’s single-layer retrieval augmentation, which requires construct- ing two datastores and retrieves the same tokens for each attention head, Unlimiformer uses one datastore and allows retrieval augmentation over any number of layer and individualized retrieval per-head.\\n\\n3For brevity, we omit the linear layers’ bias term, because\\n\\nthe softmax function is invariant to adding the same constant to all inputs.\\n\\nMethod name Training\\n\\n\\n\\ntime input\\n\\ntotal # tokens in\\n\\nexample seen at\\n\\ntraining time\\n\\nValidation\\n\\n\\n\\ntime input\\n\\n(e.g. early stopping) Test\\n\\n\\n\\ntime input\\n\\nBaseline 1024 +test Unlimiformer 1024 +early stop w/ Unlimiformer 1024 Train chunked +test Unlimiformer 1024 \"SLED (Ivgietal.,2022) ek Longformer (Beltagy et al., 2020a) 16k Random-encoded training 8-16k Retrieval training 8-16k Alternating training 8-16k\\n\\n1024 1024 1024\\n\\n1024 1024 unlimited\\n\\n1024 unlimited unlimited\\n\\nall unlimited unlimited\\n\\nek 16k 16k\\n\\n16k 16k 16k\\n\\n8\\n\\n\\n\\n16k unlimited unlimited\\n\\n8\\n\\n\\n\\n16k unlimited unlimited\\n\\n8\\n\\n\\n\\n16k unlimited unlimited\\n\\nTable 1: A comparison of the training methodologies using BART (context window size 1024) as a running exam- ple. The dashed line separates methods that are approximately the same training-time cost as the baseline from\\n\\nthose that require significant additional compute.\\n\\n3 Training Methods\\n\\nThe method as described can be used, at test time, on any already-trained model. Next, we turn our focus to training methodologies to further improve the performance of Unlimiformer. Table 1 sum- marizes and contrasts the methodologies described below, and Appendix A contains further implemen- tation details.\\n\\n3.1 Low (additional\\n\\n\\n\\n) Cost Training Methods\\n\\nWe first consider training strategies that do not re- quire significant additional compute as compared to the standard finetuning regime.\\n\\n+test Unlimiformer: As the simplest case, we use a standard fine-tuning regime, where the inpu is truncated during training. At inference time only, we inject Unlimiformer into the trained model to process full-length inputs.\\n\\n+early stop w/ Unlimiformer: We train withou Unlimiformer, but when we evaluate the model for early stopping, we use Unlimi former for generation on the validation set. This results in choosing a slightly differen checkpoint to stop training at; the additional computational cost here is minor, and comes only from the application of Unlimiformer during inference over the validation set.\\n\\nTrain chunked +test Unlimiformer: As a data augmentation strategy, we split each training example into chunks of the context-window size, and treat each chunk as its own training example. This is orthogonal to the Unlimi- former model, but has the advantage that all embeddings from the full-length training ex- ample are back-propagated into during train-\\n\\ning, instead of truncated—albeit across sev- eral examples. We apply early stopping with Unlimiformer.\\n\\n3.2 Long\\n\\n\\n\\nrange Training Methods\\n\\nWe also consider training Unlimiformer directly, which introduces additional computational cost.\\n\\nRandom-encoded training: At each training step, the full (longer-than-context-window) training example is encoded in chunks; then, the keys for each decoder layer are chosen randomly from the encoded hidden states. This weakly simulates a nearest-neighbors search, but is computationally cheaper.\\n\\nRetrieval training: At each training step, the keys for each decoder head and layer are selected using a kNN search. This is not exact if the inputs are longer than 16k tokens, as memory requirements at training-time require the trun- cation of the input; however, this is closest to the test-time computation.\\n\\nAlternating training: To gain the benefits of each, we alternate epochs of Random-encoded train- ing and Retrieval training. The use of ran- dom neighbors increases the likelihood that all tokens will be chosen as keys occasion- ally, while retrieval training is identical to the test-time setting for most inputs.\\n\\n4 Experimental Settings\\n\\n4.1 Datasets\\n\\n‘We experiment with three long-document summa- rization datasets with varying domains and prop- erties. Table 2 contains summary statistics for\\n\\nAvg # tokens\\n\\nDataset Domain # examples Input Output Input length distribution\\n\\nGovReport Government 19,402 9,616 597 74 —E 303192 SummScreen | TV shows 4,348 8,987 137 23 . | 2635 BookSum Literature 436 143,301 1294 10a06 _ . 354006\\n\\nTable 2: Dataset statistics. The last column is a visualization of the distribution of input example lengths in each dataset; the histogram is binned by powers of 2, with the minimum and maximum input size displayed on either\\n\\nend. Base model Training method ROUGE 1/2/L/BERTScore GovReport SummScreen\\n\\nBARTpase Standard finetuning 48.7/19.2/22.8/643 29.7/6.2/17.7/56.3 BARTpse +test SLED (Ivgi et al., 2022) 45.8/16.1/20.2/62.7 27.5/55/16.7/55.9 BARTpase +test Unlimiformer 49.7/19.6/22.0/64.8 30.9/6.5/18.2/57.5 BARTpse +early stop w/ Unlimiformer ~ 51.0/20.5/21.5/65.1 32.1/6.8/18.6/57.6 BARTpse Train chunked 46.2/17.8/21.7/63.3 28.1/5.6/17.0/55.6 BARTpase +test Unlimiformer 53.4/22.5/22.5/66.0 29.3/6.6/17.6/57.0 PRIMERA  Standard finetuning 55.1/239/259/67.0 323/7.1/183/57.1 PRIMERA +test Unlimiformer 33.3/7.7/19.1/57.6\\n\\n56.5/24.8/26.3/67.7\\n\\nTable 3: Test results on long-document datasets, for low-cost training methods: the training costs are no higher than standard finetuning that truncates the inputs according to the model’s max input size. The best metric in every\\n\\ndataset and every training category is marked in bold.\\n\\neach dataset. GovReport and SummScreen are in- cluded in the Scrolls benchmark (Shaham et al., 2022) . We report ROUGE 1/2/L (Lin, 2004) and BERTScore F1 (Zhang et al., 2019).\\n\\nGovReport (Huang et al., 2021) is a long- document summarization dataset where the task is to write the executive summary of a US govern- ment report.\\n\\nSummScreen (Chen et al., 2022) is a long- document summarization dataset where the task is to write the recap of a TV show episode, pro- vided the transcript of the episode as input.\\n\\nBookSum (Krysciriski et al., 2021) is a long- document summarization dataset of public-domain works of literature. BookSum has paragraph, chap- ter, and book-level settings; we consider only the BoOKSUM-Book setting, where the task is to gen- erate a book-level summary given the full text of the novel as input.\\n\\n4.2 Baselines\\n\\nBART (base) (Lewis et al., 2020b) is a pretrained seq2seq model (139M parameters), commonly used for summarization tasks. Its maximum input sequence length is 1024 tokens.\\n\\nPRIMERA (Xiao et al., 2022) is a Longformer- Encoder-Decoder (LED;..4.; Beltagy et al, 2020b) (447M parameters), pretrained specifically for multi-document summarization. Its maximum input length is 4096 tokens; in the encoder, the global attention is sparse over the full sequence with dense local attention in a 1024-token window.\\n\\nSLED (Ivgi et al., 2022) is a method for aug- menting pretrained encoder-decoder models for longer contexts by performing fusion in-decoder (Izacard and Grave, 2021); this allows the use of pretrained models, albeit with an expensive fine- tuning, and the input sequence length is eventually memory bounded. We replicate the authors’ exper- iments for BART+SLED on several datasets.\\n\\nMemorizing Transformers (Wu et al., 2022) is the most similar work to ours; they propose a train- able attention gate that moderates between the stan- dard cross-attention and attention over retrieved keys from a datastore in one layer. Since the public implementation® for this method is “not officially supported” and is not fully reproducible, we ap- proximated it by using attention over the datastore\\n\\n“https://github.com/google\\n\\n\\n\\nresearch/meliad\\n\\nin only a single decoder layer; this is equivalent to heir setting with the learned interpolation parame- ter g set to 1.> Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily everage pretrained LMs, while Unlimiformer is ully non-parametric and can improve performance without finetuning. Further, Wu et al. (2022) ap- plies attention retrieval to a single layer because of computational constraints, but our attention re- ormulation (Section 2.3) allows for the use of Un- imiformer in every decoder layer while still being more efficient than Memorizing Transformers.\\n\\n5 Experimental Results\\n\\n5.1 Long Document Summarization\\n\\nTable 3 shows the results in the long-document (4k- 16k token input) summarization datasets. First, we can see that applying Unlimiformer on an exist- ing checkpoint without any training (+test Unlim- iformer) improves BART},. by, for example, 1.8 ROUGE-1 points on both datasets. By contrast, ap- plying SLED without additional training decreases performance from the base model. Unlimiformer is the only model that can be applied training-free.\\n\\nEarly stop w/ Unlimiformer is also shown to be a very efficient training approach: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning.\\n\\nIn the long-range training methods in Table 4 , Unlimiformer shows consistent improvements. In almost all metrics and datasets, Unlimiformer out- performs the SLED and Memorizing Transformers baselines with the same base model.\\n\\nThe experiments with PRIMERA show two im- portant points: first, Unlimiformer that is based on BART}as performs better than the baseline PRIMERA, even though PRIMERA was pretrained on much more data, using a pretraining objec- tive that was designed for summarization; second, these experiments show that not only can Unlimi- former outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers.\\n\\nSThis is an approximation, but Wu et al. (2022) note that in their experiments, most heads learned a value for g such that they attended “almost exclusively” to the external memory.\\n\\n5.2 Book Summarization\\n\\nTable 5 shows the result on BookSum. In Book- Sum, we also see improvements from applying Un- limiformer, using both BARTpas and PRIMERA.\\n\\nRandom-encoded, Retrieval, and Alternating training show competitive performance, with the best method varying across datasets and models. The low-cost training methods underperform these training strategies but outperform the baseline mod- els; even applying Unlimiformer without training modifications improves over the base model in most settings.\\n\\n6 Analysis\\n\\n6.1 Entity mentions\\n\\nUnlimiformer outperforms all base models on BookSum (see Table 5), but the truncation baseline (using only the first 1024 tokens of the input) also shows relatively high performance on the automatic metrics. This is strongly counterintuitive for book summarization, where the plot of the book should not be apparent from reading the first pages. In the outputs from this baseline, we observe limited coherence and a high rate of hallucination (see Ap- pendix B for an example with analysis). However, this is not reflected in n-gram based overlaps, and BERTScore does not strongly distinguish between any of the BookSum models.\\n\\nFollowing the use of entity reference measures in medicial summarization (Zhang et al., 2021), we use an entity-based metric as a proxy for the informativeness of the candidate summaries. We use SpaCy® to tag all named entities in the gold summary and collect a set of unique entities. We then tag each candidate summary and compute the percentage of entities present in this summary (i.e. recall of unique entities). We report this metric (abbreviated as EntMent) in Table 5 for BookSum. The Unlimiformer models exhibit far higher entity recall, and even adding Unlimiformer only at test time without customized training doubles the entity recall of summaries from the base model.\\n\\n6.2 Input limitation\\n\\nTo evaluate the performance gains from using the full input, we artifically impose a maximum data- store size. Figure 3 shows the performance of the best BookSum model as the maximum length in- creases; entity recall increases with length.\\n\\nShttps://spacy.io\\n\\nBase model Training method ROUGE 1/2/L/BERTScore GovReport SummScreen\\n\\nBARTp .56 SLED (Ivgi et al., 2022) 54.7/244/254/67.0 327/79/19.1/584 LED:i.rqe PRIMERA (Xiaoetal,2022) 55.1/239/259/67.0 323/7.1/18.3/57.1 BARTpase Memorizing Transformers 55.2/25.1/264/67.5 327/7.4/19.2/574 BARTp .56 Unlimiformer (this work) 56.6/26.3/27.6/68.2 34.7/8.5/19.9/58.5 PRIMERA Memorizing transformers 57.0/253/26.5/67.7 33.0/73/18.4/57.3 PRIMERA  Unlimiformer (this work) 57.4/26.2/28.0/68.1 33.3/7.6/18.9/57.7\\n\\nTable 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold.\\n\\nBase model  Training method ROUGE 1/2/L EntMent BARTp ce Hierarchical (Kryscinski et al., 2021) 30.0/6.0/11.0 - BARTp.6e Standard finetuning 36.4/7.6/15.3 0.0 BARTp.6e +test Unlimiformer 355/7.7/154 219 BARTp ce +early stop w/ Unlimiformer 355/77/154 219 BARTp.6e Memorizing Transformers 35.6/6.4/14.6 5.7 BARTp.6e Unlimiformer (random-encoded training) 37.3/6.7/15.2  20.8 BARTp ce Unlimiformer (alternating training) 36.7/7.3/15.5 20.3 PRIMERA  Standard finetuning 38.6/7.2/15.6 1.6 PRIMERA +test Unlimiformer 38.3/7.5/159 8.9 PRIMERA +early stop w/ Unlimiformer 39.5/73/158 222 PRIMERA  Unlimiformer (retrieval training) 379/82/163 255 PRIMERA  Unlimiformer (random-encoded training) 39.5/7.1/15.9 9.7\\n\\nTable 5: Results on BookSum (average input length ~ 143k tokens). EntMent is entity recall, described in §6.1. Hi- erarchical summarization is a baseline reported by Krysciriski et al. (2021), where chapter summaries are combined and condensed to form a book summary. The best metric in every dataset is marked in bold.\\n\\n6.3 WikiSum\\n\\nPrevious work in dataset analysis has found that strong signal for many generation tasks is concen- trated in only part of the input (e.g. the layout bias in news summarization (Kedzie et al., 2018) or answering questions using a single paragraph in HotpotQA (Jiang and Bansal, 2019).\\n\\n‘We observe this trend in WikiSum, a multi- document summarization dataset where the inputs are all references for a Wikipedia article and the output summary is the intro paragraph of the arti- cle (Liu* et al., 2018)7. As a strong baseline, we follow the paragraph ranking scheme of Liu* et al. (2018), where the paragraphs across documents are presented in order of relevance according to TF- IDF. A baseline using only the first 1024 tokens of this sorted input outperformed Unlimiformer,\\n\\n7A full copy of WikiSum is not available online; details of our scraped copy are in Appendix A.2.\\n\\nsuggesting that the full input is not necessary to produce the summary on this dataset.\\n\\n6.4 Computational cost\\n\\nAlthough Unlimiformer does not introduce addi- tional trained parameters, the encoding of the full input, datastore construction, and datastore search increase the processing time necessary during both training and inference. We benchmark the GPU- time necessary to train BART-base for a single epoch and evaluate over the validation set using each training methodology.\\n\\nTable 6 shows the relative cost for each method. The Unlimiformer training methodologies are higher cost than the base training; however, the largest difference occurs during inference, where the full input (in Booksum, an average of 112,885 tokens) must be encoded, instead of the 1,024 to- kens encoded in the baseline approach.\\n\\nWe graph the computational cost of inference\\n\\n—e— Unlimiformer :\\n\\n20 |\\n\\n\\n\\n\\n\\nBART}56¢ |\\n\\n10 [ == mmm o -\\n\\nEntity Mention Recall\\n\\nTime per example (s)\\n\\n) &\\n\\nT\\n\\n|\\n\\n1K 2K 4K 8K 16K 32K 64K 100K350K Max datastore size Figure 3: As the maximum datastore size increases, the\\n\\nentity recall generally increases. At all datastore sizes, Unlimiformer outperforms the baseline (BART, in red).\\n\\nMethod Relative GPU-time Baseline training 1.00 £ 0.00 Chunked training 1.02 £0.02 +early stop w/ Unlimiformer | 1.00 £ 0.00 Retrieval training 1.89 £ 0.06 Random-encoded training 2.87+£0.28 Baseline inference 1.00 £ 0.00 Unlimiformer inference 4.48 £0.56\\n\\nTable 6: Computational effort per epoch for different training methodologies, relative to the baseline of stan- dard finetuning and inference. All are averaged over 3 runs on BookSum using a single 48 GB A6000 GPU, 32 GB RAM, and 16 CPUs.\\n\\nwith respect to the input length in Figure 4. When all inputs are restricted to 1,024 tokens, Unlimi- former requires additional time relative to the base- line for datastore construction and search. How- ever, the benefits of Unlimiformer are clear as input length increases. The GPU-time required increases sublinearly with input length.\\n\\n7 Related Work\\n\\nRetrieval-augmented transformers. Interpolat- ing language model probabilities with nearest neighbors retrieval from a datastore was origi- nally proposed by Khandelwal et al. (2019) to im- prove the language modeling of decoder-only mod- els. Additional work in this space has explored adding structure to this datastore (Alon et al., 2022) to further increase performance and improve ef- ficiency. More recent work has focused on lan- guage modeling for long-form text (Wu et al., 2022) and applying retrieval-augmented transformers to\\n\\n—e— Unlimiformer 25 |- - - BART}, 4 (truncates to 1024) b\\n\\n20\\n\\n\\n\\nB\\n\\ni i i i I i i i 1K 2K 4K 8K 16K 32K 64K 100K\\n\\nMax datastore size\\n\\nFigure 4: As the maximum datastore size increases, the inference cost increases sublinearly. Note the log scale.\\n\\ndownstream classification tasks (Shi et al., 2022). Our work furthers this area by applying retrieval- augmented methods to encoder-decoder models and sequence generation tasks.\\n\\nLong-context transformers. An orthogonal so- lution has emerged in the large language models literature: change the transformer model such that its time/space complexity is @ (Nlog(N)) or O(N) (Tay et al., 2020). Most solutions achieve this re- duction through sparsifying the attention mecha- nism as in Sparse Transformers (Child et al., 2019), Reformer (Kitaev et al., 2020), Longformer (Belt- agy et al., 2020b), Routing Transformers (Roy et al., 2020), ETC (Ainslie et al., 2020), and Big Bird (Za- heer et al., 2020). In other work, the attention mech- anism is either approximated or replaced entirely as in Linformer (Wang et al., 2020), Linear Trans- formers (Katharopoulos et al., 2020), Performers (Choromanski et al., 2020), and FNet (Lee-Thorp et al., 2021). For the most part, these are general purpose language models, not catered to a specific downstream task because of the high cost of pre- training. These models also are limited to the max- imum sequence length chosen during pretraining; that length, often chosen to be 8192 or 16384, is substantially lower than the lengths of many Book- Sum or WikiSum training examples. Long-document summarization. Prior work has proposed several strategies for long-document sum- marization. In particular, many methods select a subsection of input to summarize using TF-IDF (Liu* et al., 2018), smaller retriever models (Liu and Lapata, 2019), or sentence similarity metrics (Bajaj et al., 2021). An orthogonal approach is to summarize chunks of the input, then combine and condense these sub-summaries into a global\\n\\nsummary, either using vanilla transformer mod- els (Krysciniski et al. (2021), Zhang et al. (2022), (Zhang et al., 2021)) or a specialized architecture (Liu and Lapata (2019), Grail et al. (2021)). Other work has focused on expanding the amount of text that can be processed, by applying long-context transformers or developing new long-context meth- ods (Huang et al., 2021). However, these meth- ods all suffer from cascading errors: if the initial trimming or chunk summarization steps remove important information, there is no way to recover that information in the downstream summary.\\n\\n8 Conclusions\\n\\nWe present Unlimiformer, a method for augment- ing pretrained encoder-decoders with an exter- nal datastore to allow for unlimited length input. We demonstrate the usefulness of this approach for downstream sequence-to-sequence generation tasks, particularly long-document summarization. We examine several training methodologies for finetuning models with this method, and demon- strate that these strategies significantly improve over the base model, without adding weights.\\n\\n‘We expect that future work will further improve upon this performance, potentially by incorporating structure into the datastore or by retrieving embed- dings in chunks. The information retrieval com- munity has developed a wide variety of methods for improving retrieval, and we hope that the ap- plication of these methods will further improve the performance of retrieval-augmented LLMs on challenging downstream tasks. Toward this end, we release code® for easily injecting Unlimiformer into any model using the HuggingFace Transform- ers (Wolf et al., 2020) library.\\n\\n9 Limitations\\n\\nIn our experiments, we have only considered English-language datasets. While we have no rea- son to believe the method would suffer from the use of a different high-resourced language, the qual- ity of the nearest-neighbors search depends on the quality of the indexed embeddings; thus, this ap- proach may not be feasible in languages where a strong pretrained model is not available. Interpretability is a concern in any long-input summarization task, as the input may be infeasibly long for manual inspection. The retrieved embed-\\n\\nSMIT license; see repo for details\\n\\ndings at each step are difficult to interpret; further work here is necessary.\\n\\nThe length of inputs is theoretically bounded by the memory limitations of the computer used. More practically, using a CPU datastore is many times slower than a GPU datastore because of slower search and the need to transfer retrieved embed- dings to the GPU. In our experiments, we were able to use a GPU datastore for input examples exceeding 500k tokens (on GPUs no larger than 48 GBs), but this may be a concern when using smaller GPUs or even larger inputs. Additionally, CPU datastores are necessary for models with con- text windows larger than 2048 tokens, as the Faiss GPU datastore implementation does not support retrieving more than 2048 nearest neighbors.\\n\\nReferences\\n\\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va- clav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. Etc: Encoding long and structured inputs in transformers.\\n\\nUri Alon, Frank F. Xu, Junxian He, Sudipta Sen- gupta, Dan Roth, and Graham Neubig. 2022. Neuro- symbolic language modeling with automaton- augmented retrieval.\\n\\nAhsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Prad- hiksha Ashok Kumar, Rheeya Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer, Rajarshi Das, and Andrew McCallum. 2021. Long docu- ment summarization in a low resource setting using pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing: Stu- dent Research Workshop, pages 71-80, Online. As- sociation for Computational Linguistics.\\n\\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020a. Longformer: The long-document trans- former. arXiv preprint arXiv:2004.05150.\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020b. Longformer: The long-document trans- former.\\n\\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. SummScreen: A dataset for ab- stractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602-8615, Dublin, Ireland. Association for Computational Linguistics.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers.\\n\\nKrzysztof Choromanski, Valerii Likhosherstov, David\\n\\nDohan, Xingyou Song, Andreea Gane, Tamas Sar- os, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Rethinking attention with per- ormers.\\n\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT ook at? an analysis of BERT’s attention. In Pro- ceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276-286, Florence, Italy. Association or Computational Linguistics.\\n\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Na- zli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long docu- ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computa- tional Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\\n\\nQuentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1792-1810, Online. Associa- tion for Computational Linguistics.\\n\\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 1419-1436, On- line. Association for Computational Linguistics.\\n\\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2022. Ef- ficient long-text understanding with short-text mod- els.\\n\\nGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the As- sociation for Computational Linguistics: Main Vol- ume, pages 874-880, Online. Association for Com- putational Linguistics.\\n\\nYichen Jiang and Mohit Bansal. 2019. Avoiding rea- soning shortcuts: Adversarial evaluation, training,\\n\\nand model development for multi-hop QA. In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2726— 2736, Florence, Italy. Association for Computa- tional Linguistics.\\n\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. [EEE Transactions on Big Data, 7(3):535-547.\\n\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap- pas, and Francois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear at- tention.\\n\\nChris Kedzie, Kathleen McKeown, and Hal Daumé III. 2018. Content selection in deep learning models of summarization. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pages 1818-1828, Brussels, Belgium. Association for Computational Linguistics.\\n\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models.\\n\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer.\\n\\nToméas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The NarrativeQA read- ing comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317— 328.\\n\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan. 2022. An empirical survey on long document sum- marization: Datasets, models, and metrics. ACM Comput. Surv., 55(8).\\n\\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agar- wal, Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for long-form narrative summarization.\\n\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2021. Fnet: Mixing tokens with fourier transforms.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. BART: Denoising sequence-to-sequence pre-training for natural language generation, trans- lation, and comprehension. In Proceedings of the\\n\\n58th Annual Meeting of the Association for Compu- tational Linguistics, pages 7871-7880, Online. As- sociation for Computational Linguistics.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.\\n\\nPeter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summariz- ing long sequences. In International Conference on Learning Representations.\\n\\nYang Liu and Mirella Lapata. 2019. Hierarchical trans- formers for multi-document summarization. In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5070— 5081, Florence, Italy. Association for Computa- tional Linguistics.\\n\\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulgehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Lan- guage Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807, Brussels, Bel- gium. Association for Computational Linguistics.\\n\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020.  Efficient content-based sparse attention with routing transformers.\\n\\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. Scrolls:  Standardized comparison over long lan- guage sequences.\\n\\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022.  knn-prompt: Nearest neighbor zero-shot inference.\\n\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: A survey.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, £ ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30. Curran Associates, Inc.\\n\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38—45, Online. Asso- ciation for Computational Linguistics.\\n\\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing trans- formers. In International Conference on Learning Representations.\\n\\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. 2022. PRIMERA: Pyramid-based masked sentence pre-training for multi-document summa- rization. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5245-5263, Dublin, Ireland. Association for Computational Linguistics.\\n\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences.\\n\\nLongxiang Zhang, Renato Negrinho, Arindam Ghosh, Vasudevan Jagannathan, Hamid Reza Hassanzadeh, Thomas Schaaf, and Matthew R. Gormley. 2021. Leveraging pretrained models for automatic summa- rization of doctor-patient conversations. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3693-3712, Punta Cana, Do- minican Republic. Association for Computational Linguistics.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert.\\n\\nYusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed Awadallah, Dragomir Radev, and Rui Zhang. 2022. Summ”: A multi-stage summarization framework for long input dialogues and documents. In Proceed- ings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1592-1604, Dublin, Ireland. Associa- tion for Computational Linguistics.\\n\\nA Implementation Details\\n\\nA.1 Training details\\n\\nAt training time, we must backpropagate through the operations described above. Thus, the input length is bounded more strictly — the number of tokens in the full input must fit in GPU memory while the model is loaded. For the computationally expensive methods, we train using batch size 1 and truncate the longest inputs (generally, to 16k tokens). At test time, we use the full input without truncation. We train one model per setting, using the hyperparameter settings from SLED (Ivgi et al., 2022) and early stopping.\\n\\nA.2  WikiSum scraping\\n\\nWe rescraped the dataset, following the same pre- processing steps as the original authors. We ob- serve that many inputs in the scraped dataset are shorter than reported, likely due to changes in avail- ability of the data since 2017; as a preprocess- ing step, we remove all inputs that are less than 1457 words, which is the 40th percentile of citation size for the original dataset. We trained on 10,000 randomly selected examples from this version of WikiSum and evaluate on 2,000 randomly sam- pled examples (1,000 validation, 1,000 test), main- taining the same sample across all experiments. When sampling, we respect the original WikiSum train/validation/test split. We release the subset we trained on as well as our modified version of the scraping code.\\n\\nA.3 Evaluation details\\n\\nVanilla BERTScore is only well-defined up to 512 tokens; for GovReport and ScriptSumm, we eval- uate using facebook/bart-large-mnli instead. This model has context size 1024. For BookSum, we experimented with using allenai/longformer-large-4096 (con- text size 4096), as many references are longer than 1024 tokens; however, we found that this approach had no distinguishing power between model outputs, ranking all models tested within 0.3 points of each other despite observing significant differences with ROUGE, EntMent, and manual inspection.\\n\\nFor the named entity recognition in EntMent, we used SpaCy’s en_core_web_1g model.\\n\\nA4 Computational Cost\\n\\nWe estimate the total GPU time for results pre- sented in this paper did not exceed approximately 116 days of time on a single 48-GB A6000. The longest-training models, SLED and kNN training for GovReport, took approximately 10 days to train.\\n\\nB Validation Results\\n\\nTable 7 shows the validation metrics for GovReport and SummScreen.\\n\\nBase model  Training method\\n\\nROUGE 1/2/L/BERTScore\\n\\nGovReport\\n\\nSummScreen\\n\\nLow\\n\\n\\n\\ncost training methods:\\n\\nBARTpase Standard finetuning\\n\\nBARTpase +test SLED\\n\\nBARTpase +test Unlimiformer BARTpase +early stop w/ Unlimiformer BARTpase Train chunked\\n\\nBARTpase +test Unlimiformer\\n\\nLong\\n\\n\\n\\nrange training methods:\\n\\nBARTp45e SLED (Ivgi et al., 2022) BARTpase Memorizing Transformers BARTpase Unlimiformer\\n\\n47.7/18.5/22.3/64.0\\n\\n46.0/16.3/20.3/62.8\\n\\n49.5/19.6/21.9/64.8\\n\\n51.0/20.6/21.6/65.9\\n\\n48.3/18.1/22.3/63.8\\n\\n52.9/22.2/22.4/65.8\\n\\n55.5/24.8/25.8/66.9\\n\\n55.8/25.6/269/67.7\\n\\n57.4/26.4/27.9/68.2\\n\\n30.0/6.5/17.7/56.7\\n\\n28.4/59/17.0/56.0\\n\\n31.8/7.1/18.6/57.8\\n\\n32.5/72/199/579\\n\\n29.4/63/17.6/56.8\\n\\n29.4/63/17.6/56.8\\n\\n342/82/19.2/58.8\\n\\n32.8/7.6/19.3/57.7\\n\\n35.0/8.3/19.6/58.4\\n\\nLow-cost training methods: PRIMERA  Standard finetuning PRIMERA +test Unlimiformer PRIMERA +early stop w/ Unlimiformer Long-range training methods: PRIMERA Memorizing transformers PRIMERA  Unlimiformer\\n\\n55.0/23.6/25.9/66.9\\n\\n56.4/24.7/26.4/67.6\\n\\n56.4/25.0/26.4/67.6\\n\\n57.0/25.6/26.8/67.8\\n\\n58.0/26.5/28.6/68.3\\n\\n33.0/7.8/18.8/57.4\\n\\n33.1/79/18.7/574\\n\\n33.5/8.2/19.3/57.7\\n\\n329/7.7/185/57.5\\n\\n34.1/79/19.0/57.8\\n\\nTable 7: Validation results on long-document datasets (average input length between 4k to 16k tokens). The best metric in every dataset and every training category is marked in bold.\\n\\nC Sample Outputs\\n\\nThese outputs from BookSum are summaries of The Brothers Karamazov, an elaborate novel about a Russian family. Neither summary is fully factually correct, but the summary from the input-truncated model hallucinates several plotlines (e.g. a lover from the Congo, the many deaths of Pavel) which are not present in the original. The hallucinations in the Unlimiformer output are more constrained; for instance, it incorrectly describes Dmitri as a “nobleman” instead of a landowner and says he has been sentenced to death instead of jail. This summary features more of the novel’s characters and identifies plot details from the later parts of the book, such as Dmitri’s trial. Gold (reference) summary:\\n\\nThe Brothers Karamazov is a family tragedy centered around a father and his sons. Fyodor, the eldest Karamazov, has three sons: Dmitri, Ivan, and Alyosha. Ivan and Alyosha have the same mother, but Dmitri, the oldest, has a different mother. Fyodor is a greedy landowner, a bawdy lecher, and a neglectful father. Hence, the Karamazov brothers end up growing into young men under the care of various other people. But they all have returned home to visit their father, and it is the first time they all have been together for quite some time. Dmitri has a dispute with Fyodor over his inheritance, and Alyosha, who is living in a monastery, suggests that they see Father Zossima, Alyosha’s mentor. Alyosha believes that the wise old man can settle the dispute peacefully. Father Zossima is patient and kind, but Fyodor and Dmitri end up quarreling anyway. After Fyodor drives the men to frustration, they leave the monastery separately, and Alyosha worries about their family’s future. Alyosha talks to Dmitri, who confesses his complicated situation with women and money. Dmitri promised to marry a girl named Katerina, and she lent him 3,000 rubles. Instead of paying it back, he spent it on another girl named Grushenka. He wants to run away with Grushenka, but he feels that he needs to pay Katerina back before he can do so. This is why he is so interested in getting the money from Fyodor. Back at Fyodor’s house, Smerdyakov is talking to the Karamazovs. Smerdyakov is an epileptic servant who was adopted by Grigory and Marfa, Fyodor’s other servants. He was born to a woman named Lizaveta who died in childbirth. She was the town idiot, and she lived off charity from the other townspeople. Everyone called her \"Stinking Lizaveta,\" and when the town found out she was pregnant, they were furious at whoever could do such a thing to a helpless girl. They decided Fyodor must have been the culprit. Grigory and Marfa gave birth to a deformed child, and when they buried the child, they found Lizaveta, who had just given birth to Smerdyakov. They adopted the child immediately, and Fyodor named him. Father Zossima is dying, and Alyosha is distraught. Instead of asking Alyosha to stay with him during his last days, however, Father Zossima tells Alyosha he should leave the monastery to be with his family. His life gets even more complicated when a young crippled girl named Lise expresses that she has feelings for him. Alyosha visits Katerina, the girl who is engaged to marry Dmitri. Ivan is in love with her, but he feels that Dmitri is a better match for her. Frustrated and disgusted with his family’s situation, Ivan says he is going to leave town. Alyosha sees a boy being picked on by his schoolmates, and he tries to talk to the boy, but he bites Alyosha’s hand and runs away. Later, when Alyosha is bringing money to a man named Captain Snegiryov, who has been beaten by Dmitri, he recognizes the man’s son. It is Tlusha, the boy who bit his hand. The family is poor, but Captain Snegiryov refuses to take the money because he feels that he needs to earn his son’s respect after being humiliated by Dmitri—and accepting charity, especially from a Karamazov, is out of the question. When Alyosha goes back to see Katerina, he finds Lise, Madame Hohlakov’s daughter. The two realize that they love each other, and they decide to get married. Alyosha goes to visit Ivan, and he finds him in a restaurant. Ivan has gone there to get away from his father, and Alyosha sits down with him to have an intimate talk. Ivan tells his brother about his thoughts regarding God and the world. He recites to Alyosha a poem he has written called \"The Great Inquisitor.\" The poem describes Christ returning to earth in the sixteenth century. The Church throws him in jail, and The Great Inquisitor explains to him that his presence is problematic for the world. The Church has spent years trying to replace the sense\\n\\nof freedom Christ gave man with security. He talks about how cruel the world is, especially to innocent children. After their meal, Alyosha and Ivan part ways, feeling closer than ever. Ivan sees Smerdyakov when he goes back to his father’s house, and Smerdyakov tells him he is worried about Fyodor. He is worried Dmitri will come to kill him and the old man will be helpless to save himself. Ivan goes to sleep very troubled. Father Zossima is on his deathbed, and Alyosha goes to visit him. The Elder tells those around him how much Alyosha reminds him of his older brother, a boy who died when he was a youth. He talks about being a profligate youth in the army. One day, he challenged another man to a duel because of a girl. Before the duel, however, he had a change of heart. He did not shoot and, after the duel, he retired from the army and joined a monastery. He talks about how much the Bible has affected him and says that everyone should embrace the world and the people in it. He dies. Many predicted that a miracle would happen upon Father Zossima’s death, but his body begins to putrefy, filling the monastery with an awful smell. This fills the other monks with doubt that Father Zossima was the saintly man they thought he was. Alyosha is shaken by the news. He goes to see Grushenka, who has sent for him, and she admits to wanting to \"ruin\" him. When he tells her that Father Zossima has died, however, she becomes contrite about her callousness. She says she thinks she is a wicked person, and the two comfort each other. When Alyosha leaves, he has a renewed faith in Father Zossima and his teachings because Alyosha feels how wonderful it is to love and be loved in return. Meanwhile, Dmitri has become desperate. He wants to be with Grushenka, but he wants to pay Katerina back first. He goes on an odyssey, hoping that he can depend on the charity of others. He visits a man named Samsanov, a man who used to pursue Grushenka, and he hates Dmitri. He sends Karamazov to see a surly drunk, tricking Dmitri into thinking this man may be helpful. The man is practically incoherent, however, and Dmitri goes to find Madame Hohlakov. She tells Dmitri that the only way he will find 3,000 rubles is in the gold mines. In confusion, Dmitri concludes that Grushenka has gone to visit his father, and he goes to his father’s house in arage, carrying a brass pestle. When he arrives, he does not find Grushenka, but as he is leaving, Grigory, his father’s servant, thinks he has come to murder Fyodor. The two scuffle, and Dmitri hits Grigory on the head with the pestle. After determining that the man is not dead, Dmitri flees the scene and looks for Grushenka. She is with Kalganov, a former lover who had treated her poorly. Dmitri decides that he will not end up with Grushenka and decides to kill himself after seeing her one more time. He crashes her party and sits down with her gentleman friend and some other men. The situation becomes tense, and after the gentlemen make some disparaging remarks about Russians and Dmitri, Grushenka decides she does not want to be with such an insulting and vicious man. She decides that she loves Dmitri, and as the two are coming to terms with their love, the police come to arrest him for the murder of Fyodor. As the police question Dmitri, it becomes clear that the facts all support the conclusion that he did indeed murder his father, even though he did not commit the crime. He was at the scene of the crime, wielding a weapon, the night of the murder. He had said he would kill his father on several occasions. He publicly announced he was looking for 3,000 rubles and was desperate to find them, and Fyodor reportedly had an envelope with 3,000 rubles that was stolen the night of the murder. Dmitri is carried away, and very few people believe that he is innocent of Fyodor’s murder. Meanwhile, Alyosha is visiting Tlusha, the boy who bit his hand, in the hospital. The boy has fallen quite ill, and Alyosha has gotten to know many of the boy’s friends, who are also visiting him. One boy, Kolya Krassotkin, is a leader among the boys. He and Ilusha were friends, but they had a falling out because Ilusha fed a pin to a dog, and Kolya did not approve of his cruelty. When Alyosha comes to visit, he and Kolya talk for quite some time. The boy looks up to this wise man about which he has heard so much from the other boys, and he wants to impress him. The two become friends, and Alyosha treats all the boys as equals. When Kolya goes in to see Ilusha, he gives him a dog as a present. He reveals that the dog is none other but the dog Ilusha gave the piece of bread with a pin in it. Kolya has nursed the dog back to health and has fully trained him as a gesture of friendship to Ilusha. The mood is dampened, however, when the doctors go in\\n\\nto see Tlusha. Without even saying it, everyone understands that the boy does not have much time left. Ilusha is brave, and he tries to lift the spirits of those around him. Later, Alyosha visits his brother in jail. Dmitri tells Alyosha that Ivan has concocted a plan for his escape from jail. Alyosha goes to talk to Ivan, who feels strangely guilty about his father’s death. Alyosha tells his brother that he should not feel responsible for a crime that he did not commit, but Ivan stalks off angrily. He meets Smerdyakov, who tells Ivan he thinks the Karamazov brother is guilty as an accomplice to the murder. He says that Ivan wanted his father dead and left the night of the murder to try to free himself of the responsibility of protecting his father. Ivan is angry and troubled by this, and when he talks to Smerdyakov later, Smerdyakov flatly admits to hilling Fyodor. He says that Ivan’s theories and ideas were the basis for his crime and that Ivan’s talks with Smerdyakov basically rationalized the deed. When Ivan returns home after this meeting, he sees a devil in his room. The devil chastises him for being a wicked person with weaknesses and foibles that have led to disastrous circumstances. Alyosha bangs on the door and finds his brother in a feverish state, muttering about a devil and Smerdyakov. Alyosha stays the night with his brother to take care of him. Dmitri’s trial begins. Many people from all around come to see the spectacle of the parricide trial. Dmitri has an excellent lawyer, but it is a hard case to win. The prosecution brings many witnesses who testify to seemingly damning evidence against Dmitri. The defense, however, discredits one after another of these witnesses, showing ulterior motives or mitigating circumstances. Alyosha defends his brother from the stand, and Katerina gives a moving account of Dmitri’s honorable nature. Then Ivan comes into the courtroom, waving money and implicating Smerdyakov. Since he is yelling nonsense, disrupting the trial, and generally acting crazy, the court does not believe him. Suddenly, at the end of the trial, Katerina stands up again, showing a letter from Dmitri that clearly states Dmitri’s intention to kill Fyodor as a last resort. She has a change of heart and no longer wants to lie to protect a man who has hurt her so much. Word comes to the courtoom that Smerdyakov has hanged himself. After final statements are made, the verdict comes back: guilty. Dmitri is sentenced to jail. Dmitri welcomes this chance to become a new man, but he does not want to be in exile in Siberia for the rest of his life; he wants to return to his home country before he dies. Ivan is still sick, and Katerina takes care of him. Alyosha visits the boys with whom he has become friends. They are sad because Ilusha has died. Alyosha passes along Father Zossima’s teachings of love and understanding, and they all embrace his words, cheering him.\\n\\nBART (with input truncated to 1024) summary:\\n\\nFyodor Pavlovitch Karamazov, a landowner in his own day, lives with his three sons, Adelaida Ivanovna and Alexey. He is the son of the landowner, Fyodor, and his wife, Adele, are the daughters of a wealthy nobleman in the district. Adele is the daughter of a nobleman, and her father, Ivanovitch, is a prominent figure in the town. She is a beautiful, intelligent, and intelligent young woman, who is destined to be a great writer. She has a great deal of ambition, and she is determined to achieve her goals. However, her ambition is short-lived. She falls in love with a handsome young man named Ivanovn, who has recently returned from a trip to the Congo. He has a lot of money, but he is also a poor man, and he has no means of earning it. In fact, he is a miserly beggar, and so he spends most of his time trying to earn money from the poor. He also has a wife, a beautiful young girl, who he marries, and a young son, Ivan, who lives with him. He and Adele have two children, Ivan and Aleksey, and they are very happy, but they have a problem: Ivan is a poor beggar who is unable to pay his rent, and Aleksandra is a beggar. Ivan is very ill, so he is forced to sell his estate to a rich man named Vladimir. Vladimir is a rich, well-to-do man who is also rich, but who is not well-liked by the people who live in the neighborhood. Vladimir’s wife, Evgeny, is also ill, and Vladimir’s son, Pavel, is very sick. Vladimir has a bad case of typhus fever, which causes him to lose his job and his money. Vladimir, Pavel’s younger brother, is sick, and Pavel is sent to the hospital to recover his father’s condition. Pavel is also sick and dies, and Ivan is left to care for him. The next day, Pavel and Adelayna are reunited with their father, who died in a typhus epidemic in the country.\\n\\nPavel’s father, Vladimir, has died, and the family is now in mourning. Pavel has been unable to find a new wife, so the family decides to send him to Moscow, where he will be reunited with his father. The family is devastated, but Pavel is able to find an old friend, a young man who has just returned from the Congo, who will be able to help him. Pavel and Ivan are devastated by the loss of his father, but Ivan and Vladimir are able to make amends with Vladimir. Pavel, however, is devastated by this news, and, after a long period of mourning, he decides to return to Moscow. He returns to Moscow to find his father dead, and to find that his father has died. He then goes to Moscow and is buried in the city cemetery, where his father is buried. Pavel dies of a heart attack, and then he is buried with his mother. He was a young, beautiful, and beautiful woman who was raised by a kindly, kind, and generous family. He had a great time with his family, but his life was cut short when his father died. Pavel was left to live with his aunt and uncle in Moscow, and when he died, he was left alone with his uncle and his two sons. He died in Moscow and his mother, who had been living with him in a convent, died in the same convent. Pavel had been a poor, poor, and ill-treated man, but now he was able to raise his son and raise his own son. Pavel survived, and now he lives in Moscow with his Aunt and Uncle. He lives with their aunt and Uncle, who are very poor and poor. They live in a small house in the countryside, and their children are raised by their uncle and aunt. They have a great love for each other, but their love for one another is not reciprocated. Pavel also dies in the convent, and after a brief illness, Pavel is left with a broken heart. He dies in Moscow. His father, a wealthy man, is the father of his son, who was also a clergyman. Pavel died of a stroke in the hospital, and was buried in a nearby cemetery. He left Pavel and his family behind. Pavel tried to help Pavel, but was unable to save him, so Pavel was forced to leave Moscow. Pavel returned to Moscow in the middle of the night. He tried to save Pavel’s life, but it was too late for Pavel. Pavel managed to escape, and in the end, Pavel was killed by a bullet. Pavel took Pavel’s body and died of his own wounds. Pavel left Pavel to die in the forest. Pavel stayed with the family for a few years, until he died of an illness. Pavel became ill and Pavel was sent to\\n\\nUnlimiformer summary:\\n\\nThe first part of the novel focuses on the question of whether or not the Russian nobleman, Dmitri Fyodorovitch, has killed his father. In the town of Ivanovna, the lieutenant-colonel of the Mushenkhanovitch is accused of the murder of his brother Ivanovitch. The lieutenant-in- command, Vasilyevitch, takes the form of a dog, and the two men-the two men and the woman who are questioned by the court-martial-murphy. The two men cry out to the God of Russia for help in their quest to save the town. The man, afraid of the wrath of the God, hands the dog a bunch of letters that are supposed to be proof of his love for his brother. The old man-the one who had killed his mother, and then found the letter—arrives. He reads it—asked the old man to forgive him for the murder and then takes the dog away. The other men, all of whom are prisoners, demand that the man confess his crime to the court. The first and most important thing they tell the court is that they love the man. The court acquits the man and sentences the man to death. The second man-an old officer of the town, Alekandrovitch—askes to tell them the same thing. The third man—in the process of confessing his crime—is Vashenka, a drunk man who has been sent to the town to kill his father, for reasons which are not entirely clear to the people. The woman’s servant, Evgenyevna, is also the one who has told the court the story of the Medvedevitch’s murder, for the good old man’s and the young man’s love. The three men, who are separated for the first time, are laughing at the man’s attempt to seduce Mitya. The young man, in the meantime, is conscripted into the town-side. He tells the court that he loves her, but he has yet to tell her the true story. The men, in this room, demand a man to kill her, and she will not betray them. The women, in their own country, are rebelling against the man who had sent them three thousand roubles, and they will not allow the man of the people to see them. They will not let the man in the town be allowed to see the man—or Dmitriovitch; he will have her husband killed him. He will not tell the people who love him. The next man, named Vashenovitch, arrives, and takes the man away. They all begin to laugh at\\n\\nthe fact that he has succeeded in seducing and entrusting his brother Dmitri. He is then taken away to the old woman’s house, where the governor-side-of-the-world, and his sister, Arkadin, is being punished. The priestesses and the baron are shocked, for they have been so virtuous and well-suited. The only thing they will be able to do is kill the priest. They threaten to burn the priestess to death, for she has been so wicked and libidinous that she has not yet seen the priest, for her husband. The priests—ostensibly convinced that she is a woman who loves the priest and has been punished for her love and for allowing the priest to marry her. The last man, Yakivitch, arrives at the house, and, after a long day of drinking and then some of the men—is killed. He and the priest are ordered to leave the town so that the priest can finally be reunited with the people of the old lady. The final man, the commander of the St. Petersburg town of Arkadina, is sentenced to death for the crime of having killed and then the lieutenant of the governor, for taking the money. The commander, the former lieutenant-delegation of the People’s Army, is summarily executed, and all the men, except for the commander, have been summarily punished for their crime. The entire town is shocked and, in a very dramatic way, the priestesses plead for the forgiveness of the man, for allowing them to kill and imprison Ivan. They plead for their brother to be restored as well, for all the people they have loved, and for the priestor to tell the\\n\\nstory.' metadata={'source': 'Files_Directory/Unlimiformer - Long-Range Transformers with Unlimited Length Input.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(docs_from_directory[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs_from_directory[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs + docs_from_directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Content from Web Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_loader = UnstructuredURLLoader(\n",
    "    urls = web_url,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = url_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Toggle the table of contents\\n\\nToggle the table of contents\\n\\nAttention\\n\\n60 languages\\n\\nالعربية\\n\\nAsturianu\\n\\nAzərbaycanca\\n\\nবাংলা\\n\\nБеларуская\\n\\nБългарски\\n\\nBosanski\\n\\nCatalà\\n\\nČeština\\n\\nDansk\\n\\nDeutsch\\n\\nEesti\\n\\nEspañol\\n\\nEsperanto\\n\\nEuskara\\n\\nفارسی\\n\\nFrançais\\n\\nGaeilge\\n\\nGalego\\n\\n한국어\\n\\nՀայերեն\\n\\nहिन्दी\\n\\nHrvatski\\n\\nBahasa Indonesia\\n\\nItaliano\\n\\nעברית\\n\\nಕನ್ನಡ\\n\\nКыргызча\\n\\nLatina\\n\\nLatviešu\\n\\nLietuvių\\n\\nMagyar\\n\\nМакедонски\\n\\nNederlands\\n\\n日本語\\n\\nNorsk bokmål\\n\\nOʻzbekcha / ўзбекча\\n\\nپښتو\\n\\nPolski\\n\\nPortuguês\\n\\nRomână\\n\\nРусский\\n\\nShqip\\n\\nSimple English\\n\\nSlovenčina\\n\\nSoomaaliga\\n\\nСрпски / srpski\\n\\nSrpskohrvatski / српскохрватски\\n\\nSuomi\\n\\nSvenska\\n\\nதமிழ்\\n\\nతెలుగు\\n\\nไทย\\n\\nTürkçe\\n\\nУкраїнська\\n\\nTiếng Việt\\n\\n吴语\\n\\n粵語\\n\\nŽemaitėška\\n\\n中文\\n\\nEdit links\\n\\nArticle\\n\\nTalk\\n\\nEnglish\\n\\nRead\\n\\nEdit\\n\\nView history\\n\\nTools\\n\\nTools\\n\\nActions\\n\\nRead\\n\\nEdit\\n\\nView history\\n\\nGeneral\\n\\nWhat links here\\n\\nRelated changes\\n\\nUpload file\\n\\nSpecial pages\\n\\nPermanent link\\n\\nPage information\\n\\nCite this page\\n\\nWikidata item\\n\\nPrint/export\\n\\nDownload as PDF\\n\\nPrintable version\\n\\nIn other projects\\n\\nWikimedia Commons\\n\\nWikiquote\\n\\nFrom Wikipedia, the free encyclopedia\\n\\nPsychological process of selectively eating and have discrete aspects of information\\n\\nThis article is about the psychological concept of attention. For other uses, see \\n\\nAttention (disambiguation).\\n\\nCognitive psychology\\n\\nPerception\\n\\nVisual perception\\nObject recognition\\nFace recognition\\nPattern recognition\\n\\nAttention\\n\\nMemory\\n\\nAging and memory\\nEmotional memory\\nLearning\\nLong-term memory\\n\\nMetacognition\\n\\nLanguage\\n\\nMetalanguage\\n\\nThinking\\n\\nCognition\\n\\nConcept\\nReasoning\\nDecision making\\nProblem solving\\n\\nNumerical cognition\\n\\nNumerosity adaptation effect\\nApproximate number system\\nParallel individuation system\\n\\n.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}vte\\n\\nawareness on some\\n\\nphenomenon to the exclusion of other stimuli.\\n\\n[1]\\n\\nsubjective or\\n\\nobjective.\\n\\nWilliam James (1890) wrote that \"Attention is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or\\n\\ntrains of thought. Focalization, concentration, of\\n\\nconsciousness are of its essence.\"\\n\\n[2]\\n\\nallocation of limited cognitive processing resources.\\n\\n[3]\\n\\nbottleneck, in terms of the amount of data the\\n\\nbrain can process each second; for example, in\\n\\nhuman vision, only less than 1% of the visual input data (at around one megabyte per second) can enter the bottleneck,\\n\\n[4]\\n\\n[5]\\n\\ninattentional blindness.\\n\\n[6]\\n\\neducation,\\n\\npsychology,\\n\\nneuroscience,\\n\\ncognitive neuroscience, and\\n\\nneuropsychology. Areas of active investigation involve determining the source of the\\n\\nsensory cues and signals that generate attention, the effects of these sensory cues and signals on the\\n\\ntuning properties of sensory\\n\\nneurons, and the relationship between attention and other behavioral and cognitive processes, which may include\\n\\nworking memory and\\n\\npsychological vigilance. A relatively new body of research, which expands upon earlier research within psychopathology, is investigating the diagnostic symptoms associated with\\n\\ntraumatic brain injury and its effects on attention. Attention also varies across cultures.\\n\\n[7]\\n\\nThe relationships between attention and consciousness are complex enough that they have warranted perennial philosophical exploration. Such exploration is both ancient and continually relevant, as it can have effects in fields ranging from mental health and the study of disorders of consciousness to artificial intelligence and its domains of research.\\n\\nContemporary definition and research[edit]\\n\\npsychology as a scientific discipline, attention was studied in the field of\\n\\nphilosophy. Thus, many of the discoveries in the field of attention were made by philosophers. Psychologist\\n\\nJohn B. Watson calls\\n\\nJuan Luis Vives the father of modern psychology because, in his book\\n\\n[8]\\n\\npositron emission tomography (PET) and later\\n\\nfunctional magnetic resonance imaging (fMRI) to image the brain while monitoring tasks involving attention. Considering this expensive equipment was generally only available in hospitals, psychologists sought cooperation with neurologists. Psychologist\\n\\nMichael Posner (then already renowned for his influential work on visual selective attention) and neurologist\\n\\nMarcus Raichle pioneered brain imaging studies of selective attention.\\n\\n[9]\\n\\nneuroscientists became interested in this type of research that combines sophisticated experimental paradigms from\\n\\ncognitive psychology with these new brain imaging techniques. Although the older technique of\\n\\nelectroencephalography (EEG) had long been used to study the brain activity underlying selective attention by\\n\\ncognitive psychophysiologists, the ability of the newer techniques to actually measure precisely localized activity inside the brain generated renewed interest by a wider community of researchers. A growing body of such\\n\\nneuroimaging research has identified a\\n\\nfrontoparietal attention network which appears to be responsible for control of attention.\\n\\n[10]\\n\\nSelective and visual[edit]\\n\\nSee also: \\n\\nSelective auditory attention\\n\\nIn cognitive psychology there are at least two models which describe how visual attention operates. These models may be considered metaphors which are used to describe internal processes and to generate hypotheses that are falsifiable. Generally speaking, visual attention is thought to operate as a two-stage process.[11] In the first stage, attention is distributed uniformly over the external visual scene and processing of information is performed in parallel. In the second stage, attention is concentrated to a specific area of the visual scene (i.e., it is focused), and processing is performed in a serial fashion.\\n\\nThe first of these models to appear in the literature is the spotlight model. The term \"spotlight\" was inspired by the work of William James, who described attention as having a focus, a margin, and a fringe.[12] The focus is an area that extracts information from the visual scene with a high-resolution, the geometric center of which being where visual attention is directed. Surrounding the focus is the fringe of attention, which extracts information in a much more crude fashion (i.e., low-resolution). This fringe extends out to a specified area, and the cut-off is called the margin.\\n\\n[13]\\n\\nzoom lens one might find on a camera, and any change in size can be described by a trade-off in the efficiency of processing.\\n\\n[14]\\n\\nvisual angle,\\n\\n[12]\\n\\n[15]\\n\\nA significant debate emerged in the last decade of the 20th century in which Treisman\\'s 1993 Feature Integration Theory (FIT) was compared to Duncan and Humphrey\\'s 1989 attentional engagement theory (AET).[16]:\\u200a5–7\\u200a FIT posits that \"objects are retrieved from scenes by means of selective spatial attention that picks out objects\\' features, forms feature maps, and integrates those features that are found at the same location into forming objects.\" Treismans\\'s theory is based on a two-stage process to help solve the binding problem of attention. These two stages are the preattentive stage and the focused attention stage.\\n\\nPreattentive Stage: The unconscious detection and separation of features of an item (color, shape, size). Treisman suggests that this happens early in cognitive\\xa0 processing and that individuals are not aware of the occurrence due to the counter intuitiveness of separating a whole into its part. Evidence shows that preattentive focuses are accurate due to illusory conjunctions.[17]\\n\\nFocused Attention Stage: The combining of all feature identifiers to perceive all parts as one whole. This is possible through prior knowledge and cognitive mapping. When an item is seen within a known location and has features that people have knowledge of, then prior knowledge will help bring features all together to make sense of what is perceived. The case of R.M\\'s damage to his parietal lobe, also known as Balint\\'s syndrome, shows the incorporation of focused attention and combination of features in the role of attention.[18]\\n\\nThrough sequencing these steps, parallel and serial search is better exhibited through the formation of conjunctions of objects. Conjunctive searches, according to Treismans, are done through both stages[19] in order to create selective and focused attention on an object, though Duncan and Humphrey would disagree. Duncan and Humphrey\\'s AET understanding of attention maintained that \"there is an initial pre-attentive parallel phase of perceptual segmentation and analysis that encompasses all of the visual items present in a scene. At this phase, descriptions of the objects in a visual scene are generated into structural units; the outcome of this parallel phase is a multiple-spatial-scale structured representation. Selective attention intervenes after this stage to select information that will be entered into visual short-term memory.\"[16]:\\u200a5–7\\u200a The contrast of the two theories placed a new emphasis on the separation of visual attention tasks alone and those mediated by supplementary cognitive processes. As Rastophopoulos summarizes the debate: \"Against Treisman\\'s FIT, which posits spatial attention as a necessary condition for detection of objects, Humphreys argues that visual elements are encoded and bound together in an initial parallel phase without focal attention, and that attention serves to select among the objects that result from this initial grouping.\"[16]:\\u200a8\\n\\nNeuropsychological model[edit]\\n\\nIn the twentieth century, the pioneering research of Lev Vygotsky and Alexander Luria led to the three-part model of neuropsychology defining the working brain as being represented by three co-active processes listed as Attention, Memory, and Activation. A.R. Luria published his well-known book The Working Brain in 1973 as a concise adjunct volume to his previous 1962 book Higher Cortical Functions in Man. In this volume, Luria summarized his three-part global theory of the working brain as being composed of three constantly co-active processes which he described as the; (1) Attention system, (2) Mnestic (memory) system, and (3) Cortical activation system. The two books together are considered by Homskaya\\'s account as \"among Luria\\'s major works in neuropsychology, most fully reflecting all the aspects (theoretical, clinical, experimental) of this new discipline.\"[20] The product of the combined research of Vygotsky and Luria have determined a large part of the contemporary understanding and definition of attention as it is understood at the start of the 21st-century.\\n\\nMultitasking and divided[edit]\\n\\nSee also: \\n\\nHuman multitasking and\\n\\nDistracted driving\\n\\nMultitasking can be defined as the attempt to perform two or more tasks simultaneously; however, research shows that when multitasking, people make more mistakes or perform their tasks more slowly.[21]  Attention must be divided among all of the component tasks to perform them.  In divided attention, individuals attend or give attention to multiple sources of information at once or perform more than one task at the same time.[22]\\n\\nOlder research involved looking at the limits of people performing simultaneous tasks like reading stories, while listening and writing something else,[23] or listening to two separate messages through different ears (i.e., dichotic listening).  Generally, classical research into attention investigated the ability of people to learn new information when there were multiple tasks to be performed, or to probe the limits of our perception (c.f. Donald Broadbent).  There is also older literature on people\\'s performance on multiple tasks performed simultaneously, such as driving a car while tuning a radio[24] or driving while being on the phone.[25]\\n\\nThe vast majority of current research on human multitasking is based on performance of doing two tasks simultaneously,[21] usually that involves driving while performing another task, such as texting, eating, or even speaking to passengers in the vehicle, or with a friend over a cellphone.  This research reveals that the human attentional system has limits for what it can process: driving performance is worse while engaged in other tasks; drivers make more mistakes, brake harder and later, get into more accidents, veer into other lanes, and/or are less aware of their surroundings when engaged in the previously discussed tasks.[26][27][28]\\n\\nThere has been little difference found between speaking on a hands-free cell phone or a hand-held cell phone,[6][29] which suggests that it is the strain of attentional system that causes problems, rather than what the driver is doing with his or her hands.  While speaking with a passenger is as cognitively demanding as speaking with a friend over the phone,[30] passengers are able to change the conversation based upon the needs of the driver.  For example, if traffic intensifies, a passenger may stop talking to allow the driver to navigate the increasingly difficult roadway; a conversation partner over a phone would not be aware of the change in environment.\\n\\nThere have been multiple theories regarding divided attention. One, conceived by Kahneman,[31] explains that there is a single pool of attentional resources that can be freely divided among multiple tasks. This model seems oversimplified, however, due to the different modalities (e.g., visual, auditory, verbal) that are perceived.[32] When the two simultaneous tasks use the same modality, such as listening to a radio station and writing a paper, it is much more difficult to concentrate on both because the tasks are likely to interfere with each other. The specific modality model was theorized by Navon and Gopher in 1979. However, more recent research using well controlled dual-task paradigms points at the importance of tasks.[33]\\n\\nAs an alternative, resource theory has been proposed as a more accurate metaphor for explaining divided attention on complex tasks. Resource theory states that as each complex task is automatized, performing that task requires less of the individual\\'s limited-capacity attentional resources.[32] Other variables play a part in our ability to pay attention to and concentrate on many tasks at once. These include, but are not limited to, anxiety, arousal, task difficulty, and skills.[32]\\n\\nSimultaneous[edit]\\n\\nSimultaneous attention is a type of attention, classified by attending to multiple events at the same time. Simultaneous attention is demonstrated by children in Indigenous communities, who learn through this type of attention to their surroundings.[34] Simultaneous attention is present in the ways in which children of indigenous backgrounds interact both with their surroundings and with other individuals. Simultaneous attention requires focus on multiple simultaneous activities or occurrences. This differs from multitasking, which is characterized by alternating attention and focus between multiple activities, or halting one activity before switching to the next.\\n\\nSimultaneous attention involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. Indigenous heritage toddlers and caregivers in San Pedro were observed to frequently coordinate their activities with other members of a group in ways parallel to a model of simultaneous attention, whereas middle-class European-descent families in the U.S. would move back and forth between events.[7][35] Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially wide, keen observers.[36] This points to a strong cultural difference in attention management.\\n\\nAlternative topics and discussions[edit]\\n\\nOvert and covert orienting[edit]\\n\\nAttention may be differentiated into \"overt\" versus \"covert\" orienting.[37]\\n\\nOvert orienting is the act of selectively attending to an item or location over others by moving the eyes to point in that direction.[38] Overt orienting can be directly observed in the form of eye movements. Although overt eye movements are quite common, there is a distinction that can be made between two types of eye movements; reflexive and controlled. Reflexive movements are commanded by the superior colliculus of the midbrain. These movements are fast and are activated by the sudden appearance of stimuli. In contrast, controlled eye movements are commanded by areas in the frontal lobe. These movements are slow and voluntary.\\n\\n[12]\\n\\n[38]\\n\\n[39]\\n\\n[40]\\n\\nsaccade to that location.\\n\\n[41]\\n\\nThere are studies that suggest the mechanisms of overt and covert orienting may not be controlled separately and independently as previously believed. Central mechanisms that may control covert orienting, such as the parietal lobe, also receive input from subcortical centres involved in overt orienting.[38] In support of this, general theories of attention actively assume bottom-up (reflexive) processes and top-down (voluntary) processes converge on a common neural architecture, in that they control both covert and overt attentional systems.[42]  For example, if individuals attend to the right hand corner field of view, movement of the eyes in that direction may have to be actively suppressed.\\n\\nExogenous and endogenous orienting[edit]\\n\\nOrienting attention is vital and can be controlled through external (exogenous) or internal (endogenous) processes.  However, comparing these two processes is challenging because external signals do not operate completely exogenously, but will only summon attention and eye movements if they are important to the subject.[38]\\n\\nGreek\\n\\n[43]\\n\\n[44]\\n\\n[38]\\n\\n[45]\\n\\n[46]\\n\\n[47]\\n\\n[48]\\n\\ninhibition of return.\\n\\nEndogenous (from Greek endo, meaning \"within\" or \"internally\") orienting is the intentional allocation of attentional resources to a predetermined location or space. Simply stated, endogenous orienting occurs when attention is oriented according to an observer\\'s goals or desires, allowing the focus of attention to be manipulated by the demands of a task. In order to have an effect, endogenous cues must be processed by the observer and acted upon purposefully. These cues are frequently referred to as central cues. This is because they are typically presented at the center of a display, where an observer\\'s eyes are likely to be fixated. Central cues, such as an arrow or digit presented at fixation, tell observers to attend to a specific location.[49]\\n\\nWhen examining differences between exogenous and endogenous orienting, some researchers suggest that there are four differences between the two kinds of cues:\\n\\nexogenous orienting is less affected by cognitive load than endogenous orienting;\\n\\nobservers are able to ignore endogenous cues but not exogenous cues;\\n\\nexogenous cues have bigger effects than endogenous cues; and\\n\\nexpectancies about cue validity and predictive value affects endogenous orienting more than exogenous orienting.[50]\\n\\n[51]\\n\\nexogenous attention. These describe attentional processing which is driven by the properties of the objects themselves. Some processes, such as motion or a sudden loud noise, can attract our attention in a pre-conscious, or non-volitional way. We attend to them whether we want to or not.\\n\\n[52]\\n\\nparietal and\\n\\ntemporal cortices, as well as the\\n\\nbrainstem.\\n\\n[53]\\n\\n[54]\\n\\n[55]\\n\\n[56]\\n\\nprimary visual cortex creates a bottom-up saliency map,\\n\\n[57]\\n\\n[4]\\n\\nsuperior colliculus in the\\n\\nmidbrain area to  guide attention or gaze shifts.\\n\\nendogenous attention,\\n\\nattentional control or\\n\\nexecutive attention. This aspect of our attentional orienting is under the control of the person who is attending. It is mediated primarily by the\\n\\nfrontal cortex and\\n\\nbasal ganglia\\n\\n[53]\\n\\n[58]\\n\\nexecutive functions.\\n\\n[38]\\n\\n[53]\\n\\nworking memory,\\n\\n[59]\\n\\n[60]\\n\\nInfluence of processing load[edit]\\n\\nA \"hugely influential\"[61] theory regarding selective attention is the perceptual load theory, which states that there are two mechanisms that affect attention: cognitive and perceptual. The perceptual considers the subject\\'s ability to perceive or ignore stimuli, both task-related and non task-related. Studies show that if there are many stimuli present (especially if they are task-related), it is much easier to ignore the non-task related stimuli, but if there are few stimuli the mind will perceive the irrelevant stimuli as well as the relevant. The cognitive refers to the actual processing of the stimuli. Studies regarding this showed that the ability to process stimuli decreased with age, meaning that younger people were able to perceive more stimuli and fully process them, but were likely to process both relevant and irrelevant information, while older people could process fewer stimuli, but usually processed only relevant information.[62]\\n\\nSome people can process multiple stimuli, e.g. trained Morse code operators have been able to copy 100% of a message while carrying on a meaningful conversation.  This relies on the reflexive response due to \"overlearning\" the skill of morse code reception/detection/transcription so that it is an autonomous function requiring no specific attention to perform. This overtraining of the brain comes as the \"practice of a skill [surpasses] 100% accuracy,\" allowing the activity to become autonomic, while your mind has room to process other actions simultaneously.[63]\\n\\nBased on the primary role of the perceptual load theory, assumptions regarding its functionality surrounding that attentional resources are that of limited capacity which signify the need for all of the attentional resources to be used.[64] This performance, however, is halted when put hand in hand with accuracy and reaction time (RT). This limitation arises through the measurement of literature when obtaining outcomes for scores. This affects both cognitive and perceptual attention because there is a lack of measurement surrounding distributions of temporal and spatial attention. Only a concentrated amount of attention on how effective one is completing the task and how long they take is being analyzed making a more redundant analysis on overall cognition of being able to process multiple stimuli through perception.[65]\\n\\nClinical model[edit]\\n\\nAttention is best described as the sustained focus of cognitive resources on information while filtering or ignoring extraneous information. Attention is a very basic function that often is a precursor to all other neurological/cognitive functions. As is frequently the case, clinical models of attention differ from investigation models. One of the most used models for the evaluation of attention in patients with very different neurologic pathologies is the model of Sohlberg and Mateer.[66] This hierarchic model is based in the recovering of attention processes of brain damage patients after coma. Five different kinds of activities of growing difficulty are described in the model; connecting with the activities those patients could do as their recovering process advanced.\\n\\nFocused attention: The ability to respond discretely to specific sensory stimuli.\\n\\nSustained attention (vigilance and concentration): The ability to maintain a consistent behavioral response during continuous and repetitive activity.\\n\\nSelective attention: The ability to maintain a behavioral or cognitive set in the face of distracting or competing stimuli. Therefore, it incorporates the notion of \"freedom from distractibility.\"\\n\\nAlternating attention: The ability of mental flexibility that allows individuals to shift their focus of attention and move between tasks having different cognitive requirements.\\n\\nDivided attention: This refers to the ability to respond simultaneously to multiple tasks or multiple task demands.\\n\\nThis model has been shown to be very useful in evaluating attention in very different pathologies, correlates strongly with daily difficulties and is especially helpful in designing stimulation programs such as attention process training, a rehabilitation program for neurological patients of the same authors.\\n\\nOther descriptors for types of attention[edit]\\n\\nMindfulness:  Mindfulness has been conceptualized as a clinical model of attention.[67]  Mindfulness practices are clinical interventions that emphasize training attention functions.[68]\\n\\nVigilant attention: Remaining focused on a non-arousing stimulus or uninteresting task for a sustained period is far more difficult than attending to arousing stimuli and interesting tasks, and requires a specific type of attention called \\'vigilant attention\\'.[69] Thereby, vigilant attention is the ability to give sustained attention to a  stimulus or  task that might ordinarily be insufficiently engaging to prevent our attention being distracted by other stimuli or tasks.[70]\\n\\nNeural correlates[edit]\\n\\nMost experiments show that one neural correlate of attention is enhanced firing. If a neuron has a certain response to a stimulus when the animal is not attending to the stimulus, then when the animal does attend to the stimulus, the neuron\\'s response will be enhanced even if the physical characteristics of the stimulus remain the same.\\n\\nIn a 2007 review, Knudsen[71] describes a more general model which identifies four core processes of attention, with working memory at the center:\\n\\nWorking memory temporarily stores information for detailed analysis.\\n\\nCompetitive selection is the process that determines which information gains access to working memory.\\n\\nThrough top-down sensitivity control, higher cognitive processes can regulate signal intensity in information channels that compete for access to working memory, and thus give them an advantage in the process of competitive selection. Through top-down sensitivity control, the momentary content of working memory can influence the selection of new information, and thus mediate voluntary control of attention in a recurrent loop (endogenous attention).[72]\\n\\nBottom-up saliency filters automatically enhance the response to infrequent stimuli, or stimuli of instinctive or learned biological relevance (exogenous attention).[72]\\n\\nNeurally, at different hierarchical levels spatial maps can enhance or inhibit activity in sensory areas, and induce orienting behaviors like eye movement.\\n\\nAt the top of the hierarchy, the frontal eye fields (FEF) and the dorsolateral prefrontal cortex contain a retinocentric spatial map. Microstimulation in the FEF induces monkeys to make a saccade to the relevant location. Stimulation at levels too low to induce a saccade will nonetheless enhance cortical responses to stimuli located in the relevant area.\\n\\nAt the next lower level, a variety of spatial maps are found in the parietal cortex. In particular, the lateral intraparietal area (LIP) contains a saliency map and is interconnected both with the FEF and with sensory areas.\\n\\nExogenous attentional guidance in humans and monkeys is by a bottom-up saliency map in the primary visual cortex.[57][4]  In lower vertebrates, this saliency map is more likely in the superior colliculus (optic tectum).[73]\\n\\nCertain automatic responses that influence attention, like orienting to a highly salient stimulus, are mediated subcortically by the superior colliculi.\\n\\nAt the neural network level, it is thought that processes like lateral inhibition mediate the process of competitive selection.\\n\\nEEG. Many animals, including humans, produce\\n\\ngamma waves (40–60\\xa0Hz) when focusing attention on a particular object or activity.\\n\\n[74]\\n\\n[75]\\n\\n[40]\\n\\n[76]\\n\\nMichael Posner. He divides attention into three functional components: alerting, orienting, and\\n\\nexecutive attention\\n\\n[53]\\n\\n[77]\\n\\n[78]\\n\\n[79]\\n\\n[80]\\n\\nAlerting is the process involved in becoming and staying attentive toward the surroundings. It appears to exist in the frontal and parietal lobes of the right hemisphere, and is modulated by norepinephrine.[81][82]\\n\\nOrienting is the directing of attention to a specific stimulus.\\n\\nExecutive attention is used when there is a conflict between multiple attention cues. It is essentially the same as the central executive in Baddeley\\'s model of working memory. The Eriksen flanker task has shown that the executive control of attention may take place in the anterior cingulate cortex[83]\\n\\nCultural variation[edit]\\n\\nChildren appear to develop patterns of attention related to the cultural practices of their families, communities, and the institutions in which they participate.[84]\\n\\nIn 1955, Jules Henry suggested that there are societal differences in sensitivity to signals from many ongoing sources that call for the awareness of several levels of attention simultaneously. He tied his speculation to ethnographic observations of communities in which children are involved in a complex social community with multiple relationships.[7]\\n\\nMany Indigenous children in the Americas predominantly learn by observing and pitching in. There are several studies to support that the use of keen attention towards learning is much more common in Indigenous Communities of North and Central America than in a middle-class European-American setting.[85] This is a direct result of the Learning by Observing and Pitching In model.\\n\\nKeen attention is both a requirement and result of learning by observing and pitching-in. Incorporating the children in the community gives them the opportunity to keenly observe and contribute to activities that were not directed towards them. It can be seen from different Indigenous communities and cultures, such as the Mayans of San Pedro, that children can simultaneously attend to multiple events.[7] Most Maya children have learned to pay attention to several events at once in order to make useful observations.[86]\\n\\nOne example is simultaneous attention which involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. San Pedro toddlers and caregivers frequently coordinated their activities with other members of a group in multiway engagements rather than in a dyadic fashion.[7][35] Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially keen observers.[36]\\n\\nThis learning by observing and pitching-in model requires active levels of attention management. The child is present while caretakers engage in daily activities and responsibilities such as: weaving, farming, and other skills necessary for survival. Being present allows the child to focus their attention on the actions being performed by their parents, elders, and/or older siblings.[85] In order to learn in this way, keen attention and focus is required. Eventually the child is expected to be able to perform these skills themselves.\\n\\nModelling[edit]\\n\\ncomputer vision, efforts have been made to model the mechanism of human attention, especially the bottom-up intentional mechanism\\n\\n[87]\\n\\n[88]\\n\\n[89]\\n\\nspatial attention and\\n\\ntemporal attention have been incorporated in such classification efforts.\\n\\n[90]\\n\\n[87]\\n\\n[91]\\n\\n[87]\\n\\n[92]\\n\\n[93]\\n\\nHemispatial neglect[edit]\\n\\nMain article: \\n\\nHemispatial neglect\\n\\nHemispatial neglect, also called unilateral neglect, often occurs when people have damage to their right hemisphere.[94] This damage often leads to a tendency to ignore the left side of one\\'s body or even the left side of an object that can be seen. Damage to the left side of the brain (the left hemisphere) rarely yields significant neglect of the right side of the body or object in the person\\'s local environments.[95]\\n\\nThe effects of spatial neglect, however, may vary and differ depending on what area of the brain was damaged. Damage to different neural substrates can result in different types of neglect. Attention disorders (lateralized and nonlaterized) may also contribute to the symptoms and effects.[95] Much research has asserted that damage to gray matter within the brain results in spatial neglect.[96]\\n\\nNew technology has yielded more information, such that there is a large, distributed network of frontal, parietal, temporal, and subcortical brain areas that have been tied to neglect.[97] This network can be related to other research as well; the dorsal attention network is tied to spatial orienting.[98] The effect of damage to this network may result in patients neglecting their left side when distracted about their right side or an object on their right side.[94]\\n\\nAttention in social contexts[edit]\\n\\n[99]\\n\\n[100]\\n\\n[101]\\n\\n[101]\\n\\nautism spectrum disorders and\\n\\nWilliams syndrome.\\n\\nDistracting factors[edit]\\n\\nAccording to Daniel Goleman\\'s book, Focus: The Hidden Driver of Excellence, there are two types of distracting factors affecting focus – sensory and emotional.\\n\\nA sensory distracting factor would be, for example, while a person is reading this article, they are neglecting the white field surrounding the text.\\n\\nAn emotional distracting factor would be when someone is focused on answering an email, and somebody shouts their name. It would be almost impossible to neglect the voice speaking it. Attention is immediately directed toward the source. Positive emotions have also been found to affect attention. Induction of happiness has led to increased response times and an increase in inaccurate responses in the face of irrelevant stimuli. Two possible theories as to why emotions might make one more susceptible to distracting stimuli is that emotions take up too much of one\\'s cognitive resources and make it harder to control your focus of attention. The other theory is that emotions make it harder to filter out distractions, specifically with positive emotions due to a feeling of security.[102]\\n\\nAnother distracting factor to attention processes is insufficient sleep. Sleep deprivation is found to impair cognition, specifically performance in divided attention. Divided attention is possibly linked with the circadian processes.[103]\\n\\nFailure to attend[edit]\\n\\nInattentional blindness was first introduced in 1998 by Arien Mack and Irvic Rock. Their studies show that when people are focused on specific stimuli, they often miss other stimuli that are clearly present. Though actual blindness is not occurring here, the blindness that happens is due to the perceptual load of what is being attended to.[104] Based on the experiment performed by Mack and Rock, Ula Finch and Nilli Lavie tested participants with a perceptual task. They presented subjects with a cross, one arm being longer than the other, for 5 trials. On the sixth trial, a white square was added to the top left of the screen. The results conclude that out of 10 participants, only 2 (20%) actually saw the square. This would suggest that when a higher focus was attended to the length of the crossed arms, the more likely someone would altogether miss an object that was in plain sight.[105]\\n\\nChange blindness was first tested by Rensink and coworkers in 1997. Their studies show that people have difficulty detecting changes from scene to scene due to the intense focus on one thing, or lack of attention overall. This was tested by Rensink through a presentation of a picture, and then a blank field, and then the same picture but with an item missing. The results showed that the pictures had to be alternated back and forth a good number of times for participants to notice the difference. This idea is greatly portrayed in films that have continuity errors. Many people do not pick up on differences when in reality, the changes tend to be significant.[106]\\n\\nHistory of the study[edit]\\n\\nPhilosophical period[edit]\\n\\nPsychologist Daniel E. Berlyne credits the first extended treatment of attention to philosopher Nicolas Malebranche in his work \"The Search After Truth\". \"Malebranche held that we have access to ideas, or mental representations  of the external world, but not direct access to the world itself.\"[8] Thus in order to keep these ideas organized, attention is necessary.[107] Otherwise we will confuse these ideas. Malebranche writes in \"The Search After Truth\", \"because it often happens that the understanding has only confused and imperfect perceptions of things, it is truly a cause of our errors.... It is therefore necessary to look for means to keep our perceptions from being confused and imperfect.  And, because, as everyone knows, there is nothing that makes them clearer and more distinct than attentiveness, we must try to find the means to become more attentive than we are\".[108] According to Malebranche, attention is crucial to understanding and keeping thoughts organized.\\n\\nPhilosopher Gottfried Wilhelm Leibniz introduced the concept of apperception to this philosophical approach to attention.  Apperception refers to \"the process by which new experience is assimilated to and transformed by the residuum of past experience of an individual to form a new whole.\"[109] Apperception is required for a perceived event to become a conscious event. Leibniz emphasized a reflexive involuntary view of attention known as exogenous orienting. However, there is also endogenous orienting which is voluntary and directed attention. Philosopher Johann Friedrich Herbart agreed with Leibniz\\'s view of apperception; however, he expounded on it in by saying that new experiences had to be tied to ones already existing in the mind. Herbart was also the first person to stress the importance of applying mathematical modeling to the study of psychology.[8]\\n\\nThroughout the philosophical era, various thinkers made significant contributions to the field of attention studies, beginning with research on the extent of attention and how attention is directed. In the beginning of the 19th century, it was thought that people were not able to attend to more than one stimulus at a time. However, with research contributions by Sir William Hamilton, 9th Baronet this view was changed.  Hamilton proposed a view of attention that likened its capacity to holding marbles. You can only hold a certain number of marbles at a time before it starts to spill over. His view states that we can attend to more than one stimulus at once. William Stanley Jevons later expanded this view and stated that we can attend to up to four items at a time.[110]\\n\\n1860–1909[edit]\\n\\nThis period of attention research took the focus from conceptual findings to experimental testing. It also involved psychophysical methods that allowed measurement of the relation between physical stimulus properties and the psychological perceptions of them. This period covers the development of attentional research from the founding of psychology to 1909.\\n\\nWilhelm Wundt introduced the study of attention to the field of psychology. Wundt measured mental processing speed by likening it to differences in stargazing measurements. Astronomers in this time would measure the time it took for stars to travel.  Among these measurements when astronomers recorded the times, there were personal differences in calculation. These different readings resulted in different reports from each astronomer. To correct for this, a personal equation was developed. Wundt applied this to mental processing speed. Wundt realized that the time it takes to see the stimulus of the star and write down the time was being called an \"observation error\" but actually was the time it takes to switch voluntarily one\\'s attention from one stimulus to another. Wundt called his school of psychology voluntarism. It was his belief that psychological processes can only be understood in terms of goals and consequences.\\n\\nFranciscus Donders used mental chronometry to study attention and it was considered a major field of intellectual inquiry by authors such as Sigmund Freud. Donders and his students conducted the first detailed investigations of the speed of mental processes. Donders measured the time required to identify a stimulus and to select a motor response. This was the time difference between stimulus discrimination and response initiation.  Donders also formalized the subtractive method which states that the time for a particular process can be estimated by adding that process to a task and taking the difference in reaction time between the two tasks. He also differentiated between three types of reactions: simple reaction, choice reaction, and go/no-go reaction.\\n\\nHermann von Helmholtz also contributed to the field of attention relating to the extent of attention. Von Helmholtz stated that it is possible to focus on one stimulus and still perceive or ignore others.  An example of this is being able to focus on the letter u in the word house and still perceiving the letters h, o, s, and e.\\n\\nOne major debate in this period was whether it was possible to attend to two things at once (split attention). Walter Benjamin described this experience as \"reception in a state of distraction.\" This disagreement could only be resolved through experimentation.\\n\\nIn 1890, William James, in his textbook The Principles of Psychology, remarked:\\n\\nEveryone knows what attention is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration, of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others, and is a condition which has a real opposite in the confused, dazed, scatterbrained state which in French is called distraction, and Zerstreutheit in German.[111]\\n\\nJames differentiated between sensorial attention and intellectual attention. Sensorial attention is when attention is directed to objects of sense, stimuli that are physically present. Intellectual attention is attention directed to ideal or represented objects; stimuli that are not physically present. James also distinguished between immediate or derived attention: attention to the present versus to something not physically present. According to James, attention has five major effects. Attention works to make us perceive, conceive, distinguish, remember, and shorten reactions time.\\n\\n1910–1949[edit]\\n\\nDuring this period, research in attention waned and interest in behaviorism flourished, leading some to believe, like Ulric Neisser, that in this period, \"There was no research on attention\". However, Jersild published very important work on \"Mental Set and Shift\" in 1927. He stated, \"The fact of mental set is primary in all conscious activity.  The same stimulus may evoke any one of a large number of responses depending upon the contextual setting in which it is placed\".[112] This research found that the time to complete a list was longer for mixed lists than for pure lists. For example, if a list was names of animals versus a list of the same size with names of animals, books, makes and models of cars, and types of fruits, it takes longer to process the second list. This is task switching.\\n\\nIn 1931, Telford discovered the psychological refractory period.  The stimulation of neurons is followed by a refractory phase during which neurons are less sensitive to stimulation. In 1935 John Ridley Stroop developed the Stroop Task which elicited the Stroop Effect. Stroop\\'s task showed that irrelevant stimulus information can have a major impact on performance.  In this task, subjects were to look at a list of colors. This list of colors had each color typed in a color different from the actual text. For example, the word Blue would be typed in Orange, Pink in Black, and so on.\\n\\nExample:  Blue Purple Red Green Purple Green\\n\\nSubjects were then instructed to say the name of the ink color and ignore the text. It took 110 seconds to complete a list of this type compared to 63 seconds to name the colors when presented in the form of solid squares.[8] The naming time nearly doubled in the presence  of conflicting color words, an effect known as the Stroop Effect.\\n\\n1950–1974[edit]\\n\\nIn the 1950s, research psychologists renewed their interest in attention when the dominant epistemology shifted from positivism (i.e., behaviorism) to realism during what has come to be known as the \"cognitive revolution\".[113] The cognitive revolution admitted unobservable cognitive processes like attention as legitimate objects of scientific study.\\n\\ncocktail party problem\" by\\n\\nColin Cherry in 1953. At a cocktail party how do people select the conversation that they are listening to and ignore the rest? This problem is at times called \"focused attention\", as opposed to \"divided attention\". Cherry performed a number of experiments which became known as\\n\\ndichotic listening and were extended by\\n\\nDonald Broadbent and others.\\n\\n[114]\\n\\n:\\u200a112\\n\\nheadphones to listen to two streams of words in different\\n\\nears and selectively attend to one stream. After the task, the experimenter would question the subjects about the content of the unattended stream.\\n\\nBroadbent\\'s Filter Model of Attention states that information is held in a pre-attentive temporary store, and only sensory events that have some physical feature in common are selected to pass into the limited capacity processing system.  This implies that the meaning of unattended messages is not identified. Also, a significant amount of time is required to shift the filter from one channel to another. Experiments by Gray and Wedderburn and later Anne Treisman pointed out various problems in Broadbent\\'s early model and eventually led to the Deutsch–Norman model in 1968. In this model, no signal is filtered out, but all are processed to the point of activating their stored representations in memory. The point at which attention becomes \"selective\" is when one of the memory representations is selected for further processing. At any time, only one can be selected, resulting in the attentional bottleneck.[114]:\\u200a115–116\\n\\nDonald Broadbent), attention shuts down (in\\n\\nBroadbent\\'s model) or attenuates (in\\n\\nTriesman\\'s refinement) processing in the unattended ear before the mind can analyze its semantic content. In the late selection models (first proposed by J. Anthony Deutsch and\\n\\nDiana Deutsch), the content in both ears is analyzed semantically, but the words in the unattended ear cannot access consciousness.\\n\\n[115]\\n\\nperceptual load theory, however, \"provided elegant solution to\" what had once been a \"heated debate\".\\n\\n[116]\\n\\nSee also[edit]\\n\\nAlertness\\n\\nAttention deficit hyperactivity disorder\\n\\nAttention restoration theory\\n\\nAttention seeking\\n\\nAttention span\\n\\nAttention theft\\n\\nAttentional control\\n\\nAttentional shift\\n\\nBinding problem\\n\\nCognitive inhibition\\n\\nConsciousness\\n\\nCrossmodal attention\\n\\nFlow (psychology)\\n\\nFocusing (psychotherapy)\\n\\nInformal learning\\n\\nJoint attention\\n\\nImmanuel Kant\\n\\nMeditation\\n\\nMindfulness\\n\\nMotivation\\n\\nNonverbal communication\\n\\nObservational Learning\\n\\nOvsiankina effect\\n\\nPerceptual learning#The role of attention\\n\\nPhilosophy\\n\\nSalience (also called saliency)\\n\\nSelf\\n\\nSplit attention effect\\n\\nVigilance\\n\\nVisual search\\n\\nVisual spatial attention\\n\\nVisual temporal attention\\n\\nWorking memory\\n\\nReferences[edit]\\n\\n^ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}\"Attention | Definition, Theories, Aspects, & Facts | Britannica\". www.britannica.com.\\n\\n^ James W (1890). The Principles of Psychology. Vol.\\xa01. New York: Henry Holt. pp.\\xa0403–404.\\n\\n^ Anderson JR (2004). Cognitive Psychology and Its Implications (6th\\xa0ed.). Worth Publishers. p.\\xa0519. ISBN\\xa0978-0-7167-0110-1.\\n\\n^ a b c Zhaoping L (2014). Understanding vision: theory, models, and data. United Kingdom: Oxford University Press. ISBN\\xa09780199564668.\\n\\n^ Goldstein EB (2011). Cognitive Psychology: connecting mind, research, and everyday experience. Cengage Learning. ISBN\\xa0978-1-285-76388-0.\\n\\n^ a b Chabris CF, Simons DJ (2010). The Invisible Gorilla and Other Ways Our Intuitions Deceive Us. New York: Crown.\\n\\n^ a b c d e Chavajay P, Rogoff B (July 1999). \"Cultural variation in management of attention by children and their caregivers\". Developmental Psychology. 35 (4): 1079–90. doi:10.1037/0012-1649.35.4.1079. PMID\\xa010442876.\\n\\n^ a b c d Johnson A (2004). Attention: Theory and Practice. Thousand Oaks, CA: SAGE Publications. pp.\\xa01–24. ISBN\\xa0978-0-7619-2760-0.\\n\\n^ Raichle M (1999). \"Positron Emission Tomography\". The MIT Encyclopedia of the Cognitive Sciences. MIT Press. Archived from the original on January 17, 2015. Retrieved June 10, 2018.\\n\\n^ Scolari M, Seidl-Rathkopf KN, Kastner S (February 2015). \"Functions of the human frontoparietal attention network: Evidence from neuroimaging\". Current Opinion in Behavioral Sciences. 1: 32–39. doi:10.1016/j.cobeha.2014.08.003. PMC\\xa04936532. PMID\\xa027398396.\\n\\n^ Jonides J (1983). \"Further towards a model of the mind\\'s eye\\'s movement\" (PDF). Bulletin of the Psychonomic Society. 21 (4): 247–50. doi:10.3758/bf03334699.\\n\\n^ a b c Eriksen CW, Hoffman JE (1972). \"Temporal and spatial characteristics of selective encoding from visual displays\". Perception & Psychophysics. 12 (2B): 201–4. doi:10.3758/BF03212870.\\n\\n^ Eriksen CW, St James JD (October 1986). \"Visual attention within and around the field of focal attention: a zoom lens model\". Perception & Psychophysics. 40 (4): 225–40. doi:10.3758/BF03211502. PMID\\xa03786090.\\n\\n^ Castiello U, Umiltà C (April 1990). \"Size of the attentional focus and efficiency of processing\". Acta Psychologica. 73 (3): 195–209. doi:10.1016/0001-6918(90)90022-8. PMID\\xa02353586.\\n\\n^ Eriksen CW, Hoffman JE (1973). \"The extent of processing of noise elements during selective encoding from visual displays\". Perception & Psychophysics. 14 (1): 155–160. doi:10.3758/BF03198630.\\n\\n^ a b c Raftopoulos A (2007). \"Visual Processing and Attention\". Cognition and Perception. Oxford University Press.\\n\\n^ Treisman A, Vieira A, Hayes A (1992). \"Automaticity and Preattentive Processing\". The American Journal of Psychology. 105 (2): 341–362. doi:10.2307/1423032. ISSN\\xa00002-9556. JSTOR\\xa01423032. PMID\\xa01621885.\\n\\n^ Cohen A, Rafal RD (1991). \"Attention and Feature Integration: Illusory Conjunctions in a Patient with a Parietal Lobe Lesion\". Psychological Science. 2 (2): 106–110. doi:10.1111/j.1467-9280.1991.tb00109.x. ISSN\\xa00956-7976. JSTOR\\xa040062648. S2CID\\xa0145171384.\\n\\n^ Treisman, A. (2003), \"Feature Binding, Attention, and Object Perception\", Essential Sources in the Scientific Study of Consciousness, The MIT Press, vol.\\xa0353, no.\\xa01373, pp.\\xa01295–1306, doi:10.7551/mitpress/2834.003.0008, ISBN\\xa0978-0-262-26750-2, PMC\\xa01692340, PMID\\xa09770223\\n\\n^ Homskaya ED (2001). Alexander Romanovich Luria, A Scientific Biography. Plenum Series in Russian Neuropsychology. Translated by Krotova D. Plenum Press. pp.\\xa070–71. doi:10.1007/978-1-4615-1207-3. ISBN\\xa0978-1-4613-5441-3.\\n\\n^ a b Matlin MW (2013). Cognition (Textbook) (8th\\xa0ed.). Wiley. ISBN\\xa0978-1-118-14896-9.\\n\\n^ Gopher D, Iani C (2002). \"Attention\".  In Nadel L (ed.). Encyclopedia of Cognitive Science. London: Nature Publishing Company. ISBN\\xa0978-0-333-79261-2. Retrieved 27 January 2017.\\n\\n^ Spelke E, Hirst W, Neisser U (1976). \"Skills of divided attention\" (PDF). Cognition. 4 (3): 215–230. doi:10.1016/0010-0277(76)90018-4. S2CID\\xa019019411.\\n\\n^ Brown ID (October 1965). \"Effect of a car radio on driving in traffic\". Ergonomics. 8 (4): 475–9. doi:10.1080/00140136508930828. PMID\\xa05854152.\\n\\n^ Brown ID, Tickner AH, Simmonds DC (October 1969). \"Interference between concurrent tasks of driving and telephoning\". The Journal of Applied Psychology. 53 (5): 419–24. doi:10.1037/h0028103. PMID\\xa05366314.\\n\\n^ Strayer DL, Drews FA (2007). \"Multitasking in the automobile\".  In Kramer AF, Wiegmann DA, Kirlik A (eds.). Attention: From Theory to Practice. New York: Oxford University Press. pp.\\xa0121–33. ISBN\\xa0978-0-19-530572-2.\\n\\n^ Salvucci DD, Taatgen NA (January 2008). \"Threaded cognition: an integrated theory of concurrent multitasking\". Psychological Review. 115 (1): 101–30. CiteSeerX\\xa010.1.1.140.3655. doi:10.1037/0033-295x.115.1.101. PMID\\xa018211187.\\n\\n^ Collet C, Clarion A, Morel M, Chapon A, Petit C (November 2009). \"Physiological and behavioural changes associated to the management of secondary tasks while driving\". Applied Ergonomics. 40 (6): 1041–6. doi:10.1016/j.apergo.2009.01.007. PMID\\xa019249012.\\n\\n^ Folk CL (2010). \"Attention: Divided\".  In Goldstein EB (ed.). Encyclopedia of Perception. Thousand Oaks, CA: Sage. pp.\\xa084–7. ISBN\\xa09781412940818.\\n\\n^ Strayer DL, Cooper JM, Turrill J, Coleman J, Medeiros-Ward N, Biondi F (June 2013). \"Measuring Cognitive Distraction in the Automobile\" (PDF) (Research Report). AAA. Archived from the original (PDF) on 2013-10-28. Retrieved 2013-11-06.\\n\\n^ Kahneman D (1973). Attention and Effort (PDF). Englewood Cliffs, NJ: Prentice-Hall.\\n\\n^ a b c Sternberg RJ, Sternberg K (2012). Cognitive Psychology (Textbook). Cengage Learning. ISBN\\xa0978-1133313915.\\n\\n^ Wahn B, König P (2017). \"Is Attentional Resource Allocation Across Sensory Modalities Task-Dependent?\". Advances in Cognitive Psychology. 13 (1): 83–96. doi:10.5709/acp-0209-2. PMC\\xa05405449. PMID\\xa028450975.\\n\\n^ Correa-Chávez M, Roberts AL, Pérez MM (2011). Cultural patterns in children\\'s learning through keen observation and participation in their communities. Advances in Child Development and Behavior. Vol.\\xa040. pp.\\xa0209–41. doi:10.1016/b978-0-12-386491-8.00006-2. ISBN\\xa09780123864918. PMID\\xa021887963.\\n\\n^ a b Morelli GA, Rogoff B, Angelillo C (2003). \"Cultural variation in young children\\'s access to work or involvement in specialized child-focused activities\" (PDF). International Journal of Behavioral Development. 27 (3): 264–74. doi:10.1080/01650250244000335. S2CID\\xa0145563973.\\n\\n^ a b Silva KG, Correa-Chávez M, Rogoff B (2010). \"Mexican-heritage children\\'s attention and learning from interactions directed to others\". Child Development. 81 (3): 898–912. doi:10.1111/j.1467-8624.2010.01441.x. PMID\\xa020573112.\\n\\n^ Wright RD, Ward LM (2008). Orienting of Attention. Oxford University Press.\\n\\n^ a b c d e f Posner MI (February 1980). \"Orienting of attention\" (PDF). The Quarterly Journal of Experimental Psychology. 32 (1): 3–25. doi:10.1080/00335558008248231. PMID\\xa07367577. S2CID\\xa02842391.\\n\\n^ Eriksen CW, Colegate RL (1971). \"Selective attention and serial processing in briefly presented visual displays\". Perception & Psychophysics. 10 (5): 321–326. doi:10.3758/BF03207451.\\n\\n^ a b Gregoriou GG, Gotts SJ, Zhou H, Desimone R (May 2009). \"High-frequency, long-range coupling between prefrontal and visual cortex during attention\". Science. 324 (5931): 1207–10. Bibcode:2009Sci...324.1207G. doi:10.1126/science.1171402. PMC\\xa02849291. PMID\\xa019478185.\\n\\n^ Carrasco M, McElree B (April 2001). \"Covert attention accelerates the rate of visual information processing\". Proceedings of the National Academy of Sciences of the United States of America. 98 (9): 5363–7. Bibcode:2001PNAS...98.5363C. doi:10.1073/pnas.081074098. PMC\\xa033215. PMID\\xa011309485.\\n\\n^ Hunt AR, Kingstone A (December 2003). \"Covert and overt voluntary attention: linked or independent?\". Brain Research. Cognitive Brain Research. 18 (1): 102–5. doi:10.1016/j.cogbrainres.2003.08.006. PMID\\xa014659502.\\n\\n^ Mayer AR, Dorflinger JM, Rao SM, Seidenberg M (October 2004). \"Neural networks underlying endogenous and exogenous visual-spatial orienting\". NeuroImage. 23 (2): 534–41. doi:10.1016/j.neuroimage.2004.06.027. PMID\\xa015488402. S2CID\\xa042196703.\\n\\n^ Friesen CK, Kingstone A (1998). \"The eyes have it! Reflexive orienting is triggered by nonpredictive gaze\" (PDF). Psychonomic Bulletin & Review. 5 (3): 490–495. doi:10.3758/BF03208827. S2CID\\xa0672869.\\n\\n^ Cheal M, Lyon DR (November 1991). \"Central and peripheral precuing of forced-choice discrimination\". The Quarterly Journal of Experimental Psychology. A, Human Experimental Psychology. 43 (4): 859–80. doi:10.1080/14640749108400960. PMID\\xa01775667. S2CID\\xa013304439.\\n\\n^ Jonides J (1981). \"Voluntary versus automatic control over the mind\\'s eye movement\".  In Long JB, Braddely AD (eds.). Attention and performance IX. London: Erlbaum. pp.\\xa0187–203.\\n\\n^ Tsal Y (August 1983). \"Movements of attention across the visual field\". Journal of Experimental Psychology. Human Perception and Performance. 9 (4): 523–30. doi:10.1037/0096-1523.9.4.523. PMID\\xa06224890.\\n\\n^ Posner MI, Cohen YP (1984). \"Components of visual orienting\".  In Bouma H, Bouwhuis D (eds.). Attention and performance X. London: Erlbaum. pp.\\xa0531–566.\\n\\n^ Hodgson TL, Muller HJ (1999). \"Attentional Orienting in Two-dimensional Space\". The Quarterly Journal of Experimental Psychology A. 52 (3): 615–648. doi:10.1080/027249899390990.\\n\\n^ Jonides, J. (1981). Voluntary vs. automatic control over the mind\\'s eye\\'s movement. In J.B. Long & A.D. Baddeley (Eds.), Attention and performance IX (pp. 187–203). Hillsdale, NJ: Erlbaum.\\n\\n^ Rosen AC, Rao SM, Caffarra P, Scaglioni A, Bobholz JA, Woodley SJ,  et\\xa0al. (March 1999). \"Neural basis of endogenous and exogenous spatial orienting. A functional MRI study\". Journal of Cognitive Neuroscience. 11 (2): 135–52. doi:10.1162/089892999563283. PMID\\xa010198130. S2CID\\xa013573473.\\n\\n^ Theeuwes J (January 1991). \"Exogenous and endogenous control of attention: the effect of visual onsets and offsets\". Perception & Psychophysics. 49 (1): 83–90. doi:10.3758/bf03211619. PMID\\xa02011456.\\n\\n^ a b c d Posner MI, Petersen SE (1990). \"The attention system of the human brain\" (PDF). Annual Review of Neuroscience. 13 (1): 25–42. doi:10.1146/annurev.ne.13.030190.000325. PMID\\xa02183676. Archived from the original (PDF) on 2015-04-20. Retrieved 2015-01-10.\\n\\n^ Yan Y, Zhaoping L, Li W (October 2018). \"Bottom-up saliency and top-down learning in the primary visual cortex of monkeys\". Proceedings of the National Academy of Sciences of the United States of America. 115 (41): 10499–10504. Bibcode:2018PNAS..11510499Y. doi:10.1073/pnas.1803854115. PMC\\xa06187116. PMID\\xa030254154.\\n\\n^ Zhaoping L (May 2008). \"Attention capture by eye of origin singletons even without awareness--a hallmark of a bottom-up saliency map in the primary visual cortex\". Journal of Vision. 8 (5): 1.1–18. doi:10.1167/8.5.1. PMID\\xa018842072.\\n\\n^ Zhang X, Zhaoping L, Zhou T, Fang F (January 2012). \"Neural activities in v1 create a bottom-up saliency map\". Neuron. 73 (1): 183–92. doi:10.1016/j.neuron.2011.10.035. PMID\\xa022243756.\\n\\n^ a b Li Z (January 2002). \"A saliency map in primary visual cortex\". Trends in Cognitive Sciences. 6 (1): 9–16. doi:10.1016/s1364-6613(00)01817-9. PMID\\xa011849610. S2CID\\xa013411369.\\n\\n^ Posner MI, Rothbart MK (November 1998). \"Attention, self-regulation and consciousness\". Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences. 353 (1377): 1915–27. doi:10.1098/rstb.1998.0344. PMC\\xa01692414. PMID\\xa09854264.\\n\\n^ Astle DE, Scerif G (March 2009). \"Using developmental cognitive neuroscience to study behavioral and attentional control\". Developmental Psychobiology. 51 (2): 107–18. doi:10.1002/dev.20350. PMID\\xa018973175.\\n\\n^ Rueda MR, Rothbart MK, McCandliss BD, Saccomanno L, Posner MI (October 2005). \"Training, maturation, and genetic influences on the development of executive attention\". Proceedings of the National Academy of Sciences of the United States of America. 102 (41): 14931–6. Bibcode:2005PNAS..10214931R. doi:10.1073/pnas.0506897102. PMC\\xa01253585. PMID\\xa016192352.\\n\\n^ Murphy G, Greene CM (2016). \"Perceptual Load Affects Eyewitness Accuracy and Susceptibility to Leading Questions\". Frontiers in Psychology. 7: 1322. doi:10.3389/fpsyg.2016.01322. PMC\\xa05003837. PMID\\xa027625628.\\n\\n^ Lavie N, Hirst A, de Fockert JW, Viding E (September 2004). \"Load theory of selective attention and cognitive control\" (PDF). Journal of Experimental Psychology. General. 133 (3): 339–54. doi:10.1037/0096-3445.133.3.339. PMID\\xa015355143. Archived from the original (PDF) on 2013-06-26.\\n\\n^ Dougherty KM, Johnston JM (October 1996). \"Overlearning, fluency, and automaticity\". The Behavior Analyst. 19 (2): 289–92. doi:10.1007/bf03393171. PMC\\xa02733607. PMID\\xa022478265.\\n\\n^ Cartwright-Finch, Ula; Lavie, Nilli (2007-03-01). \"The role of perceptual load in inattentional blindness\". Cognition. 102 (3): 321–340. doi:10.1016/j.cognition.2006.01.002. ISSN\\xa00010-0277.\\n\\n^ \"Supplemental Material for The Mediating Role of Attention in the Association Between Math Anxiety and Math Performance: An Eye-Tracking Study\". Journal of Educational Psychology: edu0000759.supp. 2023. doi:10.1037/edu0000759.supp. ISSN\\xa00022-0663.\\n\\n^ Sohlberg MM, Mateer CA (1989). Introduction to cognitive rehabilitation: theory and practice. New York: Guilford Press. ISBN\\xa0978-0-89862-738-1.\\n\\n^ Carmody J (2009). \"Evolving Conceptions of Mindfulness in Clinical Settings\". Journal of Cognitive Psychotherapy. 23 (3): 270–280. doi:10.1891/0889-8391.23.3.270. S2CID\\xa0143844777.\\n\\n^ Kabat-Zinn J (2015). \"Mindfulness\". Mindfulness. 6 (6): 1481–1483. doi:10.1007/s12671-015-0456-x. S2CID\\xa0255796380.\\n\\n^ Langner R, Eickhoff SB (July 2013). \"Sustaining attention to simple tasks: a meta-analytic review of the neural mechanisms of vigilant attention\". Psychological Bulletin. 139 (4): 870–900. doi:10.1037/a0030694. PMC\\xa03627747. PMID\\xa023163491.\\n\\n^ Robertson IH, O\\'Connell R (2010). \"Vigilant attention.\".  In Nobre AC, Nobre K, Coull JT (eds.). Attention and Time. Oxford University Press. pp.\\xa079–88. ISBN\\xa0978-0-19-956345-6.\\n\\n^ Knudsen EI (2007). \"Fundamental components of attention\". Annual Review of Neuroscience. 30 (1): 57–78. doi:10.1146/annurev.neuro.30.051606.094256. PMID\\xa017417935.\\n\\n^ a b Pattyn N, Neyt X, Henderickx D, Soetens E (January 2008). \"Psychophysiological investigation of vigilance decrement: boredom or cognitive fatigue?\". Physiology & Behavior. 93 (1–2): 369–78. doi:10.1016/j.physbeh.2007.09.016. PMID\\xa017999934. S2CID\\xa09861215.\\n\\n^ Zhaoping L (October 2016). \"From the optic tectum to the primary visual cortex: migration through evolution of the saliency map for exogenous attentional guidance\". Current Opinion in Neurobiology. 40: 94–102. doi:10.1016/j.conb.2016.06.017. PMID\\xa027420378. S2CID\\xa0206952820.\\n\\n^ Kaiser J, Lutzenberger W (December 2003). \"Induced gamma-band activity and human brain function\". The Neuroscientist. 9 (6): 475–84. doi:10.1177/1073858403259137. PMID\\xa014678580. S2CID\\xa023574844.\\n\\n^ Siegel M, Donner TH, Oostenveld R, Fries P, Engel AK (November 2008). \"Neuronal synchronization along the dorsal visual pathway reflects the focus of spatial attention\". Neuron. 60 (4): 709–19. doi:10.1016/j.neuron.2008.09.010. PMID\\xa019038226. S2CID\\xa019010227.\\n\\n^ Baldauf D, Desimone R (April 2014). \"Neural mechanisms of object-based attention\". Science. 344 (6182): 424–7. Bibcode:2014Sci...344..424B. doi:10.1126/science.1247003. PMID\\xa024763592. S2CID\\xa034728448.\\n\\n^ Posner MI, Boies SJ (1971). \"Components of attention\". Psychological Review. 78 (5): 391–408. doi:10.1037/h0031333.\\n\\n^ Trautwein FM, Singer T, Kanske P (August 2016). \"Stimulus-Driven Reorienting Impairs Executive Control of Attention: Evidence for a Common Bottleneck in Anterior Insula\". Cerebral Cortex. 26 (11): 4136–4147. doi:10.1093/cercor/bhw225. PMC\\xa05066828. PMID\\xa027550866.\\n\\n^ Fan J, Gu X, Guise KG, Liu X, Fossella J, Wang H, Posner MI (July 2009). \"Testing the behavioral interaction and integration of attentional networks\". Brain and Cognition. 70 (2): 209–20. doi:10.1016/j.bandc.2009.02.002. PMC\\xa02674119. PMID\\xa019269079.\\n\\n^ Callejas A, Lupiáñez J, Tudela P (April 2004). \"The three attentional networks: on their independence and interactions\". Brain and Cognition. 54 (3): 225–7. doi:10.1016/j.bandc.2004.02.012. PMID\\xa015050779. S2CID\\xa0775862.\\n\\n^ Coull JT, Frith CD, Frackowiak RS, Grasby PM (November 1996). \"A fronto-parietal network for rapid visual information processing: a PET study of sustained attention and working memory\". Neuropsychologia. 34 (11): 1085–95. doi:10.1016/0028-3932(96)00029-2. PMID\\xa08904746. S2CID\\xa025430660.\\n\\n^ Marrocco RT, Witte EA, Davidson MC (April 1994). \"Arousal systems\". Current Opinion in Neurobiology. 4 (2): 166–70. doi:10.1016/0959-4388(94)90067-1. PMID\\xa07913640. S2CID\\xa035709525.\\n\\n^ Fan J, McCandliss BD, Flombaum JI, Thomas KM, Posner MI (2001). \"Comparing images of conflict in frontal cortex\". Annual meeting of the Cognitive Neuroscience Society. New York, NY. Archived from the original on 2015-01-10.\\n\\n^ Correa-Chavez M, Barbara R (2009). \"Cultural variation in children\\'s attention and learning\". Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society.\\n\\n^ a b Rogoff B, Correa-Chavez M, Silva KG (2011). \"Cultural variation in children\\'s attention and learning\". Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society: 1–18.\\n\\n^ Rogoff B (2003-02-13). The Cultural Nature of Human Development. Oxford University Press. ISBN\\xa09780195131338.\\n\\n^ a b c Li J, Levine MD, An X, Xu X, He H (April 2013). \"Visual saliency based on scale-space analysis in the frequency domain\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (4): 996–1010. arXiv:1605.01999. doi:10.1109/TPAMI.2012.147. PMID\\xa022802112. S2CID\\xa0350786.\\n\\n^ Zang J, Wang L, Liu Z, Zhang Q, Hua G, Zheng N (2018). \"Attention-Based Temporal Weighted Convolutional Neural Network for Action Recognition\". IFIP Advances in Information and Communication Technology. Ifip Advances in Information and Communication Technology Ifip Aict. Vol.\\xa0519. Cham: Springer International Publishing. pp.\\xa097–108. arXiv:1803.07179. doi:10.1007/978-3-319-92007-8_9. ISBN\\xa0978-3-319-92006-1. ISSN\\xa01868-4238. S2CID\\xa04058889.\\n\\n^ Wang L, Zang J, Zhang Q, Niu Z, Hua G, Zheng N (June 2018). \"Action Recognition by an Attention-Aware Temporal Weighted Convolutional Neural Network\" (PDF). Sensors. 18 (7): 1979. Bibcode:2018Senso..18.1979W. doi:10.3390/s18071979. PMC\\xa06069475. PMID\\xa029933555.\\n\\n^ Itti L, Koch C, Niebur E (1998). \"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis\". IEEE Trans Pattern Anal Mach Intell. 20 (11): 1254–1259. CiteSeerX\\xa010.1.1.53.2366. doi:10.1109/34.730558.\\n\\n^ Hou X, Zhang L (2007). \"Saliency Detection: A Spectral Residual Approach\" (PDF). 2007 IEEE Conference on Computer Vision and Pattern Recognition. pp.\\xa01–8. CiteSeerX\\xa010.1.1.579.1650. doi:10.1109/CVPR.2007.383267. ISBN\\xa0978-1-4244-1179-5. S2CID\\xa015611611. Archived from the original (PDF) on 2015-02-12. Retrieved 2015-01-10.\\n\\n^ Li J, Levine MD, An X, Xu X, He H (April 2013). \"Visual saliency based on scale-space analysis in the frequency domain\" (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (4): 996–1010. arXiv:1605.01999. doi:10.1109/TPAMI.2012.147. PMID\\xa022802112. S2CID\\xa0350786. Archived from the original (PDF) on 2013-03-01.\\n\\n^ Behnke S (2003). Hierarchical Neural Networks for Image Interpretation. Lecture Notes in Computer Science. Vol.\\xa02766. Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/b11963. ISBN\\xa0978-3-540-40722-5. S2CID\\xa01304548.\\n\\n^ a b Kalat JW (2013). Biological Psychology (11th\\xa0ed.). Cengage Learning.\\n\\n^ a b Silveri MC, Ciccarelli N, Cappa A (September 2011). \"Unilateral spatial neglect in degenerative brain pathology\". Neuropsychology. 25 (5): 554–66. doi:10.1037/a0023957. PMID\\xa021639641.\\n\\n^ Karnath HO, Rorden C, Ticini LF (October 2009). \"Damage to white matter fiber tracts in acute spatial neglect\". Cerebral Cortex. 19 (10): 2331–7. doi:10.1093/cercor/bhn250. PMC\\xa02742593. PMID\\xa019168667.\\n\\n^ Buxbaum LJ (2006). \"On the right (and left) track: Twenty years of progress in studying hemispatial neglect\". Cognitive Neuropsychology. 23 (1): 184–201. doi:10.1080/02643290500202698. PMID\\xa021049327. S2CID\\xa027750259.\\n\\n^ Ptak R, Schnider A (September 2010). \"The dorsal attention network mediates orienting toward behaviorally relevant stimuli in spatial neglect\". The Journal of Neuroscience. 30 (38): 12557–65. doi:10.1523/JNEUROSCI.2722-10.2010. PMC\\xa06633576. PMID\\xa020861361.\\n\\n^ Klein JT, Shepherd SV, Platt ML (November 2009). \"Social attention and the brain\". Current Biology. 19 (20): R958–62. doi:10.1016/j.cub.2009.08.010. PMC\\xa03387539. PMID\\xa019889376.\\n\\n^ Humphreys GW, Sui J (2016). \"Attentional control and the self: The Self-Attention Network (SAN)\". Cognitive Neuroscience. 7 (1–4): 5–17. doi:10.1080/17588928.2015.1044427. PMID\\xa025945926. S2CID\\xa052867757.\\n\\n^ a b Kuang S (2016). \"Two Polarities of Attention in Social Contexts: From Attending-to-Others to Attending-to-Self\". Frontiers in Psychology. 7: 63. doi:10.3389/fpsyg.2016.00063. PMC\\xa04734343. PMID\\xa026869965.\\n\\n^ Pacheco‐Unguetti, Antonia Pilar; Parmentier, Fabrice B. R. (August 2016). \"Happiness increases distraction by auditory deviant stimuli\". British Journal of Psychology. 107 (3): 419–433. doi:10.1111/bjop.12148. ISSN\\xa00007-1269.\\n\\n^ Drummond, Sean P. (2004-08-01). \"The Effects of Total Sleep Deprivation and Recovery Sleep on Cognitive Performance and Brain Function\". Fort Belvoir, VA. {{cite journal}}: Cite journal requires |journal= (help)\\n\\n^ Mack A (2003). \"Inattentional Blindness: Looking without Seeing\". Current Directions in Psychological Science. 12 (5): 180–184. doi:10.1111/1467-8721.01256. ISSN\\xa00963-7214. JSTOR\\xa020182872. S2CID\\xa015230550.\\n\\n^ Lavie N, Beck DM, Konstantinou N (May 2014). \"Blinded by the load: attention, awareness and the role of perceptual load\". Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences. 369 (1641): 20130205. doi:10.1098/rstb.2013.0205. JSTOR\\xa024500065. PMC\\xa03965161. PMID\\xa024639578.\\n\\n^ Rensink RA, O\\'Regan JK, Clark JJ (1997). \"To See or Not to See: The Need for Attention to Perceive Changes in Scenes\". Psychological Science. 8 (5): 368–373. doi:10.1111/j.1467-9280.1997.tb00427.x. ISSN\\xa00956-7976. JSTOR\\xa040063214. S2CID\\xa01945079.\\n\\n^ Andrew Brook and Julian Wuerth\\nhttps://plato.stanford.edu/entries/kant-mind/\\n\\n^ Malebranche N (1674). The Search After Truth. pp.\\xa0411–412.\\n\\n^ Runes DD, ed. (1972). Dictionary of Philosophy. Totowa, NJ: Littlefield, Adams, and Company.\\n\\n^ Jevons WS (9 February 1871). \"The Power of Numerical Discrimination\". Nature.\\n\\n^ James W (1890). The Principles of Psychology. Vol.\\xa01. New York: Henry Holt. pp.\\xa0403–404.\\n\\n^ Jersild AT (1927). \"Mental set and shift\". Archives of Psychology. 14 (89): 5–82.\\n\\n^ Harré R (2002). Cognitive science: A philosophical introduction. London: SAGE Publications. ISBN\\xa0978-0-7619-4746-2.\\n\\n^ a b Hampson PJ, Morris PE (1996). Understanding cognition. ISBN\\xa0978-0-631-15751-9.\\n\\n^ Deutsch JA, Deutsch D (January 1963). \"Some theoretical considerations\". Psychological Review. 70 (1): 80–90. doi:10.1037/h0039515. PMID\\xa014027390.\\n\\n^ Theeuwes J, Kramer AF, Belopolsky AV (August 2004). \"Attentional set interacts with perceptual load in visual search\". Psychonomic Bulletin & Review. 11 (4): 697–702. doi:10.3758/BF03196622. PMID\\xa015581120.\\n\\nFurther reading[edit]\\n\\nWikimedia Commons has media related to \\n\\nAttention.\\n\\nWikiquote has quotations related to \\n\\nAttention\\n\\nGoleman D (2013). Focus: The Hidden Driver of Excellence. Harper. ISBN\\xa0978-0062114860.\\n\\nWard LM (2008). \"PDF\". Scholarpedia. 3 (10): 1538. doi:10.4249/scholarpedia.1538.\\n\\nTilburg University) on focused attention\\n\\nv\\n\\nt\\n\\ne\\n\\nMental processes\\n\\nCognition\\n\\nAwareness\\nComprehension\\nConsciousness\\nCritical thinking\\nDecision-making\\nImagination\\nIntuition\\nProblem solving\\n\\nPerception\\n\\nAmodal\\nColor\\nRGB model\\nDepth\\nForm\\nHaptic (Touch)\\nPerception as interpretation\\nPeripheral\\nSocial\\nSound\\nHarmonics\\nPitch\\nSpeech\\nVisual\\n\\nMemory\\n\\nConsolidation\\nEncoding\\nStorage\\nRecall\\n\\nAttention\\nHigher nervous activity\\nIntention\\nLearning\\nMental fatigue\\nMental set\\nThinking\\nVolition\\n\\nv\\n\\nt\\n\\ne\\n\\nHuman memory\\n\\nEncoding\\nStorage\\nRecall\\nAttention\\nConsolidation\\nNeuroanatomy\\n\\nSensory\\nEchoic\\nEidetic\\nEyewitness\\nHaptic\\nIconic\\nMotor learning\\nVisual\\nShort-term\\n\"The Magical Number Seven, Plus or Minus Two\"\\nWorking memory\\nLong-term\\nActive recall\\nAutobiographical\\nExplicit\\nDeclarative\\nEpisodic\\nSemantic\\nFlashbulb\\nHyperthymesia\\nImplicit\\nMeaningful learning\\nPersonal-event\\nProcedural\\nRote learning\\nSelective retention\\nTip of the tongue\\n\\nForgetting\\n\\nAmnesia\\nanterograde\\nchildhood\\npost-traumatic\\npsychogenic\\nretrograde\\nselective\\ntransient global\\nDecay theory\\nForgetting curve\\nInterference theory\\nMemory inhibition\\nMotivated forgetting\\nRepressed memory\\nRetrieval-induced forgetting\\nWeapon focus\\n\\nMemory errors\\n\\nConfabulation\\nCryptomnesia\\nHindsight bias\\nImagination inflation\\nMemory biases\\nMemory conformity\\nMisattribution of memory\\nMisinformation effect\\nSource-monitoring error\\n\\nFalse memory\\n\\nDeese–Roediger–McDermott paradigm\\nFalse memory syndrome\\nMemory implantation\\nLost in the mall technique\\nRecovered-memory therapy\\n\\nResearch methods\\n\\nExceptional memory\\nIndirect tests of memory\\nMemory disorder\\n\\nCollective memory\\nPolitics of memory\\nCultural memory\\nMemory and social interactions\\nMemory conformity\\nTransactive memory\\n\\nAging\\nArt of memory\\nchunking\\nmnemonic\\nEffects of alcohol\\nEffects of exercise\\nEmotion\\nMemory improvement\\nSleep\\nTrauma\\n\\nMemory sport\\nWorld Memory Championships\\nShas Pollak\\n\\nAbsent-mindedness\\nAtkinson–Shiffrin memory model\\nContext-dependent & state-dependent memory\\nChildhood memory\\nExosomatic memory\\nFree recall\\nIntermediate-term memory\\nInvoluntary memory\\nflashbacks\\nLevels of Processing model\\nMetamemory\\nMuscle memory\\nPriming\\nintertrial\\nProspective & retrospective memory\\nThe Seven Sins of Memory\\n\\nResearchers\\nRichard C. Atkinson\\nRobert A. Bjork\\nStephen J. Ceci\\nSusan Clancy\\nHermann Ebbinghaus\\nSigmund Freud\\nPatricia Goldman-Rakic\\nIvan Izquierdo\\nMarcia K. Johnson\\nEric Kandel\\nElizabeth Loftus\\nGeoffrey Loftus\\nJames McGaugh\\nEleanor Maguire\\nGeorge Armitage Miller\\nBrenda Milner\\nLynn Nadel\\nHenry L. Roediger III\\nDaniel Schacter\\nRichard Shiffrin\\nArthur P. Shimamura\\nLarry Squire\\nSusumu Tonegawa\\nAnne Treisman\\nEndel Tulving\\nRobert Stickgold\\nPatients\\nHM\\nKC\\nClive Wearing\\nOther\\nJonathan Hancock\\nPaul R. McHugh\\nDominic O\\'Brien\\nBen Pridmore\\nCosmos Rossellius\\nAndriy Slyusarchuk\\n\\nPsychology portal\\n\\xa0Philosophy portal\\n\\nv\\n\\nt\\n\\ne\\n\\nConsciousness\\n\\nPhilosophy\\nAlfred North Whitehead\\nArthur Schopenhauer\\nBaruch Spinoza\\nBertrand Russell\\nBrian O\\'Shaughnessy\\nCharles Augustus Strong\\nChristopher Peacocke\\nColin McGinn\\nDaniel Dennett\\nDavid Chalmers\\nDavid Hume\\nDavid Papineau\\nDavid Pearce\\nDonald Davidson\\nDouglas Hofstadter\\nEdmund Husserl\\nFrank Jackson\\nFred Dretske\\nGalen Strawson\\nGeorge Berkeley\\nGeorge Henry Lewes\\nGeorges Rey\\nGottfried Leibniz\\nImmanuel Kant\\nJohn Eccles\\nJohn Locke\\nJohn Polkinghorne\\nJohn Searle\\nJoseph Levine\\nKarl Popper\\nKeith Frankish\\nKenneth M. Sayre\\nMaurice Merleau-Ponty\\nMax Velmans\\nMichael Tye\\nMartin Heidegger\\nNed Block\\nPatricia Churchland\\nPaul Churchland\\nPhilip Goff\\nRené Descartes\\nThomas Metzinger\\nThomas Nagel\\nWilliam Kingdon Clifford\\nWilliam Lycan\\nWilliam Seager\\nPsychology\\nCarl Gustav Jung\\nDonald D. Hoffman\\nFranz Brentano\\nGustav Fechner\\nKurt Koffka\\nMax Wertheimer\\nSigmund Freud\\nWilhelm Wundt\\nWilliam James\\nWolfgang Köhler\\nNeuroscience\\nAnil Seth\\nAntonio Damasio\\nBenjamin Libet\\nBernard Baars\\nChristof Koch\\nFrancis Crick\\nFrancisco Varela\\nGerald Edelman\\nGiulio Tononi\\nKarl Pribram\\nLawrence Weiskrantz\\nMichael Gazzaniga\\nMichael Graziano\\nPatrick Wilken\\nRoger Sperry\\nStanislas Dehaene\\nSteven Laureys\\nStuart Hameroff\\nWolf Singer\\nOthers\\nAnnaka Harris\\nDavid Bohm\\nEugene Wigner\\nErwin Schrödinger\\nMarvin Minsky\\nMax Planck\\nRoger Penrose\\nSusan Blackmore\\nVictor J. Stenger\\nWolfgang Pauli\\n\\nPhilosophy of mind\\nAnomalous monism\\nComputationalism\\nDouble-aspect theory\\nEliminative materialism\\nEmergentism\\nEpiphenomenalism\\nFunctionalism\\nIdealism\\nInteractionism\\nMaterialism\\nMind–body dualism\\nMonism\\nNeutral monism\\nNew mysterianism\\nPanpsychism\\nParallelism\\nPhysicalism\\nProperty dualism\\nQualia\\nReflexive monism\\nRevisionary materialism\\nSolipsism\\nType physicalism (reductive materialism, identity theory)\\nScience\\nAttention schema theory\\nDynamic core hypothesis\\nDamasio\\'s theory of consciousness\\nElectromagnetic theories of consciousness\\nGlobal workspace theory\\nHolonomic brain theory\\nIntegrated information theory\\nLamme\\'s recurrent feedback hypothesis\\nMultiple drafts model\\nOrchestrated objective reduction\\n\\nAgnosia\\nAltered state of consciousness\\nAnimal consciousness\\nArtificial consciousness\\nAttention\\nAwareness\\nBinding problem\\nBinocular rivalry\\nBlindsight\\nBrain\\nCartesian theater\\nConsciousness after death\\nDisorders of consciousness\\nDivided consciousness\\nDual consciousness (split-brain)\\nExperience\\nExplanatory gap\\nFree will\\nFlash suppression\\nHallucination\\nHard problem of consciousness\\nHeterophenomenology\\nHigher consciousness\\nIllusion\\nIntrospection illusion\\nKnowledge argument\\nLocked-in syndrome\\nMind\\nMind–body problem\\nMinimally conscious state\\nNeural correlates of consciousness\\nNeurophenomenology\\nOntology\\nPhenomenology\\nPhilosophical zombie\\nPhilosophy of mind\\nPrimary consciousness\\nProblem of other minds\\nReentry\\nQualia\\nQuantum mind\\nSakshi\\nPurusha\\nSecondary consciousness\\nSentience\\nSentiocentrism\\nSociology of human consciousness\\nSoul\\nStream of consciousness\\nSubconscious\\nSubjective character of experience\\nSubjectivity\\nUnconscious mind\\nUnconsciousness\\nUpanishads\\nVisual masking\\nVon Neumann–Wigner interpretation\\nYogachara\\n\\nA Universe of Consciousness\\nAssociation for the Scientific Study of Consciousness\\nConsciousness and Cognition\\nConsciousness Explained\\nCosmic Consciousness\\nHow the Self Controls Its Brain\\nJournal of Consciousness Studies\\nOnline Consciousness Conference\\nPsyche\\nThe Astonishing Hypothesis\\nThe Conscious Mind\\nThe Emperor\\'s New Mind\\nThe Science of Consciousness\\nUnderstanding Consciousness\\n\"What Is it Like to Be a Bat?\"\\nWider than the Sky\\n\\nCategory\\n Commons\\n\\nv\\n\\nt\\n\\ne\\n\\nDigital media use and mental health\\n\\nProposed or recogniseddiagnostic categories\\n\\nComputer addiction\\nInternet addiction disorder\\nInternet sex addiction\\nOnline problem gambling\\nProblematic smartphone use\\nNomophobia\\nProblematic social media use\\nTelevision addiction\\nVideo game addiction\\n\\nDisciplines involved\\n\\nDigital anthropology\\nDigital sociology\\nHuman factors and ergonomics\\nCognitive ergonomics\\nComputer-mediated communication\\nEngineering psychology\\nHuman–computer interaction\\nMedia naturalness theory\\nNeuroergonomics\\nNeuroscience\\nPsychiatry\\nEvolutionary\\nPsychology\\nClinical\\nCognitive\\nEvolutionary\\nSocial\\n\\nAssociatedpsychiatric conditions\\n\\nAnxiety disorder\\nAttention deficit hyperactivity disorder\\nAutism\\nDepression\\nEating disorder\\nAnorexia nervosa\\nBody image disturbance\\nInsomnia\\nNarcissistic personality disorder\\n\\nAttention\\n\\nAttention economy\\nAttention inequality\\nAttention management\\nAttention span\\nBetteridge\\'s law of headlines\\nChumbox\\nClickbait\\nDigital zombie\\nDoomscrolling\\nHuman multitasking\\nMedia multitasking\\nMobile phones and driving safety\\nPhubbing\\nSmartphones and pedestrian safety\\nTexting while driving\\nInfluence-for-hire\\nInfodemic\\nInformation explosion\\nInformation overload\\nInformation pollution\\nInformation–action ratio\\nInfotainment\\nKnowledge gap hypothesis\\nMissing white woman syndrome\\nNeophile\\nOne weird trick\\nRage farming\\nScreen time\\nBinge-watching\\nSocial aspects of television\\nTelevision consumption\\nSticky content\\nTabloid television\\nYellow journalism\\n\\nCognitive bias/\\n\\nConformity\\n\\nAvailability heuristic\\nBandwagon effect\\nConfirmation bias\\nConformity\\nEcho chamber\\nFalse consensus effect\\nFalse-uniqueness effect\\nFear of missing out\\nFilter bubble\\nMean world syndrome\\nNegativity bias\\nOverchoice\\nPeer pressure\\nPluralistic ignorance\\nSelf-deprecation\\nSocial-desirability bias\\nSocial influence bias\\nSocial media bias\\nSpiral of silence\\nToxic positivity\\n\\nRelated topics\\n\\nAffective polarization\\nBehavioral modernity\\nBody image\\nCognitive miser\\nComputer rage\\nCriticism of Facebook\\n2021 Facebook leak\\nCyberbullying\\nPromotion of anorexia\\nSocial media and suicide\\nSuicide and the Internet\\nCriticism of Netflix\\nDigital detox\\nDigital divide\\nEffects of violence in mass media\\nEvolution of cognition\\nEvolutionary mismatch\\nGriefer\\nKnowledge divide\\nMass shooting contagion\\nMedia manipulation\\nMoral panic\\nOnline dating\\nOnline youth radicalization\\nSealioning\\nSocial bot\\nSocial impact of YouTube\\nTechnophilia\\nTechnophobia\\nTechnostress\\nViolence and video games\\n\\nAuthority control\\n\\nFAST\\n\\nSpain\\nFrance\\nBnF data\\nGermany\\nIsrael\\nUnited States\\nJapan\\nCzech Republic\\n2\\n\\nIdRef\\n2\\n\\nRetrieved from \"\\n\\nhttps://en.wikipedia.org/w/index.php?title=Attention&oldid=1154746660\"\\n\\nCategories:\\n\\nAttention\\n\\nAttention deficit hyperactivity disorder\\n\\nBehavioral concepts\\n\\nMental processes\\n\\nNeuropsychological assessment\\n\\nUnsolved problems in neuroscience\\n\\nHidden categories: \\n\\nCS1 errors: missing periodical\\n\\nCS1: Julian–Gregorian uncertainty\\n\\nArticles with short description\\n\\nShort description is different from Wikidata\\n\\nCommons category link is on Wikidata\\n\\nArticles with FAST identifiers\\n\\nArticles with BNE identifiers\\n\\nArticles with BNF identifiers\\n\\nArticles with BNFdata identifiers\\n\\nArticles with GND identifiers\\n\\nArticles with J9U identifiers\\n\\nArticles with LCCN identifiers\\n\\nArticles with NDL identifiers\\n\\nArticles with NKC identifiers\\n\\nArticles with SUDOC identifiers' metadata={'source': 'https://en.wikipedia.org/wiki/Attention'}\n"
     ]
    }
   ],
   "source": [
    "print(url_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(url_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs + url_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter\n",
    "- Output: text_chunks (with document insides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'langchain.schema.Document'> 426\n"
     ]
    }
   ],
   "source": [
    "print(type(text_chunks), type(text_chunks[0]), len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore\n",
    "- Output: search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "search_space = Chroma.from_documents(docs, openai_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chroma collection langchain contains fewer than 4 elements.\n"
     ]
    }
   ],
   "source": [
    "search_result = search_space.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3041"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_result[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc\u0002tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs\n"
     ]
    }
   ],
   "source": [
    "print(search_result[1].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question & Answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever is got by using seach_space.as_retriever()\n",
    "QA_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = OpenAI(\n",
    "        temperature = 0,\n",
    "        max_tokens = 200\n",
    "    ),\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = search_space.as_retriever(),\n",
    "    reduce_k_below_max_tokens = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chroma collection langchain contains fewer than 4 elements.\n"
     ]
    }
   ],
   "source": [
    "result = QA_chain(\n",
    "    {\n",
    "        \"question\": \"What is attention?\",\n",
    "    },\n",
    "    return_only_outputs = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
