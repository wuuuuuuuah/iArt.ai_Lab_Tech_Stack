{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Loading\n",
    "- Environment\n",
    "    - torch\n",
    "    - fairscale\n",
    "    - fire\n",
    "    - sentencepiece==0.1.97\n",
    "    - hiq-python (if hiq is needed)\n",
    "- model and ckpt\n",
    "    - follow the command bellow (or Just using url to get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from logging import getLogger\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "class LLaMA_Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        # reload tokenizer\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self.sp_model.decode(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/params.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Transformer Contruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# The \"fairscale\" package is a PyTorch extension library that provides various tools \n",
    "# and utilities for scalable and efficient training of deep neural networks. \n",
    "# It focuses on enabling large-scale model parallelism and mixed-precision training.\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    "    ColumnParallelLinear,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        return output.float()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Generator Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLaMA:\n",
    "    def __init__(self, model: Transformer, tokenizer: LLaMA_Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> List[str]:\n",
    "        bsz = len(prompts)\n",
    "        params = self.model.params\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "        input_text_mask = tokens != self.tokenizer.pad_id\n",
    "        start_pos = min_prompt_size\n",
    "        prev_pos = 0\n",
    "        for cur_pos in range(start_pos, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            prev_pos = cur_pos\n",
    "\n",
    "        decoded = []\n",
    "        for i, t in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            try:\n",
    "                t = t[: t.index(self.tokenizer.eos_id)]\n",
    "            except ValueError:\n",
    "                pass\n",
    "            decoded.append(self.tokenizer.decode(t))\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8,\n",
    "top_p: float = 0.95,\n",
    "max_seq_len: int = 512,\n",
    "max_batch_size: int = 32,\n",
    "llama_tokenizer_n_word = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    print(\"Loading\")\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "\n",
    "    tokenizer = LLaMA_Tokenizer(model_path=tokenizer_path)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    \n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(model_args)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "    \n",
    "    generator = LLaMA(model, tokenizer)\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pth = 'pyllama_data/tokenizer.model'\n",
    "llama_tokenizer = LLaMA_Tokenizer(\n",
    "    model_path = tokenizer_pth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 269, 893]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.encode(\n",
    "    s = 'sss',\n",
    "    bos=True, \n",
    "    eos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of  32000\n",
      "the token_id of begining of the sentence is: 1\n",
      "the token_id of end of the sentence is: 2\n",
      "the token_id of padding is: -1\n"
     ]
    }
   ],
   "source": [
    "print('the number of ',llama_tokenizer.n_words)\n",
    "print('the token_id of begining of the sentence is:', llama_tokenizer.bos_id)\n",
    "print('the token_id of end of the sentence is:', llama_tokenizer.eos_id)\n",
    "print('the token_id of padding is:', llama_tokenizer.pad_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "to find happiness and be satisfied with what you have.\n",
    "People have different definitions of happiness. Some people feel that if they could only win the lottery, they would be happy. Some people feel that if they could only get that promotion, they would be happy. Some people feel that if they could only be the top scorer in a game, they would be happy.\n",
    "If you do not know what happiness is, I suggest you ask a psychologist. A psychologist has studied the subject of happiness and he or she knows what happiness is. A psychologist has a Ph.D. in psychology and is an expert on the subject of happiness. A psychologist knows how to make people happy.\n",
    "Although you might know what happiness is, you might have forgotten it. If that is the case, I suggest you consult a psychologist. A psychologist can make you happy again. A psychologist can help you discover your happiness and how to be happy.\n",
    "Happiness is a big word. Happiness is a nice word. Happiness is a beautiful word.\n",
    "I believe that the meaning of life is to find happiness and be satisfied with what you have.\n",
    "People have different definitions of happiness. Some people feel\n",
    "1) there is no absolute time or space and 2) the speed of light in a vacuum is the fastest speed possible. There are two key principles in relativity:\n",
    "(1) The laws of physics are the same in all inertial reference frames.\n",
    "(2) The speed of light is constant in all inertial reference frames.\n",
    "The second of these principles has allowed us to prove the first.\n",
    "Before Einstein, scientists believed that the speed of light was constant in all frames, but that the speed of light was not constant. This was called the constancy of the speed of light hypothesis. In the late 19th century, scientists such as Michelson and Morley and Lorentz had set up experiments to test this hypothesis.\n",
    "For example, when Michelson and Morley set up their Michelson-Morley interferometer, they expected that the light would take a different path depending on whether it was moving at the same speed as the Earth or at a different speed. They found that it didn't, but the constancy of the speed of light hypothesis said that it would.\n",
    "Why didn't the constancy of the speed of light hypothesis work? Because it was wrong\n",
    "1. Decide what you need\n",
    "What is it that you need to do? Do you want people to buy a product, do you want to have a blog or do you want to provide a service? These are all different types of websites. The type of website you need will have a large impact on how you design your website.\n",
    "2. Do you want a responsive website?\n",
    "Some websites are better suited for a mobile or tablet. If you are planning to have a mobile site, then it is recommended to have a responsive website. This allows your website to be displayed on any device.\n",
    "3. What is your target audience?\n",
    "Having a clear idea of who your target audience is can help you design a website that will be targeted to them. Also, you will be able to know how to market your website and get people to your website.\n",
    "4. What are you going to use?\n",
    "There are many different web designing platforms. Some are free and others cost money. There are pros and cons to both types of websites.\n",
    "5. Decide on the look and feel\n",
    "What is your website going to look like? What colors are you going to use? Is there a certain logo or icon that you\n",
    "Positive\n",
    "###\n",
    "Tweet: \"My heart is broken\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"I have some great news\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"My favorite band just announced a new album\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"That food was so good\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"My company just moved to a new building\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"I just ate the best lunch ever\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"We are at 87% to our goal\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"I just got an awesome new job\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"My favorite sports team just won the championship\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"My favorite sports team just lost\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My favorite sports team just won\"\n",
    "Sentiment: Positive\n",
    "##\n",
    "fromage\n",
    "\n",
    "Tell me what you need, and I'll do my best to give you the best answers.\n",
    "\n",
    "Answer: You will find many translators on the web. For example: http://www.french-translator.com/translator.html\n",
    "\n",
    "You can also use the Wikipedia as a guide to understand the french meaning.\n",
    "\n",
    "Comment: Your answer could be improved with additional supporting information. Please [edit] to add further details, such as citations or documentation, so that others can confirm that your answer is correct. You can find more information on how to write good answers [in the help center](/help/how-to-answer).\n",
    "\n",
    "Answer: There is no automatic tool to translate English to French. There are a couple of tools on the web, but I do not know if you will find anything to help you.\n",
    "\n",
    "You can find a translation in a dictionary, a bilingual dictionary, a French to English one, if you find a French definition for the term.\n",
    "\n",
    "Then you can try to match your translation with the French definition.\n",
    "\n",
    "Answer: You can use Google Translate to translate words from English to French. But it is important to note that\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I believe the meaning of life is\n",
    "Simply put, the theory of relativity states that \n",
    "Building a website can be done in 10 simple steps:\\n\n",
    "Tweet: \"I hate it when my phone battery dies.\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been 👍\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This is the link to the article\"\n",
    "Sentiment: Neutral\n",
    "###\n",
    "Tweet: \"This new music video was incredibile\"\n",
    "Sentiment:\n",
    "Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "\n",
    "peppermint => menthe poivrée\n",
    "\n",
    "plush girafe => girafe peluche\n",
    "\n",
    "cheese =>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of generated text is: 1288\n",
      "length of prompt is: 182\n"
     ]
    }
   ],
   "source": [
    "print('length of generated text is:', str(len(llama_tokenizer.encode(s=text,bos=True,eos=False))))\n",
    "print('length of prompt is:', str(len(llama_tokenizer.encode(s=prompt,bos=True,eos=False))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def gen(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 32,\n",
    "):\n",
    "    local_rank, world_size = setup_model_parallel()\n",
    "    if local_rank > 0:\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    generator = load(\n",
    "        ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"Building a website can be done in 10 simple steps:\\n\",\n",
    "        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n",
    "        \"\"\"Tweet: \"I hate it when my phone battery dies.\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been 👍\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This is the link to the article\"\n",
    "Sentiment: Neutral\n",
    "###\n",
    "Tweet: \"This new music video was incredibile\"\n",
    "Sentiment:\"\"\",\n",
    "        \"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "\n",
    "peppermint => menthe poivrée\n",
    "\n",
    "plush girafe => girafe peluche\n",
    "\n",
    "cheese =>\"\"\",\n",
    "    ]\n",
    "    start = time.time()\n",
    "    results = generator.generate(\n",
    "        prompts, max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "    )\n",
    "    stop = time.time()\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"\\n==================================\\n\")\n",
    "    print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command on At Least 40G RAM\n",
    "!torchrun --nproc_per_node 1 example.py --ckpt_dir ../pyllama/pyllama_data/7B --tokenizer_path ../pyllama/pyllama_data/tokenizer.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
