{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../my_key.txt') as f:\n",
    "    key_list = f.readlines()\n",
    "\n",
    "key_list = [key.strip() for key in key_list]\n",
    "\n",
    "keys = {}\n",
    "for key in key_list:\n",
    "    key = key.split(':')\n",
    "    if (key != ['']):\n",
    "        keys[key[0]] = key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiktoken\n",
    "- Use to count the number of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_token = 'I love cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 3021, 8415]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_list = encoding.encode(text_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Original Call\n",
    "- prompt 1765 + completion 226 = \n",
    "    - 'gpt-3.5-turbo': 18s+14s+15s /3 = 16s\n",
    "    - 'text-davinci-003': above 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "openai.organization = keys['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "# openai.Model.list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo\n",
    "- Without Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_content = \"A tiger does not lose sleep over the opinion of sheep.\"\n",
    "text2_content = \"The tiger's strength is born from the struggle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are a story teller. Make a story from two sentences.\"\"\"\n",
    "\n",
    "text = text1_content + '\\n' + text2_content\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "Sentences: {text}\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "tur_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    temperature = 0,\n",
    ")\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The article compares two popular language models, GPT-3 and BERT, and discusses their differences and similarities, exploring their capabilities and the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, while BERT is a bidirectional transformer model developed by Google AI. The main difference between the two models lies in their architecture and training dataset size, with GPT-3 being better suited for tasks such as summarization or translation, while BERT is more beneficial for sentiment analysis or natural language understanding (NLU). However, both models have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1685508068,\n",
      "  \"id\": \"chatcmpl-7M7vwE8NQeN2sDGdZBmt35eFvgzdK\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 134,\n",
      "    \"prompt_tokens\": 1750,\n",
      "    \"total_tokens\": 1884\n",
      "  }\n",
      "}\n",
      "9.403381586074829s\n"
     ]
    }
   ],
   "source": [
    "print(str(tur_response) + '\\n' + str(stop - start) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article compares two popular language models, GPT-3 and BERT, and discusses their differences and similarities, exploring their capabilities and the tools that use them. GPT-3 is an autoregressive language model developed by OpenAI, while BERT is a bidirectional transformer model developed by Google AI. The main difference between the two models lies in their architecture and training dataset size, with GPT-3 being better suited for tasks such as summarization or translation, while BERT is more beneficial for sentiment analysis or natural language understanding (NLU). However, both models have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tur_response['choices'][0]['message']['content']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo\n",
    "- With Streaming\n",
    "    - use 'stream = True'\n",
    "    - automatically call 'await'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " article\n",
      " compares\n",
      " two\n",
      " popular\n",
      " language\n",
      " models\n",
      ",\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " and\n",
      " B\n",
      "ERT\n",
      ",\n",
      " discussing\n",
      " their\n",
      " differences\n",
      " and\n",
      " similarities\n",
      ",\n",
      " exploring\n",
      " their\n",
      " capabilities\n",
      ",\n",
      " and\n",
      " discussing\n",
      " some\n",
      " of\n",
      " the\n",
      " tools\n",
      " that\n",
      " use\n",
      " them\n",
      ".\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " is\n",
      " an\n",
      " aut\n",
      "ore\n",
      "gressive\n",
      " language\n",
      " model\n",
      " developed\n",
      " by\n",
      " Open\n",
      "AI\n",
      ",\n",
      " while\n",
      " B\n",
      "ERT\n",
      " is\n",
      " a\n",
      " bid\n",
      "irectional\n",
      " transformer\n",
      " model\n",
      " developed\n",
      " by\n",
      " Google\n",
      " AI\n",
      ".\n",
      " The\n",
      " main\n",
      " difference\n",
      " between\n",
      " the\n",
      " two\n",
      " models\n",
      " lies\n",
      " in\n",
      " their\n",
      " architecture\n",
      " and\n",
      " training\n",
      " dataset\n",
      " size\n",
      ",\n",
      " with\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " being\n",
      " better\n",
      " suited\n",
      " for\n",
      " tasks\n",
      " such\n",
      " as\n",
      " summar\n",
      "ization\n",
      " or\n",
      " translation\n",
      ",\n",
      " while\n",
      " B\n",
      "ERT\n",
      " is\n",
      " more\n",
      " beneficial\n",
      " for\n",
      " sentiment\n",
      " analysis\n",
      " or\n",
      " natural\n",
      " language\n",
      " understanding\n",
      " (\n",
      "N\n",
      "LU\n",
      ").\n",
      " However\n",
      ",\n",
      " both\n",
      " models\n",
      " have\n",
      " proven\n",
      " themselves\n",
      " valuable\n",
      " tools\n",
      " for\n",
      " performing\n",
      " various\n",
      " N\n",
      "LP\n",
      " tasks\n",
      " with\n",
      " varying\n",
      " degrees\n",
      " of\n",
      " accuracy\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\"You are a story teller. Make a story from two sentences.\"\"\"\n",
    "\n",
    "text = text1_content + '\\n' + text2_content\n",
    "\n",
    "usr_prompt = f\"\"\"\n",
    "Sentences: {text}\n",
    "\"\"\"\n",
    "\n",
    "tur_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f'{sys_prompt}'},\n",
    "        {\"role\": 'user', 'content': f'{usr_prompt}'},\n",
    "    ],\n",
    "    temperature = 0,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "for chunk in tur_response:\n",
    "    try:\n",
    "        print(chunk['choices'][0]['delta']['content'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text1_content + '\\n' + text2_content\n",
    "\n",
    "usr_prompt = f\"\"\"You are a story teller. Make a story from two sentences.\n",
    "Sentences: {text}\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "da_response = openai.Completion.create(\n",
    "    engine = 'text-davinci-003',\n",
    "    prompt = usr_prompt,\n",
    "    max_tokens = 300\n",
    ")\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7M6v1z4mPhssrfNtkAWmzPXFAdDk4 at 0x7f85c38ca150> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"GPT-3 and BERT are two well-known language models used in natural language processing (NLP) applications. GPT-3 is an autoregressive model trained on 45TB of data while BERT is bidirectional and trained on 3TB of data. They have significant differences in architecture and data sizes, but both are Transformer-based and can be used for similar tasks such as question answering, summarization, and translation. GPT-3 typically outperforms BERT in tasks that require more data, while BERT is better for sentiment analysis and natural language understanding. GPT-2 models can be used to generate data from prompts, while BERT can be used for zero- and few-shot learning with the PET method. [from url1 and url2]\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685504167,\n",
       "  \"id\": \"cmpl-7M6v1z4mPhssrfNtkAWmzPXFAdDk4\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 161,\n",
       "    \"prompt_tokens\": 1832,\n",
       "    \"total_tokens\": 1993\n",
       "  }\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-3 and BERT are two well-known language models used in natural language processing (NLP) applications. GPT-3 is an autoregressive model trained on 45TB of data while BERT is bidirectional and trained on 3TB of data. They have significant differences in architecture and data sizes, but both are Transformer-based and can be used for similar tasks such as question answering, summarization, and translation. GPT-3 typically outperforms BERT in tasks that require more data, while BERT is better for sentiment analysis and natural language understanding. GPT-2 models can be used to generate data from prompts, while BERT can be used for zero- and few-shot learning with the PET method. [from url1 and url2]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da_response['choices'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
